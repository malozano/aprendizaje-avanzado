<!DOCTYPE html><html lang="es" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Apuntes de la asignatura Aprendizaje Avanzado del Grado en Ingenier√≠a en Inteligencia Artificial de la Universidad de Alicante">
      
      
        <meta name="author" content="Miguel Angel Lozano">
      
      
        <link rel="canonical" href="https://malozano.github.io/aprendizaje-avanzado/05-adaboost/">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.1.1">
    
    
      
        <title>Boosting - Aprendizaje Avanzado</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.402914a4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.a0c5b2b5.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#boosting" class="md-skip">
          Saltar a contenido
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Cabecera">
    <a href=".." title="Aprendizaje Avanzado" class="md-header__button md-logo" aria-label="Aprendizaje Avanzado" data-md-component="logo">
      
  <img src="../images/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Aprendizaje Avanzado
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Boosting
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo" aria-label="Modo oscuro" type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Modo oscuro" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
            </label>
          
        
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="white" data-md-color-accent="blue" aria-label="Modo claro" type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Modo claro" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="B√∫squeda" placeholder="B√∫squeda" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Buscar">
        
        <button type="reset" class="md-search__icon md-icon" title="Limpiar" aria-label="Limpiar" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Inicializando b√∫squeda
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navegaci√≥n" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Aprendizaje Avanzado" class="md-nav__button md-logo" aria-label="Aprendizaje Avanzado" data-md-component="logo">
      
  <img src="../images/logo.png" alt="logo">

    </a>
    Aprendizaje Avanzado
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Introducci√≥n
      </a>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2">
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Bloque I. Aprendizaje supervisado
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Bloque I. Aprendizaje supervisado
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01-modelos-no-param/" class="md-nav__link">
        1. Modelos param√©tricos y no param√©tricos. Logistic Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02-svm/" class="md-nav__link">
        2. SVM
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03-arboles-decision/" class="md-nav__link">
        3. √Årboles de decisi√≥n
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04-random-forest/" class="md-nav__link">
        4. M√©todos de ensemble. Random Forest
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3">
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          Pr√°cticas
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Pr√°cticas
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../practicas/00-datos-y-visualizacion/" class="md-nav__link">
        0. Datos y visualizaci√≥n
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../practicas/01-aprendizaje-supervisado/" class="md-nav__link">
        1. Aprendizaje supervisado
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="boosting">Boosting<a class="headerlink" href="#boosting" title="Permanent link">¬∂</a></h1>
<p><em>Boosting</em> es un meta-algoritmo de aprendizaje autom√°tico, que busca <strong>reducir principalemente el sesgo</strong>, aunque tambi√©n algo la varianza. </p>
<p>A diferencia de los m√©todos de <em>ensemble</em> estudiados hasta el momento (<em>Voting</em>, <em>Stacking</em>, <em>Bagging</em>), en los que cont√°bamos con modelos independientes que se entrenaban en paralelo, <em>Boosting</em> entrena los modelos secuencialmente, de forma que cada nuevo modelo se centra en corregir los errores cometidos por el <em>ensemble</em> actual.</p>
<h2 id="combinacion-de-clasificadores">Combinaci√≥n de clasificadores<a class="headerlink" href="#combinacion-de-clasificadores" title="Permanent link">¬∂</a></h2>
<p>La idea que buscamos con <em>Boosting</em> es combinar una serie de <strong>clasificadores d√©biles</strong> para construir un <strong>clasificador fuerte</strong>. Los definimos de la siguiente forma:</p>
<ul>
<li>
<p><strong>Clasificador d√©bil (weak learner):</strong> Se trata de clasificadores simples que funcionan mejor que la clasificaci√≥n aleatoria. Es decir, su <em>accuracy</em> debe ser mayor que 50% (mejor que lanzar una moneda). Formalmente, para clasificaci√≥n binaria, un clasificador d√©bil debe cumplir que su tasa de error sea
<span class="arithmatex">\(\epsilon &lt; \frac{1}{2}\)</span>.</p>
</li>
<li>
<p><strong>Clasificador fuerte (strong learner):</strong> Se trata de un clasificador con alta precisi√≥n, capaz de aproximarse arbitrariamente bien a la funci√≥n objetivo. Su tasa de error puede ser tan peque√±a como se desee con suficientes datos y suficiente capacidad del modelo.</p>
</li>
</ul>
<h3 id="clasificadores-debiles">Clasificadores d√©biles<a class="headerlink" href="#clasificadores-debiles" title="Permanent link">¬∂</a></h3>
<p>Algunos de los ejemplos de clasificadores d√©biles t√≠picos son los siguientes:</p>
<ul>
<li>
<p><strong>Decision stumps</strong>: √Årboles de decisi√≥n con una √∫nica divisi√≥n (profundidad <span class="arithmatex">\(1\)</span>). Equivale a una regla simple, como por ejemplo, "si <span class="arithmatex">\(x_1 &gt; 5\)</span> entonces clase positiva, si no, clase negativa".</p>
</li>
<li>
<p><strong>√Årboles poco profundos</strong>: √Årboles con profundidad m√°xima <span class="arithmatex">\(2\)</span> o <span class="arithmatex">\(3\)</span>.</p>
</li>
<li>
<p><strong>Clasificadores lineales</strong> en problemas no lineales.</p>
</li>
</ul>
<h3 id="clasificadores-fuertes">Clasificadores fuertes<a class="headerlink" href="#clasificadores-fuertes" title="Permanent link">¬∂</a></h3>
<p>A continuaci√≥n mostramos algunos ejemplos de clasificadores fuertes t√≠picos:</p>
<ul>
<li>Redes neuronales profundas</li>
<li>√Årboles de decisi√≥n muy profundos</li>
<li>SVMs con <em>kernels</em> complejos</li>
<li>Random Forests</li>
<li>Gradient Boosting</li>
</ul>
<h2 id="teorema-fundamental-del-boosting">Teorema Fundamental del Boosting<a class="headerlink" href="#teorema-fundamental-del-boosting" title="Permanent link">¬∂</a></h2>
<p>En 1988 y 1989 Kearns y Valiant (Kearns &amp; Valiant, 1988, 1989)<sup id="fnref:kearns1988cryptographic"><a class="footnote-ref" href="#fn:kearns1988cryptographic">1</a></sup> <sup id="fnref:kearns1989cryptographic"><a class="footnote-ref" href="#fn:kearns1989cryptographic">2</a></sup> plantearon la pregunta que constituye la fundaci√≥n te√≥rica del <em>Boosting</em>: <strong>"¬øPuede un conjunto de clasificadores d√©biles crear un √∫nico clasificador fuerte?"</strong>. </p>
<p>En 1990 Schapire (Schapire, 1990)<sup id="fnref:schapire1990strength"><a class="footnote-ref" href="#fn:schapire1990strength">3</a></sup> demuestra que esto es posible, lo cual supone uno de los resultados m√°s importantes en teor√≠a de aprendizaje autom√°tico. Lo que nos dice el teorema que plantea es que si existe un clasificador d√©bil que puede lograr error menor que <span class="arithmatex">\(1/2 - \gamma\)</span> (donde <span class="arithmatex">\(\gamma &gt; 0\)</span>), entonces existe un algoritmo de <em>Boosting</em> que puede combinarlo para lograr un error arbitrariamente peque√±o en el conjunto de entrenamiento.</p>
<p>Este teorema es importante porque:</p>
<ol>
<li>Demuestra que la "debilidad" es suficiente para el aprendizaje.</li>
<li>Proporciona una garant√≠a matem√°tica sobre los m√©todos de <em>Boosting</em>.</li>
<li>Nos muestra c√≥mo construir el clasificador fuerte.</li>
</ol>
<p>En este punto, nos podemos plantear la pregunta "¬øPor qu√© usar clasificadores d√©biles?". Aunque pueda parecer contraintuitivo usar modelos "d√©biles", hay varios motivos para hacerlo:</p>
<ol>
<li><strong>Prevenci√≥n del <em>overfitting</em>:</strong> </li>
<li>Los modelos d√©biles tienen baja varianza</li>
<li>Son menos propensos a aprender el ruido</li>
<li>
<p>La combinaci√≥n de modelos reduce el <em>overfitting</em></p>
</li>
<li>
<p><strong>Eficiencia computacional:</strong></p>
</li>
<li>Modelos sencillos como los <em>decision stumps</em> son extremadamente r√°pidos.</li>
<li>
<p>Podemos entrenar cientos o miles de ellos de forma eficiente.</p>
</li>
<li>
<p><strong>Interpretabilidad:</strong></p>
</li>
<li>Cada modelo d√©bil es f√°cil de entender.</li>
<li>
<p>La combinaci√≥n de modelos mantiene cierta trazabilidad.</p>
</li>
<li>
<p><strong>Teor√≠a s√≥lida:</strong></p>
</li>
<li>Existen garant√≠as matem√°ticas de convergencia.</li>
<li>El error de generalizaci√≥n puede ser acotado.</li>
</ol>
<p>Para que un clasificador d√©bil sea √∫til en un <em>ensemble</em> debe cumplir:</p>
<ol>
<li><strong>Mejor que el azar:</strong> Su error debe ser <span class="arithmatex">\(\epsilon &lt; 0.5\)</span>.</li>
<li><strong>Diversidad:</strong> Debe cometer errores diferentes a los que cometen otros clasificadores.</li>
<li><strong>Eficiencia:</strong> Debe ser r√°pido de entrenar.</li>
<li><strong>Estabilidad:</strong> No debe ser extremadamente sensible a peque√±os cambios en los datos.</li>
</ol>
<p>Respecto a la relaci√≥n con el problema <strong>sesgo-varianza</strong>, los clasificadores d√©biles tendr√°n un alto sesgo, pero baja varianza (deben ser estables ante cambios en los datos). Por el contrario, los clasificadores fuertes tendr√°n bajo sesgo, ya que pueden aprender patrones complejos, pero una alta varianza, ya que ser√°n m√°s sensibles a los datos de entrenamiento.</p>
<p>Un <em>ensemble</em> de clasificadores d√©biles reducir√° el sesgo, al combinar de forma secuencial los clasificadores, manteniendo la baja varianza de los modelos individuales, obteniendo as√≠ lo mejor de cada tipo de clasificador. </p>
<h2 id="muestreo-y-votos-ponderados">Muestreo y votos ponderados<a class="headerlink" href="#muestreo-y-votos-ponderados" title="Permanent link">¬∂</a></h2>
<p>Como hemos comentado, una de las principales diferencias de <em>Boosting</em> con los m√©todos de <em>ensemble</em> vistos anteriormente es que en lugar de entrenar los modelos en paralelo, <em>Boosting</em> realiza el entrenamiento secuencialmente, buscando que los nuevos clasificadores d√©biles corrijan los principales errores del <em>ensemble</em> actual. Para ello introduce ponderaci√≥n a dos niveles: <strong>en el muestreo de ejemplos de entrenamiento</strong> y en <strong>la importancia de cada clasificador</strong>.</p>
<p>Si recordamos las caracter√≠sticas de los m√©todos de <em>Bagging</em>, tenemos:</p>
<ul>
<li><em>Bagging</em> realiza un muestreo aleatorio de los ejemplos de entrada para entrenar cada clasificador, pero todos estos ejemplos de entrada reciben el mismo peso.</li>
<li><em>Bagging</em> da la misma importancia a todos los clasificadores. El voto de cada clasificador vale lo mismo.</li>
</ul>
<p>A diferencia de esto, <em>Boosting</em> introduce:</p>
<ul>
<li>
<p><strong>Muestreo ponderado</strong>: No se da el mismo peso a todos los ejemplos de entrenamiento. El entrenamiento se concentrar√° en los ejemplos m√°s dif√≠ciles. Intuitivamente, podr√≠amos considerar que aquellos ejemplos cercanos a la frontera de decisi√≥n son m√°s dif√≠ciles de clasificar, y deber√≠an por lo tanto recibir pesos m√°s altos. Podemos establecer una relaci√≥n entre esto y los vectores de soporte en SVM, ya que en ambos casos buscamos basarnos en los ejemplos m√°s dif√≠ciles de clasificar para obtener el clasificador. </p>
</li>
<li>
<p><strong>Votos ponderados</strong>: Se da diferente peso a los diferentes clasificadores, que al combinarlos se obtiene un "voto ponderado". Esto, junto a la estrategia de muestro anterior, ayudar√° a producir un clasificador m√°s fuerte.</p>
</li>
</ul>
<p>Considerando que contamos con un <em>dataset</em> <span class="arithmatex">\(\mathcal{D}\)</span> con <span class="arithmatex">\(N\)</span> ejemplos de entrenamiento <span class="arithmatex">\((\mathbf{x}_i, y_i)\)</span>, con <span class="arithmatex">\(i = 1, 2, \ldots, N\)</span>, y <span class="arithmatex">\(T\)</span> clasificadores d√©biles <span class="arithmatex">\(h_1, h_2, \ldots, h_T\)</span>, definimos:</p>
<ul>
<li>
<p><span class="arithmatex">\(w_i^{(t)}\)</span>: Peso del ejemplo de entrenamiento <span class="arithmatex">\((\mathbf{x}_i, y_i)\)</span> para entrenar el clasificador <span class="arithmatex">\(h_t\)</span>.</p>
</li>
<li>
<p><span class="arithmatex">\(\alpha_t\)</span>: Peso del clasificador <span class="arithmatex">\(h_t\)</span> en la votaci√≥n del <em>ensemble</em>.</p>
</li>
</ul>
<h2 id="adaboost-adaptive-boosting">AdaBoost (<em>Adaptive Boosting</em>)<a class="headerlink" href="#adaboost-adaptive-boosting" title="Permanent link">¬∂</a></h2>
<p>AdaBoost (Freund &amp; Schapire, 1997)<sup id="fnref:freund1997decision"><a class="footnote-ref" href="#fn:freund1997decision">4</a></sup> fue desarrollado por Freund y Schapire en 1997, y constituye el primer algoritmo de <em>Boosting</em> exitoso.</p>
<p>La idea central de este algoritmo es:</p>
<ol>
<li>Entrenar un <strong>clasificador d√©bil</strong>.</li>
<li><strong>Aumentar el peso</strong> de los ejemplos mal clasificados.</li>
<li>Entrenar el siguiente clasificador con los <strong>datos ponderados</strong>.</li>
<li><strong>Repetir</strong> el proceso.</li>
<li><strong>Combinar</strong> todos los clasificadores ajustando sus pesos seg√∫n su <em>accuracy</em>.</li>
</ol>
<p>Considerando el caso de clasificaci√≥n binaria, donde <span class="arithmatex">\(y_i \in \{ -1, 1\}\)</span>, el <strong>algoritmo AdaBoost</strong> a nivel general ser√≠a como se muestra a continuaci√≥n:</p>
<div class="arithmatex">\[
\begin{align*}
&amp; \text{Entrada: } \text{Conjunto de entrenamiento } \mathcal{D} \\
&amp; w_i^1 \leftarrow 1 \quad \forall i \in 1, 2, \ldots, N \quad \text{(Inicializa todos los ejemplos con peso uniforme)} \\
&amp; \text{Para  } t = 1, \ldots, T \\
&amp; \quad h_t \leftarrow \text{Entrenar un clasificador d√©bil con los pesos } \mathbf{w}^{(t)} \\
&amp; \quad \alpha_t \leftarrow \text{Calcular el peso del clasificador} h_t \\
&amp; \quad w_i^{(t+1)} \leftarrow w_i^{(t)} e^{-\alpha_t y_i h_t(x_i)} \quad \text{(Actualiza los pesos de los ejemplos para el siguiente clasificador)} \\
&amp; \text{Devuelve: } H(\mathbf{x}) = \text{signo} \left( \sum_{t=1}^T \alpha_t h_t(\mathbf{x}) \right)
\end{align*}
\]</div>
<p>Vamos a continuaci√≥n a detallar los diferentes pasos de este algoritmo.</p>
<h3 id="entrenar-un-clasificador-debil">Entrenar un clasificador d√©bil<a class="headerlink" href="#entrenar-un-clasificador-debil" title="Permanent link">¬∂</a></h3>
<p>Siguiendo el proceso del algoritmo anterior, podemos considerar que hasta la iteraci√≥n <span class="arithmatex">\(t-1\)</span> tendremos un <strong>clasificador fuete</strong> <span class="arithmatex">\(H_{t-1}\)</span>, que se obtiene como:</p>
<div class="arithmatex">\[
H_{t-1}(\mathbf{x}) = \alpha_1 h_1(\mathbf{x}) + \alpha_2 h_2(\mathbf{x}) + \ldots + \alpha_{t-1} h_{t-1}(\mathbf{x})
\]</div>
<p>En la iteraci√≥n <span class="arithmatex">\(t\)</span> buscaremos mejorarlo a√±adiendo un <strong>nuevo clasificador d√©bil</strong> <span class="arithmatex">\(h_t\)</span>:</p>
<div class="arithmatex">\[
H_{t}(\mathbf{x}) = H_{t-1}(\mathbf{x}) + \alpha_t h_t(\mathbf{x}) 
\]</div>
<p>Buscaremos el clasificador que minimice el siguiente error:</p>
<div class="arithmatex">\[
\begin{align*}
\epsilon &amp; = \sum_{i=1}^N e^{-y_i H_t(\mathbf{x}_i)} = \sum_{i=1}^N e^{-y_i (H_{t-1}(\mathbf{x}_i) + \alpha_t h_t(\mathbf{x}_i))} = \\
&amp; = \sum_{i=1}^N e^{-y_i H_{t-1}(\mathbf{x}_i) } e^{-y_i  \alpha_t h_t(\mathbf{x}_i)} = \sum_{i=1}^N w_i^{(t)} e^{-y_i  \alpha_t h_t(\mathbf{x}_i)}
\end{align*}
\]</div>
<p>En el √∫ltimo paso del desarrollo anterior hemos de considerar la forma en la que se actualizan los pesos en cada iteraci√≥n:</p>
<div class="arithmatex">\[
\begin{align*}
w_i^{(t+1)} &amp;= w_i^{(t)} e^{-\alpha_t y_i h_t(\mathbf{x}_i)} = \prod_{j=1}^t e^{-\alpha_j y_i h_j(\mathbf{x}_i)} = \\
&amp; = e^{-y_i \sum_{j=1}^t \alpha_j h_j(\mathbf{x}_i)} = e^{-y_i H_{t}(\mathbf{x}_i) }
\end{align*}
\]</div>
<p>Por lo tanto, tenemos que el error que buscamos minimizar es:</p>
<div class="arithmatex">\[
\epsilon = \sum_{i=1}^N w_i^{(t)} e^{-y_i  \alpha_t h_t(\mathbf{x}_i)}
\]</div>
<p>Si en la funci√≥n separamos los ejemplos que se clasifican correcta e incorrectamente, tendr√≠amos:</p>
<div class="arithmatex">\[
\epsilon = \sum_{y_i = h_t(\mathbf{x}_i)} w_i^{(t)} e^{-\alpha_t } + \sum_{y_i \neq h_t(\mathbf{x}_i)} w_i^{(t)} e^{\alpha_t }
\]</div>
<p>De forma equivalente, podr√≠a expresarse como:</p>
<div class="arithmatex">\[
\epsilon = \sum_{i=1}^N w_i^{(t)} e^{-\alpha_t } + \sum_{y_i \neq h_t(\mathbf{x}_i)} w_i^{(t)} (e^{\alpha_t } - e^{-\alpha_t })
\]</div>
<p>En la ecuaci√≥n anterior, vemos que s√≥lo el segundo t√©rmino depende de <span class="arithmatex">\(h_t\)</span>. Por lo tanto, el clasificador que minimizar√° el error ser√° aquel que minimice <span class="arithmatex">\(\sum_{y_i \neq h_t(\mathbf{x}_i)} w_i^{(t)}\)</span>.</p>
<h3 id="calcular-el-peso-del-clasificador">Calcular el peso del clasificador<a class="headerlink" href="#calcular-el-peso-del-clasificador" title="Permanent link">¬∂</a></h3>
<p>Deberemos buscar el valor del peso de cada clasificador que <strong>minimice el error del <em>ensemble</em></strong>. Para hacer esto, en primer lugar derivaremos la funci√≥n de error respecto al peso <span class="arithmatex">\(\alpha_t\)</span> de cada clasificador:</p>
<div class="arithmatex">\[
\begin{align*}
\frac{\partial \epsilon}{\partial \alpha_t} &amp;= \frac{\partial \left( \sum_{y_i = h_t(\mathbf{x}_i)} w_i^{(t)} e^{-\alpha_t } + \sum_{y_i \neq h_t(\mathbf{x}_i)} w_i^{(t)} e^{\alpha_t } \right)}{\partial \alpha_t} = 
\\
&amp;= -\sum_{y_i = h_t(\mathbf{x}_i)} w_i^{(t)} e^{-\alpha_t } + \sum_{y_i \neq h_t(\mathbf{x}_i)} w_i^{(t)} e^{\alpha_t }
\end{align*}
\]</div>
<p>Teniendo en cuenta que la funci√≥n de error es una funci√≥n convexa, la igualaremos a <span class="arithmatex">\(0\)</span> para buscar el punto en la que es m√≠nima:</p>
<div class="arithmatex">\[
-\sum_{y_i = h_t(\mathbf{x}_i)} w_i^{(t)} e^{-\alpha_t } + \sum_{y_i \neq h_t(\mathbf{x}_i)} w_i^{(t)} e^{\alpha_t } = 0
\]</div>
<div class="arithmatex">\[
\Downarrow
\]</div>
<div class="arithmatex">\[
\sum_{y_i = h_t(\mathbf{x}_i)} w_i^{(t)} e^{-\alpha_t } = \sum_{y_i \neq h_t(\mathbf{x}_i)} w_i^{(t)} e^{\alpha_t }
\]</div>
<div class="arithmatex">\[
\Downarrow
\]</div>
<div class="arithmatex">\[
\frac{\sum_{y_i = h_t(\mathbf{x}_i)} w_i^{(t)}}{\sum_{y_i \neq h_t(\mathbf{x}_i)} w_i^{(t)}} = \frac{e^{\alpha_t }}{e^{-\alpha_t }} = e^{2\alpha_t}
\]</div>
<p>Por lo tanto, tenemos:</p>
<div class="arithmatex">\[
\alpha_t = \frac{1}{2} \ln \frac{\sum_{y_i = h_t(\mathbf{x}_i)} w_i^{(t)}}{\sum_{y_i \neq h_t(\mathbf{x}_i)} w_i^{(t)}}
\]</div>
<p>Podemos definir el error del clasificador d√©bil <span class="arithmatex">\(h_t\)</span> como <span class="arithmatex">\(\epsilon_t\)</span>, de la siguiente forma:</p>
<div class="arithmatex">\[
\epsilon_t = \frac{\sum_{y_i \neq h_t(\mathbf{x}_i)} w_i^{(t)}}{\sum_{i=1}^N w_i^{(t)}}
\]</div>
<p>Observamos que calculamos el error de <span class="arithmatex">\(h_t\)</span> como la suma de los pesos de los ejemplos mal clasificados por dicho clasificador d√©bil, entre la suma de los pesos de todos los ejemplos. De esta forma, el clasificador tendr√° error bajo cuando el peso total de los ejemplos mal clasificados sea bajo, y los ejemplos de mayor peso hayan sido correctamente clasificados. </p>
<p>Teniendo esta definici√≥n en cuenta, tenemos:</p>
<div class="arithmatex">\[
\begin{align*}
\alpha_t &amp;= \frac{1}{2} \ln \frac{\sum_{y_i = h_t(\mathbf{x}_i)} w_i^{(t)}}{\sum_{y_i \neq h_t(\mathbf{x}_i)} w_i^{(t)}} = \\
&amp;= \frac{1}{2} \ln \frac{\sum_{i=1}^N w_i^{(t)} - \sum_{y_i \neq h_t(\mathbf{x}_i)} w_i^{(t)}}{\sum_{y_i \neq h_t(\mathbf{x}_i)} w_i^{(t)}} = \\
&amp;= \frac{1}{2} \ln \frac{ \frac{\sum_{i=1}^N w_i^{(t)}}{\sum_{i=1}^N w_i^{(t)}} - \frac{\sum_{y_i \neq h_t(\mathbf{x}_i)} w_i^{(t)}}{\sum_{i=1}^N w_i^{(t)}}}{\frac{\sum_{y_i \neq h_t(\mathbf{x}_i)} w_i^{(t)}}{\sum_{i=1}^N w_i^{(t)}}} = \\
&amp;= \frac{1}{2} \ln \frac{1-\epsilon_t}{\epsilon_t}
\end{align*}
\]</div>
<p>Con esto podemos observar que <span class="arithmatex">\(\alpha_t\)</span> ser√° mayor cuanto menor error <span class="arithmatex">\(\epsilon_t\)</span> tenga el clasificador.</p>
<h3 id="algoritmo-detallado">Algoritmo detallado<a class="headerlink" href="#algoritmo-detallado" title="Permanent link">¬∂</a></h3>
<p>Con todo lo anterior, podemos escribir de forma completa el algoritmo AdaBoost detallando en cada paso la forma en la que se calculan los errores y los pesos:</p>
<div class="arithmatex">\[
\begin{align*}
&amp; \text{Entrada: } \text{Conjunto de entrenamiento } \mathcal{D} \\
&amp; w_i^1 \leftarrow 1 \quad \forall i \in 1, 2, \ldots, N \quad \text{(Inicializa todos los ejemplos con peso uniforme)} \\
&amp; \text{Para  } t = 1, \ldots, T \\
&amp; \quad h_t \leftarrow \text{Entrenar un clasificador d√©bil que minimice } \sum_{y_i \neq h_t(\mathbf{x}_i)} w_i^{(t)} \\
&amp; \quad \epsilon_t \leftarrow \frac{\sum_{y_i \neq h_t(\mathbf{x}_i)} w_i^{(t)}}{\sum_{i=1}^N w_i^{(t)}} \quad \text{(Calcular el error del clasificador d√©bil)} \\
&amp; \quad \alpha_t \leftarrow \frac{1}{2} \ln \left( \frac{1-\epsilon_t}{\epsilon_t} \right) \quad \text{(Calcular el peso del clasificador)}  \\
&amp; \quad w_i^{(t+1)} \leftarrow w_i^{(t)} e^{-\alpha_t y_i h_t(x_i)} \quad \text{(Actualiza los pesos de los ejemplos para el siguiente clasificador)} \\
&amp; \text{Devuelve: } H(\mathbf{x}) = \text{signo} \left( \sum_{t=1}^T \alpha_t h_t(\mathbf{x}) \right)
\end{align*}
\]</div>
<h3 id="ejemplo-paso-a-paso">Ejemplo paso a paso<a class="headerlink" href="#ejemplo-paso-a-paso" title="Permanent link">¬∂</a></h3>
<p><strong>Paso 2: Calcular error ponderado</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>Œµ‚Çú = Œ£·µ¢ w‚Çú(i) ¬∑ ùüô(h‚Çú(x·µ¢) ‚â† y·µ¢)
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>Donde ùüô es la funci√≥n indicadora (1 si error, 0 si correcto)
</code></pre></div><p></p>
<p><strong>Paso 3: Calcular peso del clasificador</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>Œ±‚Çú = (1/2) ¬∑ ln((1 - Œµ‚Çú) / Œµ‚Çú)
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>Interpretaci√≥n:
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>- Si Œµ‚Çú peque√±o (buen clasificador) ‚Üí Œ±‚Çú grande (mucho peso)
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>- Si Œµ‚Çú = 0.5 (azar) ‚Üí Œ±‚Çú = 0 (sin peso)
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>- Si Œµ‚Çú &gt; 0.5 (peor que azar) ‚Üí Œ±‚Çú negativo (invertir predicci√≥n)
</code></pre></div><p></p>
<p><strong>Paso 4: Actualizar pesos de ejemplos</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>w‚Çú‚Çä‚ÇÅ(i) = w‚Çú(i) ¬∑ exp(-Œ±‚Çú ¬∑ y·µ¢ ¬∑ h‚Çú(x·µ¢))
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>Simplificado:
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>- Si h‚Çú clasifica correctamente x·µ¢: w‚Çú‚Çä‚ÇÅ(i) disminuye
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>- Si h‚Çú clasifica incorrectamente x·µ¢: w‚Çú‚Çä‚ÇÅ(i) aumenta
</code></pre></div><p></p>
<p><strong>Paso 5: Normalizar pesos</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>w‚Çú‚Çä‚ÇÅ(i) = w‚Çú‚Çä‚ÇÅ(i) / Œ£‚±º w‚Çú‚Çä‚ÇÅ(j)
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>(Para que los pesos sumen 1)
</code></pre></div><p></p>
<p><strong>Predicci√≥n final:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>H(x) = sign(Œ£‚Çú‚Çå‚ÇÅ·µÄ Œ±‚Çú ¬∑ h‚Çú(x))
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a>Es decir, suma ponderada de todos los clasificadores
</code></pre></div><p></p>
<h3 id="ejemplo-paso-a-paso_1">Ejemplo paso a paso<a class="headerlink" href="#ejemplo-paso-a-paso_1" title="Permanent link">¬∂</a></h3>
<p>Consideremos un ejemplo simple con 10 puntos:
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="c1"># Dataset</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a><span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="n">x4</span><span class="p">,</span> <span class="n">x5</span><span class="p">,</span> <span class="n">x6</span><span class="p">,</span> <span class="n">x7</span><span class="p">,</span> <span class="n">x8</span><span class="p">,</span> <span class="n">x9</span><span class="p">,</span> <span class="n">x10</span><span class="p">]</span>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">]</span>
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a><span class="c1"># Iteraci√≥n 1:</span>
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a><span class="c1"># Pesos iniciales: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]</span>
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a><span class="c1"># Clasificador d√©bil h‚ÇÅ: clasifica bien el 70% (Œµ‚ÇÅ = 0.3)</span>
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a><span class="c1"># Œ±‚ÇÅ = 0.5 * ln((1-0.3)/0.3) = 0.42</span>
<a id="__codelineno-5-9" name="__codelineno-5-9" href="#__codelineno-5-9"></a><span class="c1"># </span>
<a id="__codelineno-5-10" name="__codelineno-5-10" href="#__codelineno-5-10"></a><span class="c1"># Errores en: x4, x7, x9</span>
<a id="__codelineno-5-11" name="__codelineno-5-11" href="#__codelineno-5-11"></a><span class="c1"># Nuevos pesos (normalizados):</span>
<a id="__codelineno-5-12" name="__codelineno-5-12" href="#__codelineno-5-12"></a><span class="c1"># Correctos: peso disminuye a ~0.07</span>
<a id="__codelineno-5-13" name="__codelineno-5-13" href="#__codelineno-5-13"></a><span class="c1"># Incorrectos: peso aumenta a ~0.17</span>
<a id="__codelineno-5-14" name="__codelineno-5-14" href="#__codelineno-5-14"></a>
<a id="__codelineno-5-15" name="__codelineno-5-15" href="#__codelineno-5-15"></a><span class="c1"># Iteraci√≥n 2:</span>
<a id="__codelineno-5-16" name="__codelineno-5-16" href="#__codelineno-5-16"></a><span class="c1"># Ahora h‚ÇÇ se entrena dando m√°s importancia a x4, x7, x9</span>
<a id="__codelineno-5-17" name="__codelineno-5-17" href="#__codelineno-5-17"></a><span class="c1"># h‚ÇÇ clasifica bien el 80% en datos ponderados</span>
<a id="__codelineno-5-18" name="__codelineno-5-18" href="#__codelineno-5-18"></a><span class="c1"># Œ±‚ÇÇ = 0.5 * ln((1-0.2)/0.2) = 0.69</span>
<a id="__codelineno-5-19" name="__codelineno-5-19" href="#__codelineno-5-19"></a><span class="c1"># ...</span>
<a id="__codelineno-5-20" name="__codelineno-5-20" href="#__codelineno-5-20"></a>
<a id="__codelineno-5-21" name="__codelineno-5-21" href="#__codelineno-5-21"></a><span class="c1"># Predicci√≥n final:</span>
<a id="__codelineno-5-22" name="__codelineno-5-22" href="#__codelineno-5-22"></a><span class="c1"># H(x) = sign(0.42*h‚ÇÅ(x) + 0.69*h‚ÇÇ(x) + 0.54*h‚ÇÉ(x) + ...)</span>
</code></pre></div><p></p>
<h3 id="propiedades-importantes">Propiedades Importantes<a class="headerlink" href="#propiedades-importantes" title="Permanent link">¬∂</a></h3>
<h4 id="training-error-bound">Training Error Bound<a class="headerlink" href="#training-error-bound" title="Permanent link">¬∂</a></h4>
<p>AdaBoost tiene una garant√≠a te√≥rica sobre el error de entrenamiento:</p>
<div class="arithmatex">\[\text{Training Error} \leq \prod_{t=1}^{T} \sqrt{\epsilon_t(1-\epsilon_t)} \leq \exp\left(-2\sum_{t=1}^{T}\gamma_t^2\right)\]</div>
<p>Donde <span class="arithmatex">\(\gamma_t = 0.5 - \epsilon_t\)</span> es el "margen" del clasificador d√©bil.</p>
<p><strong>Implicaci√≥n:</strong> Si cada clasificador d√©bil es solo ligeramente mejor que el azar (<span class="arithmatex">\(\epsilon_t &lt; 0.5\)</span>), el error de entrenamiento converge exponencialmente r√°pido a 0.</p>
<h4 id="no-requiere-conocer-t-de-antemano">No requiere conocer Œµ‚Çú de antemano<a class="headerlink" href="#no-requiere-conocer-t-de-antemano" title="Permanent link">¬∂</a></h4>
<p>A diferencia de otros m√©todos, AdaBoost:
- No necesita que especifiques qu√© tan "d√©bil" es tu clasificador
- Se adapta autom√°ticamente seg√∫n el error observado
- Funciona con cualquier clasificador d√©bil v√°lido</p>
<h3 id="implementacion-en-scikit-learn">Implementaci√≥n en scikit-learn<a class="headerlink" href="#implementacion-en-scikit-learn" title="Permanent link">¬∂</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a><span class="c1"># AdaBoost con decision stumps (default)</span>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a><span class="n">ada</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span>
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a>    <span class="n">estimator</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># Decision stump</span>
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>         <span class="c1"># N√∫mero de clasificadores d√©biles</span>
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>       <span class="c1"># Peso en la actualizaci√≥n (default=1.0)</span>
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a>    <span class="n">algorithm</span><span class="o">=</span><span class="s1">'SAMME.R'</span><span class="p">,</span>     <span class="c1"># SAMME.R usa probabilidades (mejor)</span>
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#__codelineno-6-11"></a><span class="p">)</span>
<a id="__codelineno-6-12" name="__codelineno-6-12" href="#__codelineno-6-12"></a>
<a id="__codelineno-6-13" name="__codelineno-6-13" href="#__codelineno-6-13"></a><span class="n">ada</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-6-14" name="__codelineno-6-14" href="#__codelineno-6-14"></a>
<a id="__codelineno-6-15" name="__codelineno-6-15" href="#__codelineno-6-15"></a><span class="c1"># Inspeccionar clasificadores y pesos</span>
<a id="__codelineno-6-16" name="__codelineno-6-16" href="#__codelineno-6-16"></a><span class="nb">print</span><span class="p">(</span><span class="s2">"Pesos de clasificadores:"</span><span class="p">,</span> <span class="n">ada</span><span class="o">.</span><span class="n">estimator_weights_</span><span class="p">)</span>
<a id="__codelineno-6-17" name="__codelineno-6-17" href="#__codelineno-6-17"></a><span class="nb">print</span><span class="p">(</span><span class="s2">"Errores de clasificadores:"</span><span class="p">,</span> <span class="n">ada</span><span class="o">.</span><span class="n">estimator_errors_</span><span class="p">)</span>
</code></pre></div>
<h3 id="variantes-de-adaboost">Variantes de AdaBoost<a class="headerlink" href="#variantes-de-adaboost" title="Permanent link">¬∂</a></h3>
<h4 id="samme-stagewise-additive-modeling-using-a-multiclass-exponential-loss">SAMME (Stagewise Additive Modeling using a Multiclass Exponential loss)<a class="headerlink" href="#samme-stagewise-additive-modeling-using-a-multiclass-exponential-loss" title="Permanent link">¬∂</a></h4>
<p>Generalizaci√≥n de AdaBoost para m√°s de 2 clases:</p>
<div class="arithmatex">\[\alpha_t = \ln\left(\frac{1-\epsilon_t}{\epsilon_t}\right) + \ln(K-1)\]</div>
<p>Donde K es el n√∫mero de clases.</p>
<h4 id="sammer-real-valued-predictions">SAMME.R (Real-valued predictions)<a class="headerlink" href="#sammer-real-valued-predictions" title="Permanent link">¬∂</a></h4>
<p>Usa probabilidades en lugar de predicciones discretas:
- M√°s suave y generalmente mejor rendimiento
- Requiere que el clasificador base tenga <code>predict_proba</code>
- Es el default en scikit-learn</p>
<h3 id="ventajas-de-adaboost">Ventajas de AdaBoost<a class="headerlink" href="#ventajas-de-adaboost" title="Permanent link">¬∂</a></h3>
<ul>
<li><strong>Simple conceptualmente:</strong> F√°cil de entender e implementar</li>
<li><strong>Garant√≠as te√≥ricas fuertes:</strong> Bounds en error de entrenamiento</li>
<li><strong>Flexible:</strong> Funciona con cualquier clasificador d√©bil</li>
<li><strong>Poco tuning:</strong> Pocos hiperpar√°metros</li>
<li><strong>No requiere conocer Œµ:</strong> Se adapta autom√°ticamente</li>
<li><strong>Efectivo:</strong> Puede alcanzar alta accuracy con clasificadores muy simples</li>
</ul>
<h3 id="limitaciones-de-adaboost">Limitaciones de AdaBoost<a class="headerlink" href="#limitaciones-de-adaboost" title="Permanent link">¬∂</a></h3>
<ul>
<li><strong>Sensible a ruido y outliers:</strong> Les da mucho peso</li>
<li><strong>Sensible a overfitting:</strong> Con muchas iteraciones</li>
<li><strong>No paralelizable:</strong> Los clasificadores deben entrenarse secuencialmente</li>
<li><strong>Puede ser lento:</strong> Con clasificadores d√©biles lentos</li>
<li><strong>Requiere clasificadores d√©biles apropiados:</strong> No funciona con clasificadores peores que el azar</li>
</ul>
<h3 id="cuando-usar-adaboost">Cu√°ndo usar AdaBoost<a class="headerlink" href="#cuando-usar-adaboost" title="Permanent link">¬∂</a></h3>
<p><strong>Usar AdaBoost cuando:</strong>
- ‚úÖ Tienes un <strong>clasificador d√©bil</strong> bueno y r√°pido
- ‚úÖ Dataset <strong>limpio</strong> (poco ruido)
- ‚úÖ Quieres entender <strong>boosting conceptualmente</strong>
- ‚úÖ Clasificaci√≥n binaria o multiclase simple
- ‚úÖ Dataset peque√±o-mediano</p>
<p><strong>Considerar alternativas cuando:</strong>
- ‚ùå Dataset con mucho <strong>ruido/outliers</strong> ‚Üí Gradient Boosting m√°s robusto
- ‚ùå Buscas <strong>m√°ximo rendimiento</strong> ‚Üí XGBoost/LightGBM
- ‚ùå Dataset muy grande ‚Üí LightGBM m√°s eficiente
- ‚ùå Muchas features categ√≥ricas ‚Üí CatBoost</p>
<h3 id="ejemplo-completo">Ejemplo Completo<a class="headerlink" href="#ejemplo-completo" title="Permanent link">¬∂</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">learning_curve</span>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<a id="__codelineno-7-7" name="__codelineno-7-7" href="#__codelineno-7-7"></a>
<a id="__codelineno-7-8" name="__codelineno-7-8" href="#__codelineno-7-8"></a><span class="c1"># Generar datos</span>
<a id="__codelineno-7-9" name="__codelineno-7-9" href="#__codelineno-7-9"></a><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
<a id="__codelineno-7-10" name="__codelineno-7-10" href="#__codelineno-7-10"></a>                          <span class="n">n_redundant</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-7-11" name="__codelineno-7-11" href="#__codelineno-7-11"></a><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-7-12" name="__codelineno-7-12" href="#__codelineno-7-12"></a>
<a id="__codelineno-7-13" name="__codelineno-7-13" href="#__codelineno-7-13"></a><span class="c1"># Decision stump individual</span>
<a id="__codelineno-7-14" name="__codelineno-7-14" href="#__codelineno-7-14"></a><span class="n">stump</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-7-15" name="__codelineno-7-15" href="#__codelineno-7-15"></a><span class="n">stump</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-7-16" name="__codelineno-7-16" href="#__codelineno-7-16"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Decision stump: </span><span class="si">{</span><span class="n">stump</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-7-17" name="__codelineno-7-17" href="#__codelineno-7-17"></a>
<a id="__codelineno-7-18" name="__codelineno-7-18" href="#__codelineno-7-18"></a><span class="c1"># AdaBoost con stumps</span>
<a id="__codelineno-7-19" name="__codelineno-7-19" href="#__codelineno-7-19"></a><span class="n">ada</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span>
<a id="__codelineno-7-20" name="__codelineno-7-20" href="#__codelineno-7-20"></a>    <span class="n">estimator</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
<a id="__codelineno-7-21" name="__codelineno-7-21" href="#__codelineno-7-21"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-7-22" name="__codelineno-7-22" href="#__codelineno-7-22"></a>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<a id="__codelineno-7-23" name="__codelineno-7-23" href="#__codelineno-7-23"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-7-24" name="__codelineno-7-24" href="#__codelineno-7-24"></a><span class="p">)</span>
<a id="__codelineno-7-25" name="__codelineno-7-25" href="#__codelineno-7-25"></a><span class="n">ada</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-7-26" name="__codelineno-7-26" href="#__codelineno-7-26"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"AdaBoost (100 stumps): </span><span class="si">{</span><span class="n">ada</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-7-27" name="__codelineno-7-27" href="#__codelineno-7-27"></a>
<a id="__codelineno-7-28" name="__codelineno-7-28" href="#__codelineno-7-28"></a><span class="c1"># Analizar evoluci√≥n del error</span>
<a id="__codelineno-7-29" name="__codelineno-7-29" href="#__codelineno-7-29"></a><span class="n">train_errors</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-7-30" name="__codelineno-7-30" href="#__codelineno-7-30"></a><span class="n">test_errors</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-7-31" name="__codelineno-7-31" href="#__codelineno-7-31"></a><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">):</span>
<a id="__codelineno-7-32" name="__codelineno-7-32" href="#__codelineno-7-32"></a>    <span class="n">ada_temp</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span>
<a id="__codelineno-7-33" name="__codelineno-7-33" href="#__codelineno-7-33"></a>        <span class="n">estimator</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
<a id="__codelineno-7-34" name="__codelineno-7-34" href="#__codelineno-7-34"></a>        <span class="n">n_estimators</span><span class="o">=</span><span class="n">n</span><span class="p">,</span>
<a id="__codelineno-7-35" name="__codelineno-7-35" href="#__codelineno-7-35"></a>        <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-7-36" name="__codelineno-7-36" href="#__codelineno-7-36"></a>    <span class="p">)</span>
<a id="__codelineno-7-37" name="__codelineno-7-37" href="#__codelineno-7-37"></a>    <span class="n">ada_temp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-7-38" name="__codelineno-7-38" href="#__codelineno-7-38"></a>    <span class="n">train_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">ada_temp</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
<a id="__codelineno-7-39" name="__codelineno-7-39" href="#__codelineno-7-39"></a>    <span class="n">test_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">ada_temp</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<a id="__codelineno-7-40" name="__codelineno-7-40" href="#__codelineno-7-40"></a>
<a id="__codelineno-7-41" name="__codelineno-7-41" href="#__codelineno-7-41"></a><span class="c1"># Visualizar</span>
<a id="__codelineno-7-42" name="__codelineno-7-42" href="#__codelineno-7-42"></a><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<a id="__codelineno-7-43" name="__codelineno-7-43" href="#__codelineno-7-43"></a><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">),</span> <span class="n">train_errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Train Error'</span><span class="p">)</span>
<a id="__codelineno-7-44" name="__codelineno-7-44" href="#__codelineno-7-44"></a><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">),</span> <span class="n">test_errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Test Error'</span><span class="p">)</span>
<a id="__codelineno-7-45" name="__codelineno-7-45" href="#__codelineno-7-45"></a><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'N√∫mero de clasificadores'</span><span class="p">)</span>
<a id="__codelineno-7-46" name="__codelineno-7-46" href="#__codelineno-7-46"></a><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Error'</span><span class="p">)</span>
<a id="__codelineno-7-47" name="__codelineno-7-47" href="#__codelineno-7-47"></a><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<a id="__codelineno-7-48" name="__codelineno-7-48" href="#__codelineno-7-48"></a><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Evoluci√≥n del error en AdaBoost'</span><span class="p">)</span>
<a id="__codelineno-7-49" name="__codelineno-7-49" href="#__codelineno-7-49"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<h3 id="comparacion-adaboost-vs-bagging">Comparaci√≥n: AdaBoost vs Bagging<a class="headerlink" href="#comparacion-adaboost-vs-bagging" title="Permanent link">¬∂</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span><span class="p">,</span> <span class="n">AdaBoostClassifier</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a><span class="c1"># Bagging de stumps</span>
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a><span class="n">bagging_stumps</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span>
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a>    <span class="n">estimator</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#__codelineno-8-7"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-8-8" name="__codelineno-8-8" href="#__codelineno-8-8"></a><span class="p">)</span>
<a id="__codelineno-8-9" name="__codelineno-8-9" href="#__codelineno-8-9"></a>
<a id="__codelineno-8-10" name="__codelineno-8-10" href="#__codelineno-8-10"></a><span class="c1"># AdaBoost de stumps</span>
<a id="__codelineno-8-11" name="__codelineno-8-11" href="#__codelineno-8-11"></a><span class="n">adaboost_stumps</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span>
<a id="__codelineno-8-12" name="__codelineno-8-12" href="#__codelineno-8-12"></a>    <span class="n">estimator</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
<a id="__codelineno-8-13" name="__codelineno-8-13" href="#__codelineno-8-13"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-8-14" name="__codelineno-8-14" href="#__codelineno-8-14"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-8-15" name="__codelineno-8-15" href="#__codelineno-8-15"></a><span class="p">)</span>
<a id="__codelineno-8-16" name="__codelineno-8-16" href="#__codelineno-8-16"></a>
<a id="__codelineno-8-17" name="__codelineno-8-17" href="#__codelineno-8-17"></a><span class="n">bagging_stumps</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-8-18" name="__codelineno-8-18" href="#__codelineno-8-18"></a><span class="n">adaboost_stumps</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-8-19" name="__codelineno-8-19" href="#__codelineno-8-19"></a>
<a id="__codelineno-8-20" name="__codelineno-8-20" href="#__codelineno-8-20"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Bagging: </span><span class="si">{</span><span class="n">bagging_stumps</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-8-21" name="__codelineno-8-21" href="#__codelineno-8-21"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"AdaBoost: </span><span class="si">{</span><span class="n">adaboost_stumps</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<p><strong>Resultado t√≠pico:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a>Bagging: 0.78
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a>AdaBoost: 0.92
</code></pre></div><p></p>
<p>AdaBoost supera a Bagging con clasificadores d√©biles porque:
- Bagging reduce varianza (pero stumps ya tienen baja varianza)
- AdaBoost reduce sesgo (stumps tienen alto sesgo)</p>
<hr>
<div class="footnote">
<hr>
<ol>
<li id="fn:kearns1988cryptographic">
<p>Kearns, M., &amp; Valiant, L. G. (1988). <em>Learning Boolean formulae or finite automata is as hard as factoring</em> (Nos. TR-14-88). Harvard University Aiken Computation Laboratory.&nbsp;<a class="footnote-backref" href="#fnref:kearns1988cryptographic" title="Jump back to footnote 1 in the text">‚Ü©</a></p>
</li>
<li id="fn:kearns1989cryptographic">
<p>Kearns, M., &amp; Valiant, L. G. (1989). Cryptographic limitations on learning Boolean formulae and finite automata. <em>Proceedings of the 21st Annual ACM Symposium on Theory of Computing (STOC'89)</em>, 433--444. <a href="https://doi.org/10.1145/73007.73049">https://doi.org/10.1145/73007.73049</a>&nbsp;<a class="footnote-backref" href="#fnref:kearns1989cryptographic" title="Jump back to footnote 2 in the text">‚Ü©</a></p>
</li>
<li id="fn:schapire1990strength">
<p>Schapire, R. E. (1990). The strength of weak learnability. <em>Machine Learning</em>, <em>5</em>(2), 197--227.&nbsp;<a class="footnote-backref" href="#fnref:schapire1990strength" title="Jump back to footnote 3 in the text">‚Ü©</a></p>
</li>
<li id="fn:freund1997decision">
<p>Freund, Y., &amp; Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. <em>Journal of Computer and System Sciences</em>, <em>55</em>(1), 119--139.&nbsp;<a class="footnote-backref" href="#fnref:freund1997decision" title="Jump back to footnote 4 in the text">‚Ü©</a></p>
</li>
</ol>
</div>





                
              </article>
            </div>
          
          
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"></path></svg>
            Volver al principio
          </a>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.sections", "navigation.expand", "navigation.top", "navigation.indexes", "toc.follow", "toc.integrate", "search.suggest", "search.highlight", "content.code.copy", "content.code.annotate"], "search": "../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copiado al portapapeles", "clipboard.copy": "Copiar al portapapeles", "search.result.more.one": "1 m\u00e1s en esta p\u00e1gina", "search.result.more.other": "# m\u00e1s en esta p\u00e1gina", "search.result.none": "No se encontraron documentos", "search.result.one": "1 documento encontrado", "search.result.other": "# documentos encontrados", "search.result.placeholder": "Teclee para comenzar b\u00fasqueda", "search.result.term.missing": "Falta", "select.version": "Seleccionar versi\u00f3n"}}</script>
    
    
      <script src="../assets/javascripts/bundle.b78d2936.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>