{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introducci\u00f3n","text":""},{"location":"#aprendizaje-avanzado","title":"Aprendizaje Avanzado","text":"<p>Este material se proporciona como apuntes para la asignatura Aprendizaje Avanzado del Grado en Ingenier\u00eda en Inteligencia Artificial de la Universidad de Alicante. </p>"},{"location":"#contexto-de-la-asignatura","title":"Contexto de la asignatura","text":"<p>La asignatura Aprendizaje Avanzado forma parte de la materia Aprendizaje Autom\u00e1tico  junto con Fundamentos del Aprendizaje Autom\u00e1tico y Redes Neuronales y Aprendizaje Profundo. Se imparte en el segundo cuatrimestre del tercer curso, como continuaci\u00f3n natural de Fundamentos del Aprendizaje Autom\u00e1tico y de forma paralela con Redes Neuronales y Aprendizaje Profundo.</p> <p>Una vez establecidas las bases del aprendizaje autom\u00e1tico en la asignatura de fundamentos, Aprendizaje Avanzado se centra en proporcionar una visi\u00f3n amplia de los diferentes tipos de modelos existentes, buscando adquirir la capacidad de seleccionar, integrar, optimizar y evaluar m\u00faltiples t\u00e9cnicas para resolver problemas complejos del mundo real.</p> <p>Se recomienda haber cursado previamente Fundamentos del Aprendizaje Autom\u00e1tico.</p>"},{"location":"#planificacion","title":"Planificaci\u00f3n","text":"<p>La asignatura se estructura en cuatro grandes bloques:</p> <ul> <li>Bloque I. Aprendizaje supervisado</li> <li>Bloque II. Aprendizaje no supervisado</li> <li>Bloque III. Aprendizaje por refuerzo</li> <li>Bloque IV. Aprendizaje aplicado a problemas del mundo real</li> </ul> <p>A continuaci\u00f3n se detalla la planificaci\u00f3n de sesiones de la asignatura:</p> Fecha Teor\u00eda Pr\u00e1cticas S1 (26 de enero) I.1 Modelos param\u00e9tricos y no param\u00e9tricos. Modelos lineales. Regresi\u00f3n Log\u00edstica. P0. Datos y visualizaci\u00f3n S2 (3 de febrero) I.2 SVM y kernel trick P0. Datos y visualizaci\u00f3n S3 (10 de febrero) * Charla mes cultural EPS P0. Aprendizaje supervisado S4 (17 de febrero) I.3 \u00c1rboles de decisi\u00f3n P1. Aprendizaje supervisado S5 (24 de febrero) I.4 M\u00e9todos de ensamble. Bagging. Random Forest P1. Aprendizaje supervisado S6 (3 de marzo) I.5 Boosting. AdaBoost. Control 1 P1. Aprendizaje supervisado S7 (10 de marzo) I.6 Gradient Boosting. XGBoost P1. Aprendizaje supervisado S8 (17 de marzo) II.7 Clustering. GMM y EM P2. Aprendizaje no supervisado S9 (24 de marzo) II.8 DBSCAN P2. Aprendizaje no supervisado S10 (31 de marzo) II.9 Clustering espectral. Control 2 P2. Aprendizaje no supervisado S11 (21 de abril) III.10 Aprendizaje por refuerzo P3. Aprendizaje por refuerzo S12 (28 de abril) III.11 Aprendizaje por refuerzo P3. Aprendizaje por refuerzo S13 (5 de mayo) IV.12 Selecci\u00f3n y optimizaci\u00f3n de modelos P3. Aprendizaje por refuerzo S14 (12 de mayo) IV.13 Aplicaci\u00f3n a problemas del mundo real. Control 3 P3. Aprendizaje por refuerzo S15 (19 de mayo) Presentaci\u00f3n de proyectos Presentaci\u00f3n de proyectos"},{"location":"#licencia","title":"Licencia","text":"<p>\u00a9 2026 Departamento de Ciencia de la Computaci\u00f3n e Inteligencia Artificial. Universidad de Alicante</p> <p> </p> <p>Este material est\u00e1 disponible bajo licencia Creative Commons Attribution 4.0 International.  Esto significa que puedes usar, compartir y adaptar estos apuntes libremente, siempre que des el cr\u00e9dito apropiado.</p>"},{"location":"01-modelos-no-param/","title":"1. Modelos param\u00e9tricos y no param\u00e9tricos. Logistic Regression","text":""},{"location":"01-modelos-no-param/#sesion-1-modelos-parametricos-y-no-parametricos","title":"Sesi\u00f3n 1: Modelos param\u00e9tricos y no param\u00e9tricos","text":"<p>Los modelos no param\u00e9tricos, a diferencia de los modelos param\u00e9tricos, son una familia de modelos que no asumen una forma funcional fija. Una implicaci\u00f3n importante de este hecho es que la complejidad del modelo puede crecer en funci\u00f3n del conjunto de datos. </p> <p>En un modelo param\u00e9trico se define de antemano la forma funcional, que podr\u00eda ser por ejemplo lineal o cuadr\u00e1tica, y tendremos que estimar un n\u00famero fijo de par\u00e1metros, pero si la estructura de los datos es m\u00e1s compleja y no conocemos su forma, puede que no puedan ajustarse de forma adecuada al modelo.</p> <p>Los modelos no param\u00e9tricos no imponen una estructura fija, sino que permiten que sean los propios datos los que determinen la complejidad del modelo. </p> <p>Desde el punto de vista del compromiso (trade-off) entre sesgo (bias) y varianza, podemos intuir que los modelos param\u00e9tricos tender\u00e1n a tener un mayor sesgo, si el modelo no es lo suficientemente complejo como para poder representar los datos (riesgo de underfitting), mientras que los modelos no param\u00e9tricos tender\u00e1n a tener menor sesgo pero mayor varianza (riesgo de overfitting), siendo sensibles a los cambios en los datos de entrenamiento.</p> <p>Vamos a ilustrarlo a continuaci\u00f3n mediante un clasificador param\u00e9trico sencillo.</p>"},{"location":"01-modelos-no-param/#modelos-lineales","title":"Modelos lineales","text":"<p>Una familia caracter\u00edstica de modelos param\u00e9tricos son los modelos lineales, que son aquellos que asumen una relaci\u00f3n lineal entre los datos de entrada y la variable objetivo.</p> <p>El modelo lineal predice la salida como una combinaci\u00f3n lineal ponderada de las variables de entrada, con la siguiente forma:</p> \\[ \\hat{y} = w_1 x_1 + w_2 x_2 + \\ldots + w_d x_d = \\mathbf{x}^T \\mathbf{w} + b \\] <p>Donde \\(\\mathbf{x}\\) es el vector de caracter\u00edsticas de entrada, \\(\\mathbf{w}\\) es el vector de coeficientes que el modelo aprende, \\(b\\) es el sesgo o desplazamiento y \\(\\hat{y}\\) la predicci\u00f3n que nos da el modelo. </p> <p>Desde el punto de vista geom\u00e9trico, esta ecuaci\u00f3n define un hiperplano. Con esto, podemos dar la siguiente interpretaci\u00f3n al funcionamiento de estos modelos:</p> <ul> <li>Tareas de regresi\u00f3n: Se busca el hiperplano que mejor se ajusta a los datos de entrada.</li> <li>Tareas de clasificaci\u00f3n: Se busca el hiperplano que mejor separa los datos de dos clases. </li> </ul> <p>Vamos a centrarnos a continuaci\u00f3n en la tarea de clasificaci\u00f3n, y estudiaremos dos de los principales m\u00e9todos lineales para clasificaci\u00f3n: Regresi\u00f3n Log\u00edstica y SVM (Hastie et al., 2009)1. </p>"},{"location":"01-modelos-no-param/#hiperplanos-y-clasificacion","title":"Hiperplanos y clasificaci\u00f3n","text":"<p>En un problema de clasificaci\u00f3n, si contamos con datos linealmente separables podremos encontrar un hiperplano que nos permita clasificarlos sin errores. El hiperplano tendr\u00e1 la siguiente forma:</p> \\[ \\mathbf{x^T} \\mathbf{w} + b = 0 \\] <p>Donde sus par\u00e1metros son \\(\\mathbf{w}\\), que representa un vector perpendicular al hiperplano, y \\(b\\), como t\u00e9rmino de desplazamiento. </p> <p>Para facilitar la visualizaci\u00f3n, vamos a considerar el caso concreto de dos dimensiones, donde el hiperplano anterior ser\u00eda una l\u00ednea, con la siguiente ecuaci\u00f3n:</p> \\[ w_1 x_1 + w_2 x_2 + b = 0 \\ \\] <p>Por ejemplo, si consideramos como par\u00e1metros el vector \\(\\mathbf{w} = [0.45, 0.89]\\) y \\(b = -2\\), tendremos la siguiente recta:</p> <p>Figura 1: Ejemplo de hiperplano para separar dos conjuntos de datos </p> <p>En la  podemos observar que si el vector \\(\\mathbf{w}\\) es unitario, como en el ejemplo anterior, entonces el t\u00e9rmino \\(b\\) coincide con la distancia del plano al origen. En el caso general en el que \\(\\mathbf{w}\\) no es unitario, la distancia al origen ser\u00e1 \\(|b| / \\lVert \\mathbf{w} \\rVert\\).  </p> <p>La ecuaci\u00f3n del hiperplano nos proporciona una forma sencilla de clasificar los puntos seg\u00fan se encuentren a uno u otro lado, tomando dicha ecuaci\u00f3n como funci\u00f3n:</p> \\[ f(\\mathbf{\\mathbf{x}}) = \\mathbf{x^T} \\mathbf{w} + b \\] <p>En la funci\u00f3n \\(f(\\mathbf{x})\\), todos los puntos que pertenezcan al hiperplano nos dar\u00e1n \\(f(\\mathbf{x})=0\\) (cumplir\u00edan la ecuaci\u00f3n del hiperplano), pero lo que realmente nos interesa es que todos los puntos que est\u00e9n al lado al que apunta el vector \\(\\mathbf{w}\\) har\u00e1n que \\(f(\\mathbf{x})\\) tenga signo positivo, mientras que los que est\u00e9n al lado contrario har\u00e1n que tenga signo negativo.  Es decir, podemos utilizar el signo de dicha funci\u00f3n como clasificador:</p> \\[ G(\\mathbf{x}) = signo[f(\\mathbf{x})] \\] <p>De esta forma, diferentes puntos de datos quedar\u00edan clasificados tal como se muestra en la </p> <p>Figura 2: Ejemplo de clasificaci\u00f3n con un hiperplano </p> <p>Adem\u00e1s, en caso de que \\(\\mathbf{w}\\) sea unitario, la funci\u00f3n \\(f(\\mathbf{\\mathbf{x}})\\) nos dar\u00e1 la distancia (con signo) desde cada punto al plano. Podemos ver esto de forma intuitiva, considerando que el producto escalar \\(\\mathbf{x^T} \\mathbf{w}\\) nos da la proyecci\u00f3n de \\(\\mathbf{x}\\) sobre el vector \\(\\mathbf{w}\\) perpendicular al hiperplano. Esto nos dar\u00e1 la distancia al hiperplano si pasara por el origen, y el t\u00e9rmino \\(b\\) introduce un desplazamiento.</p> <p>Existen diferentes m\u00e9todos que nos permiten aprender, a partir de un conjunto de datos, un hiperplano que separe los datos de dos clases. </p>"},{"location":"01-modelos-no-param/#regresion-logistica","title":"Regresi\u00f3n Log\u00edstica","text":"<p>Aunque el nombre pueda resultar confuso, se trata de un m\u00e9todo de clasificaci\u00f3n, y no de regresi\u00f3n. Si bien con el m\u00e9todo de regresi\u00f3n lineal se busca ajustar un hiperplano a un conjunto de puntos, de forma que se minimice la distancia entre los puntos del conjunto de entrada y el hiperplano, en el caso de la regresi\u00f3n log\u00edstica buscamos el hiperplano que mejor separe dos clases de datos. Hablamos en este caso de regresi\u00f3n log\u00edstica binomial, en la que contamos \u00fanicamente con dos clases, aunque tambi\u00e9n podr\u00edamos extender este m\u00e9todo a un mayor n\u00famero de clases, hablando en este caso de regresi\u00f3n log\u00edstica multinomial. </p> <p>Vamos a centrarnos de momento por simplicidad en el caso binomial, en el que la salida \\(y\\) podr\u00e1 tomar dos posibles valores \\(\\{0, 1\\}\\). Los ejemplos ser\u00e1n clasificados en una clase u otra seg\u00fan el lado del hiperplano en el que se sit\u00faen.   </p> <p>Como hemos visto anteriormente, a partir de la ecuaci\u00f3n del hiperplano podemos determinar si un punto est\u00e1 a uno u otro lado a partir del signo de la funci\u00f3n \\(f(x)\\), siendo positiva (clase \\(1\\)) en el lado hacia el que apunta el vector \\(\\mathbf{w}\\), y negativa (clase \\(0\\)) en el otro lado. </p> <p>Este modelo destaca por su interpretabilidad, y es ampliamente utilizado como modelo base en numerosos problemas de clasificaci\u00f3n. </p> <p>La clave principal de la regresi\u00f3n logistica consiste en aplicar sobre la funci\u00f3n anterior la funci\u00f3n sigmoide, modelando mediante esta funci\u00f3n la probabilidad de pertenencia a cada clase.</p>"},{"location":"01-modelos-no-param/#funcion-sigmoide","title":"Funci\u00f3n sigmoide","text":"<p>La funci\u00f3n sigmoide \\(\\sigma(z)\\) tiene la siguiente forma:</p> \\[ \\sigma(z) = \\frac{1}{1+ e^{-z}} \\] <p>Podemos verla representada en la .</p> <p>Figura 3: Forma de la funci\u00f3n sigmoide </p> <p>Como podemos observar, esta funci\u00f3n presenta una transici\u00f3n suave desde \\(0\\) (cuando \\(z \\rightarrow -\\infty\\)) hasta \\(1\\) (cuando \\(z \\rightarrow \\infty\\)), teniendo su punto medio en \\(\\sigma(0) = 0.5\\). </p> <p>La funci\u00f3n es siempre creciente y derivable, lo cual es una propiedad importante para la optimizaci\u00f3n.</p> <p>La funci\u00f3n sigmoide tiene la siguiente derivada:</p> \\[ \\sigma'(z) = \\sigma(z) (1 - \\sigma(z)) \\] <p>Esta forma simplifica mucho el c\u00e1lculo del gradiente.</p> <p>Podemos sustituir \\(z\\) por nuestra funci\u00f3n \\(f(\\mathbf{x})\\) que nos permite clasificar los puntos en funci\u00f3n del signo, teniendo:</p> \\[ \\sigma(f(\\mathbf{x})) =  \\frac{1}{1 + e^{-f(\\mathbf{x})}} = \\frac{1}{1 + e^{-(\\mathbf{x}^T \\mathbf{w} + b)}} \\] <p>Podemos interpretar la funci\u00f3n anterior como la probabilidad estimada de que \\(y = 1\\) (es decir, de que pertenezca a la clase positiva). Definimos de esta forma:</p> \\[ p_i = \\sigma(f(\\mathbf{x}_i)) = \\frac{1}{1 + e^{-(\\mathbf{x}_i^T \\mathbf{w} + b)}} \\] <p>Donde \\(p_i\\) define la probabilidad estimada de que el ejemplo \\(i\\) pertenezca a la clase \\(1\\). Tendremos por lo tanto:</p> \\[ \\begin{align*} P(y_i=1 | \\mathbf{x}_i) &amp;= p_i \\\\ P(y_i=0 | \\mathbf{x}_i) &amp;= 1 - p_i  \\end{align*} \\] <p>Considerando estos dos posibles valores para \\(y_i\\), podemos escribir ambos casos en una \u00fanica f\u00f3rmula, teniendo as\u00ed la verosimilitud de observar \\(y_i\\) cuando la entrada es \\(\\mathbf{x}_i\\):</p> \\[ P(y_i | \\mathbf{x}_i) = (p_i)^y (1 - p_i)^{1-y}  \\] <p>Estimaremos los par\u00e1metros mediante m\u00e1xima verosimilitud, lo cual equivale a minimizar la p\u00e9rdida logar\u00edtmica (log-loss).</p>"},{"location":"01-modelos-no-param/#funcion-de-coste","title":"Funci\u00f3n de coste","text":"<p>La funci\u00f3n de p\u00e9rdida logar\u00edtmica (log-loss) o binary cross-entropy para una sola muestra tiene la siguiente forma:</p> \\[ L(p_i, y_i) = -y_i \\log (p_i) - (1-y_i) log(1-p_i) \\] <p>Esta funci\u00f3n tiene la propiedad de que penaliza fuertemente las predicciones confiadas pero incorrectas. Es decir, si la salida esperada es \\(y_i=1\\) pero la predicci\u00f3n \\(p_i \\rightarrow 0\\), entonces la penalizaci\u00f3n ser\u00e1 alta.</p> <p>Consideremos ahora que tenemos un conjunto de entrenamiento con \\(N\\) pares \\((\\mathbf{x_i}, y_i)\\) con \\(\\mathbf{x_i} \\in \\mathbb{R}^d\\) y \\(y_i \\in \\{0, 1\\}\\) (problema binomial), siendo \\(d\\) el n\u00famero de features.</p> <p>Con todo ello, para el conjunto de muestras podemos construir la siguiente funci\u00f3n de coste:</p> \\[ J(\\mathbf{w}) = -\\frac{1}{N} \\sum_{i=1}^N [y_i \\log (p_i) + (1-y_i) \\log (1 - p_i)] \\] <p>Esta funci\u00f3n tiene la propiedad de que es convexa (tiene un \u00fanico m\u00ednimo global) y diferenciable, y como hemos comentado, penaliza las predicciones claramente incorrectas.</p>"},{"location":"01-modelos-no-param/#optimizacion","title":"Optimizaci\u00f3n","text":"<p>Buscamos encontrar los pesos \\(\\mathbf{w}\\) que minimicen la funci\u00f3n de coste anterior. </p> \\[ \\mathbf{\\hat{w}} = \\arg \\min_{\\mathbf{w}} J(\\mathbf{w})  \\] <p>Obtenemos el gradiente de la funci\u00f3n, mediante la derivada parcial respecto a cada peso \\(w_j\\):</p> \\[ \\frac{\\partial J(\\mathbf{w})}{\\partial w_j} = \\frac{1}{N} \\sum_{i=1}^N (p_i - y_i) x_{ij} \\] <p>Podemos expresar el gradiente en forma vectorial, para todos los pesos, de la siguiente forma:</p> \\[ \\nabla J(\\mathbf{w}) = \\frac{1}{N} \\mathbf{X}^T (\\mathbf{p} - \\mathbf{y})  \\] <p>Donde \\(\\mathbf{X}\\) es una matriz de dimensi\u00f3n \\(N \\times d\\) (una fila para cada ejemplo de entrada), \\(\\mathbf{p}\\) es un vector de predicciones (\\(p_i\\)) de dimensi\u00f3n \\(N \\times 1\\) y \\(\\mathbf{y}\\) es un vector de etiquetas de dimensi\u00f3n \\(N \\times 1\\). </p> <p>De la misma forma, podemos obtener la derivada parcial respecto al sesgo (\\(b\\)):</p> \\[ \\frac{\\partial J(\\mathbf{w})}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^N (p_i - y_i) \\] <p>Con esto, podremos aplicar Descenso por Gradiente o Descenso por Gradiente estoc\u00e1stico (SGD) para optimizar los pesos. Tambi\u00e9n tenemos otros algoritmos de optimizaci\u00f3n como Coordinate Descent (Hsieh et al., 2008)2, en el que en lugar de aplicar descenso por gradiente a la vez sobre todas las coordenadas, se selecciona de forma iterativa una coordenada, se congela el resto, y se optimiza para la coordenada seleccionada. El algoritmo itera por las diferentes coordenadas hasta la convergencia. Encontramos tambi\u00e9n otros m\u00e9todos de optimizaci\u00f3n, como el m\u00e9todo de Newton (Nocedal &amp; Wright, 2006)3 que utiliza segundas derivadas y presenta la ventaja de una convergencia m\u00e1s r\u00e1pida, aunque resulta algo costoso. Tenemos tambi\u00e9n L-BFGS (Liu &amp; Nocedal, 1989)4 que es una aproximaci\u00f3n eficiente del m\u00e9todo de Newton y es el utilizado por defecto en la implementaci\u00f3n de <code>LogisticRegression</code> en sklearn. Esta implementaci\u00f3n  incluye diferentes solvers alternativos que podemos utilizar para la optimizaci\u00f3n. </p>"},{"location":"01-modelos-no-param/#regularizacion","title":"Regularizaci\u00f3n","text":"<p>Podemos a\u00f1adir a la funci\u00f3n de coste un t\u00e9rmino de penalizaci\u00f3n para prevenir el overfitting y controlar la complejidad del modelo. Encontramos diferentes tipos de regularizaci\u00f3n:</p>"},{"location":"01-modelos-no-param/#regularizacion-l2-ridge","title":"Regularizaci\u00f3n L2 (Ridge)","text":"<p>Busca penalizar pesos grandes, para favorecer soluciones m\u00e1s simples:</p> \\[ \\begin{align*} J(\\mathbf{w}) = &amp;-\\frac{1}{N} \\sum_{i=1}^N [y_i \\log (p_i) + (1-y_i) \\log (1 - p_i)] + \\\\ &amp; + \\frac{\\lambda}{2N} \\sum_{j=1}^d w_j^2 \\end{align*} \\]"},{"location":"01-modelos-no-param/#regularizacion-l1-lasso","title":"Regularizaci\u00f3n L1 (Lasso)","text":"<p>Favorece que algunos pesos puedan ser exactamente \\(0\\), actuando de esta forma como una selecci\u00f3n de caracter\u00edsticas:</p> \\[ \\begin{align*} J(\\mathbf{w}) = &amp;-\\frac{1}{N} \\sum_{i=1}^N [y_i \\log (p_i) + (1-y_i) \\log (1 - p_i)] + \\\\ &amp; + \\frac{\\lambda}{N} \\sum_{j=1}^d | w_j | \\end{align*} \\]"},{"location":"01-modelos-no-param/#regularizacion-elastic-net","title":"Regularizaci\u00f3n Elastic Net","text":"<p>Combina L1 y L2:</p> \\[ \\begin{align*} J(\\mathbf{w}) = &amp;-\\frac{1}{N} \\sum_{i=1}^N [y_i \\log (p_i) + (1-y_i) \\log (1 - p_i)] + \\\\ &amp; + \\frac{\\lambda_1}{N} \\sum_{j=1}^d | w_j | + \\frac{\\lambda_2}{2N} \\sum_{k=1}^d w_k^2  \\end{align*} \\] <p>En todos estos casos tenemos un hiper-par\u00e1metro \\(\\lambda\\) con el que podemos ajustar la regularizaci\u00f3n. Con \\(\\lambda = 0\\) no aplicamos regularizaci\u00f3n, con lo que tendremos mayor riesgo de overfitting, mientras que con valores muy altos podr\u00edamos tener mayor riesgo de underfitting. </p>"},{"location":"01-modelos-no-param/#clasificacion-multi-clase","title":"Clasificaci\u00f3n multi-clase","text":"<p>Hemos visto hasta ahora el caso binomial (2 clases), pero como hemos comentado, podemos aplicar Regresi\u00f3n Log\u00edstica tambi\u00e9n a problemas de clasificaci\u00f3n multi-clase. </p> <p>Consideramos ahora que debemos clasificar los ejemplos de entrada en \\(K\\) clases. Tenemos dos formas para hacer esto:</p> <ul> <li>One-vs-Rest (OvR): Creamos \\(K\\) clasificadores binarios, uno para cada clase.</li> <li>Multinomial: Se entrana un \u00fanico modelo que modela la distribuci\u00f3n de probabilidad sobre todas las clases. </li> </ul> <p>Vamos a ver cada uno de estos casos.</p>"},{"location":"01-modelos-no-param/#one-vs-rest","title":"One-vs-Rest","text":"<p>Podemos aplicar cualquier clasificador binario a problemas multiclase utilizando la estrategia One-vs-Rest (OvR). </p> <p>Esta estrategia consiste en crear \\(K\\) clasificadores binarios, uno para cada clase. Cada clasificador binario \\(k\\), con \\(k = 1, 2, \\ldots, K\\), clasifica los ejemplos en dos categor\u00edas: los que pertenecen a la clase \\(k\\) y los que pertenecen a cualquier de las otras clases.</p> <p>Para la predicci\u00f3n, se calcula la probabilidad de pertenencia con cada uno de los clasificadores \\(k = 1, 2, \\ldots, K\\). Cada clasificador \\(k\\) nos dar\u00e1 la probabilidad de que el ejemplo pertenezca a la correspondiente clase \\(k\\):</p> \\[ P(y=k | \\mathbf{x}) = \\frac{1}{1 + e^{-(\\mathbf{x}^T \\mathbf{w}_k + b_k)}} \\] <p>Aquel que obtenga una mayor probabilidad ser\u00e1 la clase seleccionada como predicci\u00f3n:</p> \\[ \\hat{k} = \\arg \\max_k P(y=k | \\mathbf{x}) \\] <p>En sklearn podemos utilizar este enfoque utilizando la clase OneVsRestClassifier. Podemos aplicar este m\u00e9todo a cualquier clasificador binario. Tambi\u00e9n encontramos OneVsOneClassifier que entrena un clasificador para cada par de clases. Aunque One-vs-One (OvO) es m\u00e1s robusto frente a desbalanceo que OvR, requiere entrenar \\(K(K-1)/2\\) clasificadores, lo que puede ser prohibitivo cuando \\(K\\) es grande. En OvO, cada clasificador votar\u00e1 por una clase, y la clase que reciba m\u00e1s votos ser\u00e1 la seleccionada.</p> <p>Aunque OvR es un modelo sencillo, f\u00e1cil de implementar y r\u00e1pido de entrenar, tiene una serie de desventajas. Encontramos en primer lugar que los datos de entrenamiento de cada clasificador est\u00e1n fuertemente desbalanceados, ya que hay muchos menos ejemplos de la clase \\(k\\) correspondiente al clasificador que del resto de clases.</p> <p>Por otro lado, este enfoque no nos da probabilidades bien calibradas: los scores de los \\(K\\) clasificadores no suman necesariamente 1, ya que cada clasificador se entrena de forma independiente. Si necesitamos contar con probabilidades bien calibradas, podemos utilizar el enfoque de clasificaci\u00f3n multinomial.</p>"},{"location":"01-modelos-no-param/#multinomial","title":"Multinomial","text":"<p>En este caso, las probabilidades de pertenencia a cada clase en lugar de modelarse con una funci\u00f3n sigmoide se modelan mediante la funci\u00f3n softmax:</p> \\[ P(y=k | \\mathbf{x}) = \\frac{ e^{(\\mathbf{x}^T \\mathbf{w}_k + b_k)} } { \\sum_{j=1}^K e^{(\\mathbf{x}^T \\mathbf{w}_j + b_j)}} \\] <p>Con esto garantizamos que las probabilidades est\u00e9n entre \\(0\\) y \\(1\\) y que todas ellas sumen exactamente \\(1\\):</p> <ul> <li>\\(P(y = k | \\mathbf{x}) \\in  [0, 1]\\)</li> <li>\\(\\sum_{k=1}^{K} P(y = k | \\mathbf{x}) = 1\\) </li> </ul> <p>En este caso la funci\u00f3n a optimizar ser\u00e1 cross-entropy multiclase (o categorical cross-entropy). Este enfoque modela las relaciones entre clases y nos proporciona probabilidades bien calibradas, aunque es m\u00e1s costoso que el anterior.</p> <p>Si tenemos un gran n\u00famero de clases y necesitamos m\u00e1s velocidad, puede ser conveniente utilizar el enfoque OvR, mientras que si el n\u00famero de clases es menor, o necesitamos probabilidades calibradas ser\u00e1 m\u00e1s apropiado el m\u00e9todo multinomial. En la implementaci\u00f3n <code>LogisticRegression</code> de sklearn, todos los solvers utilizar\u00e1n el enfoque multinomial en problemas multi-clase excepto <code>liblinear</code>, que solo soporta clasificaci\u00f3n binaria. En este \u00faltimo caso, si queremos aplicarlo a un problema multiclase, deberemos utilizar OvR. </p>"},{"location":"01-modelos-no-param/#limitaciones-de-los-modelos-lineales","title":"Limitaciones de los modelos lineales","text":"<p>Como hemos visto, los modelos lineales como regresi\u00f3n log\u00edstica buscan el hiperplano que mejor separe los datos de las diferentes clases. Sin embargo, esto no siempre ser\u00e1 posible.</p>"},{"location":"01-modelos-no-param/#regresion-logistica-con-datos-linealmente-separables","title":"Regresi\u00f3n log\u00edstica con datos linealmente separables","text":"<p>En caso de tener datos linealmente separables, como los que se muestran en la  , existir\u00e1 un hiperplano que los separe. En este caso, un clasificador lineal con solo 3 par\u00e1metros (\\(w_1, w_2, b\\)) ser\u00e1 suficiente para representar los datos.</p> <p>Figura 4: Conjunto de datos linealmente separables </p> <p>Con esta distribuci\u00f3n de los datos, incluso si apareciese alg\u00fan solape entre las dos clases o alg\u00fan outlier, seguir\u00eda siendo posible encontrar un hiperplano que nos permita clasificarlos con una alta precisi\u00f3n, y solo un peque\u00f1o porcentaje de errores (ver ).</p> <p>Figura 5: Conjunto de datos separables con solape </p> <p>Podemos adem\u00e1s ver en la  el mapa de probabilidades que nos proporciona el modelo de regresi\u00f3n log\u00edstica.</p> <p>Figura 6: Mapa de probabilidades </p>"},{"location":"01-modelos-no-param/#regresion-logistica-con-datos-linealmente-no-separables","title":"Regresi\u00f3n log\u00edstica con datos linealmente no separables","text":"<p>Sin embargo, si la distribuci\u00f3n de los datos cambiase, y pasaran a ser no separables, como los que se muestran en la , el modelo anterior no ser\u00eda suficiente para representarlos.</p> <p>Figura 7: Conjunto de datos linealmente no separables </p> <p>En este caso los datos siguen un patr\u00f3n conocido como XOR, distribuido en \\(4\\) cuadrantes, y no es posible encontrar ning\u00fan hiperplano que los separe. Cualquier hiperplano nos dar\u00e1 siempre un error aproximado del \\(50\\%\\), que equivale a lanzar una moneda al aire para predecir la clase, tal como se ve en la figura anterior. </p> <p>Vemos en este caso como el no poder adaptar la complejidad del modelo a los datos (estar\u00edamos limitados a \\(3\\) par\u00e1metros \\(w_1, w_2, b\\)) hace que el modelo no pueda ajustarse de forma adecuada.</p>"},{"location":"01-modelos-no-param/#posibles-soluciones","title":"Posibles soluciones","text":"<p>Para clasificar los puntos del \u00faltimo ejemplo minimizando el error de clasificaci\u00f3n, podr\u00edamos optar por:</p> <ul> <li> <p>Clasificador lineal con ingenier\u00eda de caracter\u00edsticas: Crear manualmente caracter\u00edsticas no lineales (por ejemplo \\(x_1^2,x_2^2,x_1 \\cdot x_2\\)) y usar un modelo lineal sobre ellas. Esto requiere tener conocimiento del dominio, para determinar qu\u00e9 caracter\u00edsticas necesitar\u00edamos para que la funci\u00f3n pueda ajustarse a nuestros datos.</p> </li> <li> <p>Clasificador param\u00e9trico no lineal: Por ejemplo, una red neuronal con n\u00famero fijo de par\u00e1metros. Necesitaremos determinar la arquitectura m\u00e1s adecuada.</p> </li> <li> <p>Clasificador no param\u00e9trico: En este caso no ser\u00e1 necesario conocer previamente la estructura de los datos (su forma funcional), sino que el modelo se adaptar\u00e1 autom\u00e1ticamente a su complejidad.</p> </li> </ul> <p>Veremos a continuaci\u00f3n ejemplos de cada una de estas soluciones.</p>"},{"location":"01-modelos-no-param/#ingenieria-de-caracteristicas","title":"Ingenier\u00eda de caracter\u00edsticas","text":"<p>Vamos en primer lugar a ver c\u00f3mo podr\u00edamos utilizar ingenier\u00eda de caracter\u00edsticas para separar el conjunto de datos anterior (ejemplo XOR). </p> <p>Al estar en dos dimensiones contamos con las features \\(x_1\\) y \\(x_2\\). Lo que vamos a hacer es a\u00f1adir adem\u00e1s las features \\(x_1^2\\), \\(x_1 x_2\\) y \\(x_2^2\\). N\u00f3tese que se trata de features derivadas de las originales. Esto nos va a permitir definir una frontera curva entre los datos, donde el papel de la caracter\u00edstica \\(x_1 x_2\\) ser\u00e1 fundamental, ya que seg\u00fan su signo podremos inferir la clase en una operaci\u00f3n XOR. Podemos ver esto ilustrado en la .</p> <p>Figura 8: Ingenier\u00eda de caracter\u00edsticas </p> <p>Si aplicamos a este conjunto de datos el modelo de regresi\u00f3n log\u00edstica introduciendo las features indicadas anteriormente, obtenemos la frontera de clasificaci\u00f3n que se muestra en la .</p> <p>Figura 9: Frontera de decisi\u00f3n con ingenier\u00eda de caracter\u00edsticas </p> <p>Lo que hemos hecho ha sido crear \\(M\\)  caracter\u00edsticas de entrada \\(h(\\mathbf{x}) = (h_1(\\mathbf{x}), h_2(\\mathbf{x}), \\ldots, h_M(\\mathbf{x}))\\) a partir de las caracter\u00edsticas originales \\(\\mathbf{x}\\). Es decir, con \\(h(\\mathbf{x})\\) estamos proyectando las caracter\u00edsticas originales en un nuevo espacio de caracter\u00edsticas. Con esto, el hiperplano de separaci\u00f3n quedar\u00eda de la siguiente forma:</p> \\[ f(\\mathbf{x}) = h(\\mathbf{x})^T \\mathbf{w} + b \\] <p>Podemos observar que aunque el modelo sigue siendo lineal respecto a las caracter\u00edsticas proyectadas \\(h(\\mathbf{x})\\), puede que ya no lo sea en el espacio original de caracter\u00edsticas de \\(\\mathbf{x}\\). Esto es lo que ocurre en el caso del ejemplo anterior, en el que las caracter\u00edsticas proyectadas son \\(h(\\mathbf{x}) = (x_1, x_2, x_1 x_2, x_1^2, x_2^2)\\), y por lo tanto tenemos un polinomio de grado 2 en el espacio original.</p>"},{"location":"01-modelos-no-param/#modelos-parametricos-no-lineales","title":"Modelos param\u00e9tricos no lineales","text":"<p>Un tipo destacado modelo param\u00e9trico no lineal son las Redes Neuronales. Vamos a establecer la relaci\u00f3n de la Regresi\u00f3n Log\u00edstica con las Redes Neuronales y a estudiar como estos modelos pueden resolver problemas no lineales como el planteado.</p> <p>Es f\u00e1cil determinar que el modelo de regresi\u00f3n log\u00edstica binomial es equivalente a un perceptr\u00f3n con \\(d\\) entradas (tantas como features) que aplique una funci\u00f3n sigmoide como funci\u00f3n de activaci\u00f3n (ver ) y funci\u00f3n de p\u00e9rdida binary cross-entropy. </p> <p>Figura 10: Perceptr\u00f3n con \\(d\\) entradas y funci\u00f3n de activaci\u00f3n Sigmoide </p> <p>En este caso, la salida de la neurona ser\u00eda:</p> \\[ y = \\sigma(\\sum_{i=1}^d w_i x_i + b) \\] <p>Donde \\(x_i\\) son las entradas, \\(w_b\\) los pesos para cada entrada y \\(b\\) el sesgo o bias. </p> <p>De la misma forma, un modelo de regresi\u00f3n log\u00edstica multinomial ser\u00eda equivalente a una red con una \u00fanica capa softmax.</p> <p>Cuando a la red le a\u00f1adimos varias capas ocultas (ver ), lo que estaremos haciendo es aprender a transformar el espacio original de caracter\u00edsticas \\(\\mathbf{X}\\) en un espacio latente \\(\\mathbf{H}\\) que facilite su clasificaci\u00f3n. Podremos encontrar estas caracter\u00edsticas en la \u00faltima capa de la red, y sobre ellas se aplicar\u00e1 una clasificaci\u00f3n equivalente a la regresi\u00f3n log\u00edstica.</p> <p>Figura 11: Red neuronal profunda con neurona de salida Sigmoidea </p> <p>Esta transformaci\u00f3n del espacio de caracter\u00edsticas nos podr\u00eda permitir mapear nuestros datos de entrada no linealmente separables sobre un espacio en el que si que lo sean. En este caso tenemos un modelo param\u00e9trico pero no lineal. </p> <p>Cabe destacar que aplicando ingenier\u00eda de caracter\u00edsticas debemos dise\u00f1ar manualmente las nuevas caracter\u00edsticas que derivamos del conjunto original, por lo que es necesario tener conocimiento sobre la forma funcional de los datos. Sin embargo, en este caso es la propia red quien aprende de forma autom\u00e1tica las caracter\u00edsticas m\u00e1s adecuadas para resolver el problema.</p>"},{"location":"01-modelos-no-param/#modelos-no-parametricos","title":"Modelos no param\u00e9tricos","text":"<p>Nuestra tercera opci\u00f3n es el uso de modelos no param\u00e9tricos. Estos modelos cuentan con la ventaja de que su complejidad se adapta a los datos. </p> <p>Dentro de este grupo encontramos por ejemplo modelos basados en vecindad como K-NN, que son capaces de adaptarse sin problema al problema XOR anterior y a formas m\u00e1s complejas. En la  vemos la frontera de decisi\u00f3n obtenida cuando aplicamos K-NN al conjunto de datos que sigue el patr\u00f3n XOR.</p> <p>Figura 12: Aplicaci\u00f3n de K-NN al problema XOR </p> <p>En la pr\u00f3xima sesi\u00f3n veremos el modelo SVM, que originalmente se plantea como un modelo param\u00e9trico lineal, pero que puede transformarse en un modelo no param\u00e9trico mediante el conocido como kernel trick. </p> <p>Encontramos muchos otros modelos no param\u00e9tricos, parte de los cuales enumeramos a continuaci\u00f3n:</p> <ul> <li> <p>M\u00e9todos basados en vecindad. Encontramos K-NN tanto para clasificaci\u00f3n como para regresi\u00f3n, as\u00ed como variantes como K-NN ponderado por distancia, as\u00ed como m\u00e9todos de estimaci\u00f3n de densidad como Kernel Density Estimation (KDE)</p> </li> <li> <p>M\u00e9todos de kernel, como SVM y sus variantes. </p> </li> <li> <p>M\u00e9todos basados en \u00e1rboles. Encontramos los \u00e1rboles de decisi\u00f3n, que nos permiten abordar problemas de clasificaci\u00f3n y regresi\u00f3n, y m\u00e9todos de ensemble que combinan diferentes clasificadores \"d\u00e9biles\" para construir un clasificador \"fuerte\". Dentro de este \u00faltimo subgrupo, encontramos m\u00e9todos como Random Forest, AdaBoost, Gradient Boosting y XGBoost.</p> </li> <li> <p>M\u00e9todos de clustering, como DBSCAN,  Gaussian Mixture Models (GMM) y Spectral Clustering.  </p> </li> <li> <p>M\u00e9todos de aprendizaje por refuerzo, como Q-learning y SARSA.</p> </li> <li> <p>M\u00e9todos de reducci\u00f3n de la dimensionalidad, como t-SNE y UMAP.</p> </li> </ul> <p>En las pr\u00f3ximas sesiones estudiaremos varios de estos modelos.</p> <ol> <li> <p>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). The elements of statistical learning: Data mining, inference, and prediction (2nd ed., pp. 119--128). Springer.\u00a0\u21a9</p> </li> <li> <p>Hsieh, C.-J., Chang, K.-W., Lin, C.-J., Keerthi, S. S., &amp; Sundararajan, S. (2008). A dual coordinate descent method for large-scale linear SVM. Proceedings of the 25th International Conference on Machine Learning, 408--415.\u00a0\u21a9</p> </li> <li> <p>Nocedal, J., &amp; Wright, S. J. (2006). Numerical optimization (2nd ed.). Springer.\u00a0\u21a9</p> </li> <li> <p>Liu, D. C., &amp; Nocedal, J. (1989). On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(1-3), 503--528.\u00a0\u21a9</p> </li> </ol>"},{"location":"02-svm/","title":"2. SVM","text":""},{"location":"02-svm/#sesion-2-support-vector-machines-svm","title":"Sesi\u00f3n 2: Support Vector Machines (SVM)","text":"<p>Las Support Vector Machines (SVM) (Cortes &amp; Vapnik, 1995)1 son algoritmos de aprendizaje supervisado utilizados principalmente para clasificaci\u00f3n, aunque tambi\u00e9n pueden aplicarse a regresi\u00f3n. </p>"},{"location":"02-svm/#aplicaciones-de-svm","title":"Aplicaciones de SVM","text":"<p>Aunque en la actualidad los modelos basados en aprendizaje profundo dominan en campos como la Visi\u00f3n por Computador (CV) o el Procesamiento del Lenguaje Natural (NLP), y en general cuando contamos con extensos conjuntos de datos y disponibilidad de gran capacidad de computaci\u00f3n, modelos como SVM pueden ser competitivos cuando contemos con datos tabulares de tama\u00f1o peque\u00f1o o mediano.</p> <p>SVM ofrece un buen rendimiento con  conjuntos de datos peque\u00f1os. A modo orientativo, con conjuntos de menos de 1.000 ejemplos puede resultar la opci\u00f3n m\u00e1s adecuada, y podr\u00eda mantenerse competitivo incluso con datasets del orden de 10.000 ejemplos.  Encontramos otros modelos que siguen ofreciendo resultados competitivos en estos casos, como XGBoost o Random Forest. </p> <p>Por ejemplo, un \u00e1rea en la que estos modelos pueden resultar de inter\u00e9s es en el an\u00e1lisis de datos m\u00e9dicos, en los que contamos con datasets peque\u00f1os (datos de pacientes) pero con alta dimensionalidad (por ejemplo teniendo en cuenta la expresi\u00f3n de diferentes genes). Adem\u00e1s, tenemos la ventaja de que este tipo de modelos facilita la interpretabilidad, lo cual los hace especialmente interesantes en estos \u00e1mbitos. </p>"},{"location":"02-svm/#maximizacion-del-margen","title":"Maximizaci\u00f3n del margen","text":"<p>Como hemos visto anteriormente, en un problema de clasificaci\u00f3n binaria buscamos encontrar un hiperplano que separe los datos de las dos clases, pero, \u00bfcu\u00e1l es el hiperplano de separaci\u00f3n \u00f3ptimo? Lo que plantea SVM es buscar el hiperplano que maximiza el margen entre las dos clases, a diferencia del modelo de regresi\u00f3n log\u00edstica en el que lo que se buscaba era maximizar la verosimilitud. Es decir, regresi\u00f3n log\u00edstica proporciona probabilidades bien calibradas, mientras que SVM prioriza la robustez del margen, sin producir probabilidades de forma directa.</p> <p>El margen ser\u00e1 la distancia desde el hiperplano hasta los puntos m\u00e1s cercanos de cada clase. Estos puntos m\u00e1s cercanos son conocidos como vectores de soporte (ver ). </p> <p>Figura 1: Maximizaci\u00f3n del margen </p> <p>Adem\u00e1s, como veremos m\u00e1s adelante, SVM se puede generalizar para casos en los que los datos no sean separables de forma l\u00edneal. </p>"},{"location":"02-svm/#margen-duro","title":"Margen duro","text":"<p>Vamos en primer lugar a suponer que los datos son linealmente separables. Hablamos entonces de margen duro, ya que estableceremos la restricci\u00f3n de que los puntos pertenecientes a cada clase deben quedar siempre al lado correcto del margen.</p> <p>Consideremos que tenemos un conjunto de entrenamiento con \\(N\\) pares \\((\\mathbf{x_i}, y_i)\\) con \\(\\mathbf{x_i} \\in \\mathbb{R}^d\\) y \\(y_i \\in \\{-1, 1\\}\\) (problema de clasificaci\u00f3n binaria), siendo \\(d\\) el n\u00famero de features.</p> <p>En caso de que el vector \\(\\mathbf{w}\\) sea unitario, la funci\u00f3n \\(f(\\mathbf{x})\\) nos dar\u00e1 la distancia desde el hiperplano a cada punto \\(\\mathbf{x}\\). </p> <p>Con esto, para buscar el hiperplano que maximice el margen \\(M\\), deberemos resolver el siguiente problema de optimizaci\u00f3n:</p> \\[ \\begin{align*} \\max_{\\mathbf{w}, b, \\lVert \\mathbf{w}  \\rVert=1} \\quad  &amp; M \\\\ \\text{s.a.} \\quad &amp; y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq M, i = 1, \\ldots, N \\end{align*} \\] <p>Podemos eliminar la restricci\u00f3n de que \\(\\mathbf{w}\\) sea unitario dividiendo la ecuaci\u00f3n del hiperplano entre \\(\\lVert \\mathbf{w} \\rVert\\). Si dividimos toda la ecuaci\u00f3n seguir\u00e1 representando al mismo hiperplano y nos permitir\u00e1  reemplazar la condici\u00f3n con:</p> \\[ \\frac{1}{\\lVert \\mathbf{w} \\rVert} y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq M \\] <p>O lo que es lo mismo:</p> \\[ y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq M \\lVert \\mathbf{w} \\rVert \\] <p>En este caso, el hiperplano seguir\u00e1 siendo el mismo independientemente del valor de \\(\\lVert \\mathbf{w} \\rVert\\). Por lo tanto, podemos considerar de forma arbitraria que \\(\\lVert \\mathbf{w} \\rVert = 1 / M\\), lo cual nos permite reescribir la restricci\u00f3n anterior de la siguiente forma:</p> \\[ y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq 1  \\]"},{"location":"02-svm/#forma-primal","title":"Forma primal","text":"<p>Con lo anterior, el problema de optimizaci\u00f3n a resolver tendr\u00eda la siguiente forma:</p> \\[ \\begin{align*} \\min_{\\mathbf{w}, b} \\quad &amp; \\mathbf{\\lVert w \\rVert} \\\\ \\text{s.a.} \\quad &amp; y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq 1, i = 1, \\ldots, N \\end{align*} \\] <p>Esta es la conocida como forma primal, en la que tenemos nuestra funci\u00f3n objetivo y una serie de restricciones. Podr\u00edamos resolver este problema aplicando alg\u00fan m\u00e9todo de optimizaci\u00f3n como descenso por gradiente, o descenso por gradiente estoc\u00e1stico (SGD), para buscar los par\u00e1metros \\(\\mathbf{w}\\) y \\(b\\) \u00f3ptimos. </p> <p>Sin embargo, vamos a utilizar el m\u00e9todo de los multiplicadores de Lagrange (Boyd &amp; Vandenberghe, 2004)2 para transformar este problema con restricciones a un problema en el que las restricciones se transforman en penalizaciones a la funci\u00f3n objetivo.</p> <p>El problema de optimizaci\u00f3n anterior ser\u00eda equivalente al siguiente, ya que el m\u00ednimo de \\(\\mathbf{\\lVert w \\rVert}\\) ser\u00e1 el mismo que el de \\(\\frac{1}{2} \\mathbf{\\lVert w \\rVert}^2\\):</p> \\[ \\begin{align*} \\min_{ \\mathbf{w}, b} \\quad  &amp; \\frac{1}{2} \\mathbf{\\lVert w \\rVert}^2 \\\\ \\text{s.a.} \\quad &amp; y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq 1, i = 1, \\ldots, N \\end{align*} \\] <p>Sin embargo, esta segunda forma nos da ventajas importantes, especialmente la diferenciabilidad de \\(\\frac{1}{2} \\mathbf{\\lVert w \\rVert}^2\\), que es derivable en todos sus puntos, mientras \\(\\mathbf{\\lVert w \\rVert}\\) no es derivable cuando \\(\\mathbf{\\lVert w \\rVert}\\) = 0. </p> <p>Debemos recordar que estamos asumiendo de momento que los datos son separables (margen duro), y por lo tanto consideramos \u00fanicamente dos casos posibles:</p> <ul> <li>\\(y_i(\\mathbf{x}_i^T\\mathbf{w} + b) &gt; 1\\) : correctamente clasificados fuera del margen.</li> <li>\\(y_i(\\mathbf{x}_i^T\\mathbf{w} + b) = 1\\) : Vectores de soporte, pertenecientes al margen.</li> </ul> <p>Para resolver el problema de optimizaci\u00f3n mediante multiplicadores de Lagrange, la funci\u00f3n Lagrangiana primal que deberemos minimizar respecto a \\(\\mathbf{w}\\) y \\(b\\) es la siguiente:</p> \\[ L_P(\\mathbf{w}, b, \\alpha) = \\frac{1}{2} \\lVert \\mathbf{w} \\rVert^2 - \\sum_{i=1}^N \\alpha_i [ y_i ( \\mathbf{x}_i^T \\mathbf{w} + b ) - 1 ] \\] <p>Hemos transformado cada restricci\u00f3n en un t\u00e9rmino de la funci\u00f3n a minimizar, y aplicado a cada uno de estos t\u00e9rminos un multiplicador \\(\\alpha_i\\) (multiplicador de Lagrange). </p> <p>Derivamos la funci\u00f3n anterior respecto a \\(\\mathbf{w}\\) y \\(b\\), y establecemos las derivadas a \\(0\\) para buscar el punto en el que la funci\u00f3n presenta un m\u00ednimo (condici\u00f3n de estacionariedad). Tenemos entonces:</p> \\[ \\begin{align*} \\frac {\\partial L(\\mathbf{w}, b, \\alpha)}{\\partial \\mathbf{w}} &amp;= \\mathbf{w} - \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i = 0 &amp; \\Rightarrow \\mathbf{w} = \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i  \\\\ \\frac {\\partial L(\\mathbf{w}, b, \\alpha)}{\\partial b} &amp;= \\sum_{i=1}^N \\alpha_i y_i  = 0  &amp; \\Rightarrow 0 = \\sum_{i=1}^N \\alpha_i y_i \\end{align*} \\] <p>Sustituyendo \\(\\mathbf{w}\\) en la Lagrangiana (teniendo en cuenta que \\(\\lVert \\mathbf{w} \\rVert^2 = \\mathbf{w}^T \\mathbf{w}\\)) tenemos:</p> \\[ \\begin{align*} L_D(\\mathbf{w}, b, \\alpha) &amp;= \\frac{1}{2}  \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i^T \\sum_{j=1}^N \\alpha_j y_j \\mathbf{x}_j   - \\sum_{i=1}^N \\alpha_i [ y_i ( \\mathbf{x}_i^T \\sum_{j=1}^N \\alpha_j y_j \\mathbf{x}_j + b ) - 1 ] = \\\\ &amp;= \\frac{1}{2}  \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i^T \\sum_{j=1}^N \\alpha_j y_j \\mathbf{x}_j     - \\sum_{i=1}^N \\alpha_i y_i  \\mathbf{x}_i^T \\sum_{j=1}^N \\alpha_j y_j \\mathbf{x}_j  - b \\sum_{i=1}^N \\alpha_i y_i   + \\sum_{i=1}^N \\alpha_i = \\\\  &amp;= \\sum_{i=1}^N \\alpha_i - \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j  \\end{align*} \\]"},{"location":"02-svm/#forma-dual","title":"Forma dual","text":"<p>Tenemos entonces el problema dual. A diferencia del problema primal donde minimiz\u00e1bamos la norma de \\(\\mathbf{w}\\), ahora buscamos maximizar la funci\u00f3n \\(L_D(\\alpha)\\):</p> \\[ \\begin{align*} \\max_\\alpha \\quad &amp; L_D(\\alpha) =  \\sum_{i=1}^N \\alpha_i - \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j  \\\\ s.a.\\quad  &amp; \\sum_{i=1}^N \\alpha_i y_i  = 0  \\\\ &amp;\\alpha_i \\geq 0 \\  \\forall i=1, \\ldots, N \\end{align*} \\] <p>Esto ocurre porque al transformar el problema mediante multiplicadores de Lagrange, la funci\u00f3n dual nos proporciona una cota inferior del valor \u00f3ptimo del problema primal, por lo que para encontrar la mejor soluci\u00f3n debemos maximizar esta cota. </p> <p>Debemos destacar en este punto que en el caso de la forma dual deberemos optimizar los multiplicadores \\(\\alpha_i\\), en lugar de los par\u00e1metro \\(\\mathbf{w}\\) y \\(b\\) como ocurr\u00eda en el caso de la forma primal. </p> <p>Nos encontramos con un problema de programaci\u00f3n cuadr\u00e1tica (QP) convexo con restricciones lineales. Este tipo de problemas tienen la siguiente forma general:</p> \\[ f(\\mathbf{\\alpha}) = \\frac{1}{2} \\mathbf{\\alpha}^T Q \\mathbf{\\alpha} + c^T \\mathbf{\\alpha} \\] <p>Para que el problema sea convexo, la matriz \\(Q\\) debe ser semidefinida positiva, y esto se cumple en el caso de SVM, ya que tenemos:</p> \\[  Q_{ij} = y_i y^j \\mathbf{x}^T_i \\mathbf{x}_j \\] <p>Podremos por lo tanto aplicar alg\u00fan algoritmo de optimizaci\u00f3n para este tipo de problemas. Encontramos diferentes solvers, como por ejemplo CVXOPT o OSQP en Python. </p> <p>En la pr\u00e1ctica, el algoritmo m\u00e1s utilizado es SMO (Sequential Minimal Optimization) (Platt, 1998)3. Este es el algoritmo utilizado por ejemplo por LIBSVM, que es la librer\u00eda que encontramos integrada en scikit-learn. En este caso, en lugar, de resolver un problema QP completo con \\(N\\) variables, selecciona solo dos variables \\(\\alpha_i\\) y \\(\\alpha_j\\) iterativamente, fijando el resto, las optimiza, e itera hasta la convergencia. </p>"},{"location":"02-svm/#condiciones-kkt","title":"Condiciones KKT","text":"<p>En un problema de optimizaci\u00f3n convexo con restricciones para que un punto sea \u00f3ptimo debe satisfacer un conjunto de condiciones conocidas como condiciones KKT (Karush-Kuhn-Tucker) (Boyd &amp; Vandenberghe, 2004; Kuhn &amp; Tucker, 1951)4 2:</p> <ol> <li>Estacionariedad. Buscamos que la funci\u00f3n Lagrangiana tenga gradiente \\(0\\). Se cumple al haber igualado las derivadas a \\(0\\). </li> <li>Factibilidad. El punto debe ser factible y cumplir las restricciones.</li> <li> <p>Signo. Todos los multiplicadores asociados a restricciones de desigualdad deben tener signo positivo: $$ \\alpha_i \\geq 0 \\quad \\forall i=1, \\ldots, N $$  </p> </li> <li> <p>Complementariedad. Adem\u00e1s de las condiciones anteriores, es importante cumplir tambi\u00e9n la siguiente condici\u00f3n:</p> </li> </ol> \\[ \\alpha_i [ y_i ( \\mathbf{x}_i^T \\mathbf{w} + b) - 1 ] = 0 \\quad \\forall i=1, \\ldots, N \\] <p>Esta \u00faltima condici\u00f3n nos dice que:</p> <ul> <li> <p>Si \\(\\alpha_i &gt; 0\\), entonces la restricci\u00f3n es activa y debe cumplirse \\([ y_i ( \\mathbf{x}_i^T \\mathbf{w} + b) - 1 ] = 0\\). Estos ser\u00e1n los puntos conocidos como vectores de soporte, que se encuentran justo en el margen de separaci\u00f3n.</p> </li> <li> <p>En caso de que \\(y_i (\\mathbf{x}_i^T \\mathbf{w} + b) &gt; 1\\), entonces el punto estar\u00e1 fuera del margen y la restricci\u00f3n no ser\u00e1 activa, siendo \\(\\alpha_i = 0\\). En este caso no se tratar\u00e1 de un vector de soporte.</p> </li> </ul> <p>Es importante destacar que solo los puntos con \\(\\alpha_i &gt; 0\\) (vectores de soporte) contribuyen a la soluci\u00f3n. El resto de puntos no afectar\u00e1n al hiperplano. </p> <p>A partir de la funci\u00f3n obtenida al calcular la derivada parcial respecto a cada uno de los coeficientes, podemos observar que \\(\\mathbf{w}\\) se obtendr\u00e1 como combinaci\u00f3n lineal de los vectores de soporte \\(\\mathbf{x}_i\\) (aquellos con \\(\\alpha_i &gt; 0\\)):</p> \\[ \\mathbf{w} = \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i  \\] <p>El par\u00e1metro \\(b\\) se puede obtener resolviendo la condici\u00f3n de complementariedad para cualquiera de los vectores de soporte.</p>"},{"location":"02-svm/#margen-blando","title":"Margen blando","text":"<p>Todo lo anterior es v\u00e1lido bajo la suposici\u00f3n de que los datos son linealmente separables, pero si esto no se cumple entonces el problema primal no tendr\u00e1 soluci\u00f3n. </p> <p>Supongamos ahora que existe un solape entre los datos. Una forma de tratar con este solape es maximizar \\(M\\) permitiendo que algunos datos est\u00e9n en el lado incorrecto del margen, para lo cual se definen las variables \\(\\xi = (\\xi_1, \\xi_2, \\ldots, \\xi_N)\\), relajando la restricci\u00f3n del primal de la siguiente forma:</p> \\[ y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq 1 - \\xi_i \\quad \\forall i, \\xi_i \\geq 0 \\] <p>Podemos interpretar \\(\\xi_i\\) como la cantidad proporcional que permitimos que una predicci\u00f3n est\u00e9 en el lado incorrecto del margen (ver ). Si tenemos \\(\\xi_i &gt; 1\\) entonces la correspondiente predicci\u00f3n estar\u00eda mal clasificada, mientras que con valores \\(0 &lt; \\xi_i &lt; 1\\) estar\u00eda correctamente clasificada pero en el lado incorrecto del margen.</p> <p>Figura 2: Margen blando y variables \\(\\xi_i\\) </p> <p>Si acotamos el sumatorio \\(\\sum_{i=1}^N \\xi_i\\) a un valor constante, entonces estaremos acotando el n\u00famero m\u00e1ximo de errores de clasificaci\u00f3n de los datos de entrenamiento a dicha constante. Esto lo trasladaremos como una penalizaci\u00f3n a nuestra funci\u00f3n objetivo.</p>"},{"location":"02-svm/#forma-primal_1","title":"Forma primal","text":"<p>Al igual que hicimos en el caso con margen duro, describimos el problema como una soluci\u00f3n de programaci\u00f3n cuadr\u00e1tica utilizando multiplicadores de Lagrange, en este caso introduciendo las variables \\(\\xi_i\\), con la siguiente funci\u00f3n objetivo:</p> \\[ \\begin{align*} \\min_{ \\mathbf{w}, b} \\quad  &amp; \\frac{1}{2} \\mathbf{\\lVert w \\rVert}^2 + C \\sum^N_{i=1} \\xi_i \\\\ \\text{s.a.} \\quad &amp; \\xi_i \\geq 0,\\ y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq 1 - \\xi_i, \\quad \\forall i = 1, \\ldots, N \\end{align*} \\] <p>Podemos ver que el par\u00e1metro \\(C\\) grad\u00faa la penalizaci\u00f3n de las variables \\(\\xi_i\\). Cuanto m\u00e1s alto sea \\(C\\), m\u00e1s penalizar\u00e1 cada punto fuera del margen. En el caso extremo, con \\(C=\\infty\\) equivaldr\u00eda al caso con margen duro y no se permitir\u00eda ning\u00fan punto en el lado incorrecto del margen.</p> <p>La funci\u00f3n de Langrange primal en este caso es:</p> \\[ L_P(\\mathbf{w}, b, \\xi, \\alpha, \\mu) = \\frac{1}{2} \\lVert \\mathbf{w} \\rVert^2 + C \\sum^N_{i=1} \\xi_i - \\sum_{i=1}^N \\alpha_i [ y_i ( \\mathbf{x}_i^T \\mathbf{w} + b ) - (1-\\xi_i) ] - \\sum_{i=1}^N \\mu_i \\xi_i \\] <p>Tendremos que minimizar esta funci\u00f3n respecto a \\(\\mathbf{w}\\), \\(b\\) y \\(\\xi_i\\), por lo que igualaremos las correspondientes derivadas a \\(0\\):</p> \\[ \\begin{align*} \\frac {\\partial L_P(\\mathbf{w}, b, \\xi, \\alpha, \\mu)}{\\partial \\mathbf{w}} &amp;= \\mathbf{w} - \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i = 0 &amp; \\Rightarrow \\mathbf{w} = \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i  \\\\ \\frac {\\partial L_P(\\mathbf{w}, b, \\xi, \\alpha, \\mu)}{\\partial b} &amp;= \\sum_{i=1}^N \\alpha_i y_i  = 0  &amp; \\Rightarrow 0 = \\sum_{i=1}^N \\alpha_i y_i   \\\\ \\frac {\\partial L_P(\\mathbf{w}, b, \\xi, \\alpha, \\mu)}{\\partial \\xi_i} &amp;= C - \\alpha_i - \\mu_i = 0 &amp; \\Rightarrow \\alpha_i = C - \\mu_i \\end{align*} \\]"},{"location":"02-svm/#forma-dual_1","title":"Forma dual","text":"<p>Sustituyendo las derivadas anteriores en la funci\u00f3n primal, obtenemos la forma dual:</p> \\[ \\begin{align*} \\max_\\alpha \\quad &amp; L_D(\\alpha) =  \\sum_{i=1}^N \\alpha_i - \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j  \\\\ s.a.\\quad  &amp; \\sum_{i=1}^N \\alpha_i y_i  = 0  \\\\ &amp;0 \\leq \\alpha_i \\leq C \\quad  \\forall i=1, \\ldots, N \\end{align*} \\] <p>La funci\u00f3n \\(L_D\\) nos da una cota inferior de la funci\u00f3n objetivo para cualquier punto viable, por lo que buscaremos maximizarla. </p> <p>Adem\u00e1s, se deben cumplir las diferentes condiciones KKT:</p> <ol> <li> <p>Estacionariedad. Se cumple habiendo igualado las derivadas a \\(0\\).</p> </li> <li> <p>Factibilidad. Deben cumplirse las restricciones originales del problema primal: $$  y_i(\\mathbf{x}_i^T\\mathbf{w} + b)  \\geq 1 - \\xi_i \\quad \\forall i $$ $$ \\xi_i  \\geq 0 \\quad \\forall i $$</p> </li> <li> <p>Signo. Los multiplicadores asociados a restricciones de desigualdad no deben ser negativos: $$  \\alpha_i  \\geq 0 \\quad \\forall i $$ $$ \\mu_i  \\geq 0 \\quad \\forall i $$</p> </li> <li> <p>Complementariedad. Esta es la m\u00e1s importante a tener en cuenta, ya que define qu\u00e9 restricciones son activas (aquellas con par\u00e1metros \\(\\alpha_i &gt; 0\\) y \\(\\mu_i &gt; 0\\)), indicando de esta forma cu\u00e1les son los vectores de soporte.  $$ \\alpha_i[y_i(\\mathbf{x}_i^T \\mathbf{w} + b) - 1 + \\xi_i] = 0 \\quad \\forall i $$ $$ \\mu_i \\xi_i = 0 \\Rightarrow (C-\\alpha_i) \\xi_i = 0 \\quad \\forall i  $$</p> </li> </ol> <p>Podemos distinguir varios casos:</p> <ul> <li> <p>\\(\\alpha_i = 0\\). Son puntos correctamente clasificados, que no son vectores de soporte. En este caso siempre tendremos \\(\\xi_i = 0\\) debido a las condiciones de complementariedad.</p> </li> <li> <p>\\(0 &lt;  \\alpha_i &lt; C\\). Estos son los vectores de soporte que se sit\u00faan exactamente en el margen. En estos casos \\(\\mu_i &gt; 0\\), y por lo tanto \\(\\xi_i = 0\\), por lo que no hay violaci\u00f3n del margen.</p> </li> <li> <p>\\(\\alpha_i = C\\). En este caso tenemos vectores de soporte que violan el margen. En estos casos \\(\\mu_i = 0\\), por lo que podemos tener \\(\\xi_i &gt; 0\\). Teniendo en cuenta que se debe cumplir \\(y_i (\\mathbf{x}^T_i \\mathbf{w} + b) = 1 - \\xi_i\\), si \\(0 &lt; \\xi_i &lt; 1\\) entonces el vector viola el margen pero estar\u00e1 bien clasificado, mientras que en caso de que \\(\\xi &gt; 1\\) entonces estar\u00e1 mal clasificado.</p> </li> </ul> <p>Una vez resuelto el problema de optimizaci\u00f3n y obtenidos los \\(\\alpha_i\\) \u00f3ptimos, podemos observar que los coeficientes \\(\\mathbf{w}\\) se obtendr\u00edan como combinaci\u00f3n lineal \u00fanicamente de los vectores de soporte (entradas \\(\\mathbf{x_i}\\) para las que \\(\\alpha_i &gt; 0\\)):</p> \\[ \\mathbf{w} = \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i  \\] <p>Una vez obtenidos los coeficientes, para despejar \\(b\\), podemos utilizar cualquiera de los puntos del margen (\\(\\alpha_i &gt; 0\\), \\(\\xi_i = 0\\)) en la primera ecuaci\u00f3n de la restricci\u00f3n de complementariedad, aunque habitualmente se suele hacer una media de la estimaci\u00f3n de todos ellos para tener una mayor estabilidad num\u00e9rica. </p>"},{"location":"02-svm/#efecto-del-parametro-c","title":"Efecto del par\u00e1metro C","text":"<p>Es importante entender el rol del par\u00e1metro \\(C\\):</p> <ul> <li>Con valores altos de \\(C\\), se penalizar\u00e1n \\(\\xi_i\\) positivos, y podremos tender al overfitting. </li> <li>Por el contrario, con valores bajos de \\(C\\) se tender\u00e1 a valores peque\u00f1os de \\(\\lVert \\mathbf{w} \\rVert\\), lo que causar\u00e1 que la frontera sea m\u00e1s suave (ampliando el margen).</li> </ul>"},{"location":"02-svm/#primal-vs-dual","title":"Primal VS Dual","text":"<p>Como hemos visto, SVM lineal puede resolverse de ambas formas. Son dos formas complementarias que resuelven el mismo problema. La clave est\u00e1 en que en cada caso cambian las variables a optimizar. En el caso de la forma primal optimizamos directamente los par\u00e1metros del modelo \\((\\mathbf{w}, b)\\), donde \\(\\mathbf{w} \\in \\mathbb{R}^d\\) (siendo \\(d\\) el n\u00famero de features), mientras que en el caso de la forma dual estaremos optimizando los \\(N\\) multiplicadores \\(\\alpha_i\\) (uno para cada ejemplo de entrenamiento).</p> <p>Por lo tanto, la conclusi\u00f3n m\u00e1s inmediata es que si \\(N \\gg d\\) convendr\u00eda utilizar la forma primal, mientras que en el caso de tener \\(N &lt; d\\) ser\u00eda preferible la forma dual. </p> <p>La implementaci\u00f3n SVCLinear de sklearn nos permite elegir entre utilizar la forma dual o la forma primal, e incluso nos permite dejar que la implementaci\u00f3n seleccione el problema autom\u00e1ticamente en funci\u00f3n del n\u00famero de samples, el n\u00famero de features y otros par\u00e1metros. Esta implementaci\u00f3n utiliza internamente como optimizador el m\u00e9todo Coordinate Descent (Hsieh et al., 2008)5, implementado en la librer\u00eda LIBLINEAR. Recordemos que es un m\u00e9todo similar a descenso por gradiente, pero en el que se seleccionan caracter\u00edsticas una a una. Se fijan todas las variables excepto una, se optimiza para esa variable, y repite iterativamente para cada variable, iterando hasta la convergencia. </p> <p>Por otro lado, en caso de contar con un extenso conjunto de datos, puede ser conveniente utilizar Descenso por Gradiente Estoc\u00e1stico. En este caso, contamos con SGDClassifier que nos permite especificar diferentes funciones de p\u00e9rdida para diferentes modelos lineales. Por defecto, utiliza la funci\u00f3n de p\u00e9rdida <code>hinge</code> que equivale a SVM Lineal (maximizar el margen es equivamente a minimizar el hinge loss). En este caso estaremos resolviendo siempre el problema primal (hemos de tener en cuenta que utilizaremos esta implementaci\u00f3n cuando el n\u00famero de samples sea muy grande). Si como funci\u00f3n de p\u00e9rdida utilizamos <code>log_loss</code> entonces tendremos un clasificador de regresi\u00f3n log\u00edstica.   </p> <p>Adem\u00e1s del criterio de n\u00famero de samples frente a n\u00famero de features, otra ventaja de la forma dual es la esparsidad de la soluci\u00f3n, ya que solo unos pocos puntos tienen \\(\\alpha_i &gt; 0\\) y contribuyen.</p> <p>Pero la ventaja m\u00e1s destacada de utilizar la forma lineal es que nos permite aplicar el conocido como Kernel trick, con el que podremos transformar el modelo en no lineal, e incluso en no param\u00e9trico.</p>"},{"location":"02-svm/#kernel-trick","title":"Kernel trick","text":"<p>Hemos visto como mediante ingenier\u00eda de caracter\u00edsticas podemos proyectar las caracter\u00edsticas originales \\(\\mathbf{x}\\) en un nuevo espacio de caracter\u00edsticas \\(h(\\mathbf{x})\\). El clasificador SVM presenta una extensi\u00f3n de esta idea, que nos permite que la dimensi\u00f3n del nuevo espacio de caracter\u00edsticas sea muy alta, e incluso infinita en algunos casos.</p> <p>Para que esto sea abordable, la idea es representar el problema de optimizaci\u00f3n de forma que las caracter\u00edsticas de entrada se presenten solo como producto escalar. </p> <p>Podemos representar la funci\u00f3n dual de la siguiente forma:</p> \\[ L_D(\\alpha) =  \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j \\langle h(\\mathbf{x}_i), h(\\mathbf{x}_j) \\rangle \\] <p>Donde hemos sustituido el producto escalar de \\(\\mathbf{x_i}^T \\mathbf{x_j}\\) por el producto escalar entre las caracter\u00edsticas transformadas. </p> <p>De esta forma, la funci\u00f3n del hiperplano de separaci\u00f3n quedar\u00eda expresada de la siguiente forma:</p> \\[ f(x) = h(\\mathbf{x})^T \\mathbf{w} + b  \\] <p>Recordando que los pesos se calculan como una combinaci\u00f3n lineal de los vectores de soporte, tendr\u00edamos:</p> \\[ \\begin{align*} f(x) &amp;= h(\\mathbf{x})^T  \\sum_{i=1}^N \\alpha_i y_i h(\\mathbf{x}_i)  + b = \\\\ &amp;= \\sum_{i=1}^N \\alpha_i y_i \\langle h(\\mathbf{x}), h(\\mathbf{x}_i) \\rangle  + b \\end{align*} \\] <p>Podemos ver entonces que tanto en la formulaci\u00f3n del problema dual como en la funci\u00f3n de separaci\u00f3n soluci\u00f3n del problema las variables de entrada \\(h(\\mathbf{x})\\) est\u00e1n involucradas \u00fanicamente en forma de producto escalar, por lo que lo \u00fanico que necesitamos conocer de ellas es lo que conocemos como funci\u00f3n de Kernel:</p> \\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\langle h(\\mathbf{x}_i), h(\\mathbf{x}_j) \\rangle \\] <p>La funci\u00f3n \\(K\\) produce el producto escalar de las caracter\u00edsticas en el espacio transformado, y solo necesitamos conocer esta funci\u00f3n, no hace falta que trabajemos en el espacio transformado. Esto es lo que se conoce como Kernel trick (Boser et al., 1992)6. Hay que destacar que la aplicaci\u00f3n de este Kernel trick solo es posible con la formulaci\u00f3n del problema dual. Por lo tanto, en la implementaci\u00f3n SVC de sklearn siempre se resolver\u00e1 el problema dual, permitiendo de esta forma el uso de diferentes Kernels.</p> <p>Utilizando diferentes funciones de Kernel podremos aplicar de forma sencilla y eficiente diferentes transformaciones del espacio de caracter\u00edsticas (ver ). Vamos a ver los Kernels m\u00e1s comunes.</p> <p>Figura 3: Comparativa de SVM con diferentes Kernels aplicados a datos no separables linealmente: lineal (izquierda), polinomial (centro) y RBF (derecha) </p>"},{"location":"02-svm/#kernel-lineal","title":"Kernel lineal","text":"<p>Generalizando el uso de los Kernels, podemos ver que el modelo SVM lineal que hemos estudiado hasta el momento se podr\u00eda considerar un caso particular en el que se utiliza el siguiente Kernel:</p> \\[ K(\\mathbf{x}_i, \\mathbf{x}_j) =\\mathbf{x}_i^T \\mathbf{x}_j \\] <p>Este Kernel podr\u00e1 resultar adecuado cuando sepamos que los datos son linealmente separables o en casos en los que tengamos alta dimensionalidad. Este tipo de Kernel facilita la interpretabilidad.</p>"},{"location":"02-svm/#kernel-polinomial","title":"Kernel polinomial","text":"<p>Est\u00e1 relacionado con el uso de caracter\u00edsticas polinomiales, pero tiene la ventaja de que no es necesario definir las caracter\u00edsticas expl\u00edcitamente, sino que se apoya en el Kernel trick para obtener una mayor eficiencia. Tiene la siguiente forma:</p> \\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma  \\mathbf{x}_i^T \\mathbf{x}_j + r) ^d \\] <p>Donde \\(d\\) es el grado del polinomio, \\(\\gamma\\) controla la influencia de cada sample en la frontera de decisi\u00f3n y \\(r\\) es un t\u00e9rmino de sesgo en el polinomio. </p> <p>En este caso, el Kernel est\u00e1 mapeando las caracter\u00edsticas a un espacio de mayor dimensionalidad, pero la dimensi\u00f3n sigue siendo finita y fija, por lo que seguimos teniendo un modelo param\u00e9trico. En este caso la frontera de decisi\u00f3n puede que ya no sea lineal en el espacio original de caracter\u00edsticas (aunque lo seguir\u00e1 siendo en el transformado), tal como hemos visto anteriormente en el caso de ingenier\u00eda de caracter\u00edsticas.</p> <p>En la  podemos ver el efecto del par\u00e1metro de grado \\(d\\) en el Kernel polinomial. Con un mayor grado podemos tener fronteras m\u00e1s complejas, pero tambi\u00e9n mayor riesgo de overfitting.</p> <p>Figura 4: Efecto del grado en el Kernel polinomial </p>"},{"location":"02-svm/#kernel-radial-basis-function-rbf","title":"Kernel Radial Basis Function (RBF)","text":"<p>Se trata de uno de los Kernels m\u00e1s utilizados, y permite representar relaciones no lineales complejas. Se basa en crear campanas de similaridad alrededor de los puntos, generando algo parecido a un mapa de calor. Tiene la siguiente forma:</p> \\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = e^{- \\gamma \\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert^2} \\] <p>El par\u00e1metro \\(\\gamma\\) controla la influencia de cada sample en la frontera de decisi\u00f3n, permitiendo as\u00ed ajustar la suavidad de la frontera. </p> <p>En este caso, el Kernel Gaussiano mapea a un espacio de dimensi\u00f3n infinita.  Intuitivamente, esto significa que crea una funci\u00f3n base (una 'campana gaussiana') centrada en cada punto del conjunto de datos, permitiendo representar fronteras de decisi\u00f3n arbitrariamente complejas. El modelo seleccionar\u00e1 como vectores de soporte \u00fanicamente aquellos puntos necesarios para definir la frontera, lo que permite que la complejidad del modelo se adapte autom\u00e1ticamente a los datos de entrada. Por ello, en este caso el modelo pasa a ser no param\u00e9trico.</p> <p>En la figura  podemos ver el efecto del par\u00e1metro \\(\\gamma\\) en el Kernel RBF. Con valores altos de este par\u00e1metro (\\(\\gamma = 10\\)) podemos observar fronteras muy irregulares y un mayor n\u00famero de vectores de soporte, indicando tendencia al overfitting. Por el contrario, con valores bajos de \\(\\gamma\\) (por ejemplo \\(\\gamma=0.1\\)) se obtienen fronteras m\u00e1s suaves que generalizan mejor, aunque si es demasiado bajo podr\u00eda causar underfitting.  </p> <p>Figura 5: Efecto del par\u00e1metro \\(\\gamma\\) en el Kernel RBF </p>"},{"location":"02-svm/#kernel-sigmoide","title":"Kernel sigmoide","text":"<p>Tambi\u00e9n conocido como Kernel neural network, tiene un comportamiento similar a redes neuronales de una capa, y tiene la siguiente forma:</p> \\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh (\\gamma  \\mathbf{x}_i^T \\mathbf{x}_j + r) \\] <p>Donde, al igual que en los casos anteriores, \\(\\gamma\\) controla la influencia de cada sample en la frontera de decisi\u00f3n y \\(r\\) es un t\u00e9rmino de sesgo que desplaza los datos en una direcci\u00f3n y otra.</p> <p>Es menos utilizado actualmente, y su uso se limita a casos muy concretos en los que los datos tienen una forma sigmoidal. </p>"},{"location":"02-svm/#problemas-multi-clase","title":"Problemas multi-clase","text":"<p>Las SVM son originalmente clasificadores binarios, pero podemos aplicarlos a problemas multiclase siguiendo estrategias como One-vs-Rest (OvR) o One-vs-One (OvO), como vimos en la sesi\u00f3n anterior.</p>"},{"location":"02-svm/#implementacion-con-svc","title":"Implementaci\u00f3n con <code>SVC</code>","text":"<p>En sklearn, cuando presentamos a <code>SVC</code> un problema multi-clase, internamente siempre entrenar\u00e1 utilizando la estrategia OvO, es decir, en caso de tener \\(K\\) posibles clases, entrenar\u00e1 \\(K (K-1) / 2\\) clasificadores binarios.</p> <p>Sin embargo, con el par\u00e1metro <code>decision_function_shape</code> podemos decidir la forma de salida de la funci\u00f3n de decisi\u00f3n (funci\u00f3n <code>decision_function()</code>). Esta funci\u00f3n nos dice, para cada ejemplo de entrada y para cada clasificador binario, la distancia (o un valor proporcional a la distancia si el vector \\(\\mathbf{w}\\) no es unitario) del ejemplo de entrada al hiperplano de separaci\u00f3n. </p> <p>Si el par\u00e1metro <code>decision_function_shape</code> toma como valor <code>'ovo'</code>, entonces <code>decision_function()</code> nos devolver\u00e1 un array de dimensi\u00f3n \\((N, K(K-1)/2)\\). Es decir, para cada ejemplo de entrada (filas), tendremos una columna para cada clasificador binario, y habr\u00e1 un clasificador binario para cada par de clases. </p> <p>Sin embargo, si <code>decision_function_shape</code> toma como valor <code>'ovr'</code>, la funci\u00f3n de decisi\u00f3n interna de OvO se transformar\u00e1 en OvR, y pasar\u00e1 a tener dimensi\u00f3n \\((N, K)\\), es decir, tendremos un valor para cada clase.</p> <p>Es importante remarcar que este par\u00e1metro solo afecta la forma de salida de <code>decision_function()</code>, pero no cambia la estrategia de entrenamiento interna, que siempre es OvO.</p>"},{"location":"02-svm/#implementacion-con-linearsvc","title":"Implementaci\u00f3n con <code>LinearSVC</code>","text":"<p>Si utilizamos la versi\u00f3n lineal de SVC, con la clase <code>LinearSVC</code>, el comportamiento ser\u00e1 diferente. En este caso contamos con un par\u00e1metro <code>multi_class</code> que nos permite determinar la estrategia a seguir si tenemos m\u00e1s de dos clases. En este caso tenemos dos opciones: <code>ovr</code>, para utilizar la estrategia One-vs-Rest, y <code>crammer_singer</code> para utilizar la estrategia Crammer-Singer, que optimiza una funci\u00f3n objetivo conjunta para todas las clases.</p>"},{"location":"02-svm/#estrategia-crammer-singer","title":"Estrategia Crammer-Singer","text":"<p>La estrategia Crammer-Singer tiene inter\u00e9s fundamentalmente a nivel te\u00f3rico, pero es poco utilizada en la pr\u00e1ctica. Esta estrategia se basa en optimizar simult\u00e1neamente \\(K\\) funciones de decisi\u00f3n, como se muestra a continuaci\u00f3n:</p> \\[ \\begin{align*} \\min_{ \\mathbf{w}, \\xi} \\quad  &amp; \\frac{1}{2} \\sum_{k=1}^K \\lVert \\mathbf{w}_k \\rVert^2  + C \\sum^N_{i=1} \\xi_i \\\\ \\text{s.a.} \\quad &amp; \\mathbf{x}_i^T \\mathbf{w}_{y_i} - \\mathbf{x}_i^T\\mathbf{w}_k \\geq  1 - \\xi_i , \\quad \\forall k \\neq y_i \\end{align*} \\] <p>En este caso tendremos un \u00fanico modelo, pero que contendr\u00e1 \\(K\\) funciones de clasificaci\u00f3n, lo cual supone un significante incremento del coste del entrenamiento. </p>"},{"location":"02-svm/#svm-para-regresion","title":"SVM para regresi\u00f3n","text":"<p>En regresi\u00f3n, las SVM funcionan de forma conceptualmente similar a la clasificaci\u00f3n, pero en este caso el objetivo cambiar\u00e1.</p> <p>Si bien en el caso de la clasificaci\u00f3n busc\u00e1bamos maximizar el margen de separaci\u00f3n de las dos clases, en el caso de regresi\u00f3n buscamos un \"tubo\" de tolarancia alrededor de la funci\u00f3n de predicci\u00f3n. Intentaremos que la gran mayor\u00eda de puntos est\u00e9n contenidos dentro de este tubo.</p> <p>Figura 6: Tubo de tolerancia en SVM para regresi\u00f3n </p>"},{"location":"02-svm/#tubo-epsilon-insensitive","title":"Tubo epsilon-insensitive","text":"<p>Una de las claves ser\u00e1 un par\u00e1metro \\(\\epsilon\\), que regular\u00e1 la anchura del tubo de tolerancia (ver ). La anchura total del tubo ser\u00e1 de \\(2 \\epsilon\\), y todos los puntos que est\u00e9n contenidos dentro de este tubo no supondr\u00e1n ninguna penalizaci\u00f3n en la funci\u00f3n de p\u00e9rdida (es decir, los puntos que est\u00e9n a una distancia m\u00e1xima \\(\\epsilon\\) del la funci\u00f3n de predicci\u00f3n).</p> <p>Tendremos la siguiente funci\u00f3n de p\u00e9rdida:</p> \\[ L(y, f(\\mathbf{x})) = \\max (0, |y - f(\\mathbf{x})| - \\epsilon) \\] <p>Tal como vemos en la funci\u00f3n, no se penalizar\u00e1n los errores menores que \\(\\epsilon\\). Por este motivo se conoce esta funci\u00f3n como epsilon-insensitive loss. Solo los puntos fuera del tubo contribuir\u00e1n a la p\u00e9rdida, permitiendo de esta forma cierta tolerancia al ruido.</p>"},{"location":"02-svm/#forma-primal_2","title":"Forma primal","text":"<p>Queremos en este caso encontrar la funci\u00f3n de predicci\u00f3n \\(f(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{w} + b\\) minimizando \\(\\lVert \\mathbf{w} \\rVert^2\\) para reducir la complejidad del modelo, buscando que la mayor parte de los puntos est\u00e9n dentro del tubo \\(\\epsilon\\):</p> \\[ \\min_{\\mathbf{w}, b} \\quad   \\frac{1}{2}  \\lVert \\mathbf{w} \\rVert^2  + C \\sum^N_{i=1} \\max (0, |y_i - f(\\mathbf{x}_i)| - \\epsilon)  \\] <p>Este problema primal es optimizado directamente por la implementaci\u00f3n LinearSVR. </p> <p>El problema anterior puede ser expresado como un problema de optimizaci\u00f3n sujeto a restricciones, dando lugar a la siguiente forma primal:</p> \\[ \\begin{align*} \\min_{ \\mathbf{w}, b, \\xi, \\xi^*} \\quad  &amp; \\frac{1}{2}  \\lVert \\mathbf{w} \\rVert^2  + C \\sum^N_{i=1} (\\xi_i + \\xi_i^*) \\\\ \\text{s.a.} \\quad &amp; y_i - (\\mathbf{x}^T \\mathbf{w} + b) \\leq \\epsilon + \\xi_i \\\\ &amp; (\\mathbf{x}^T \\mathbf{w} + b) - y_i \\leq \\epsilon + \\xi_i^* \\\\ &amp; \\xi_i, \\xi_i^* \\geq 0 \\end{align*} \\] <p>Donde \\(\\xi_i\\) y \\(\\xi_i^*\\) son las holguras para desviaciones por arriba y por abajo del tubo, respectivamente, y el par\u00e1metro \\(C\\) controla el trade-off entre complejidad del modelo y tolerancia a errores. Aquellos puntos que est\u00e9n fuera del tubo \\(\\epsilon\\) penalizar\u00e1n con \\(\\xi_i\\) o \\(\\xi_i^*\\) seg\u00fan est\u00e9n arriba o debajo del tubo, mientras que los que est\u00e9n dentro del tubo no penalizar\u00e1n.</p>"},{"location":"02-svm/#forma-dual_2","title":"Forma dual","text":"<p>El problema dual asociado es el siguiente (expresado de forma matricial):</p> \\[ \\begin{align*} \\min_{ \\alpha, \\alpha^*} \\quad  &amp; \\frac{1}{2} (\\alpha - \\alpha^*)^T Q(\\alpha - \\alpha^*) + \\epsilon e^T (\\alpha + \\alpha^*) - \\mathbf{y}^T(\\alpha - \\alpha^*) \\\\ \\text{s.a.} \\quad &amp; e^T (\\alpha - \\alpha^*) = 0 \\\\ &amp; 0 \\leq \\alpha_i, \\alpha_i^* \\leq C, \\quad i=1, \\ldots, N \\end{align*} \\] <p>Donde \\(e\\) es un vector de unos, y \\(Q\\) es una matriz semi-definida positiva, donde \\(Q_{ij} = K(\\mathbf{x}_i, \\mathbf{x}_j)\\) es el valor del kernel entre los puntos \\(\\mathbf{x}_i\\) y \\(\\mathbf{x}_j\\). De esta forma, podremos aplicar diferentes kernels al igual que en el caso de clasificaci\u00f3n.</p> <p>En el caso de regresi\u00f3n, los vectores de soporte ser\u00e1n:</p> <ul> <li>Puntos que est\u00e1n exactamente en el borde del tubo: \\((|y-f(\\mathbf{x})| = \\epsilon)\\)</li> <li>Puntos que quedan fuera del tubo: \\((|y-f(\\mathbf{x})| &gt; \\epsilon)\\)</li> </ul> <p>Los puntos dentro del tubo no influir\u00e1n en la soluci\u00f3n final, al igual que ocurr\u00eda en el caso de la clasificaci\u00f3n con los puntos al lado correcto del margen. </p>"},{"location":"02-svm/#ajuste-de-hiper-parametros","title":"Ajuste de hiper-par\u00e1metros","text":"<p>Tenemos en este caso dos hiperpar\u00e1metros clave: </p> <ul> <li>\\(C\\): Penalizaci\u00f3n por quedar fuera del tubo. A mayor \\(C\\), habr\u00e1 menos tolerancia a errores fuera del tubo, y con un \\(C\\) menor el modelo ser\u00e1 m\u00e1s robusto frente a outliers.</li> <li>\\(\\epsilon\\): Ancho del tubo de tolerancia. A mayor \\(\\epsilon\\) tendremos un modelo m\u00e1s simple, con menos vectores de soporte, mientras que con un \\(\\epsilon\\) menor el ajuste ser\u00e1 m\u00e1s preciso y habr\u00e1 m\u00e1s vectores de soporte.</li> </ul> <p>La implementaci\u00f3n SVR optimizar\u00e1 el problema dual y permitir\u00e1 utilizar los kernels <code>'linear'</code>, <code>'poly'</code>, <code>'rbf'</code> y <code>'sigmoid'</code> al igual que en el caso de clasificaci\u00f3n.</p>"},{"location":"02-svm/#consideraciones-finales","title":"Consideraciones finales","text":"<p>Hemos visto que las SVM representan un enfoque elegante y con rigor matem\u00e1tico para abordar problemas tanto de clasificaci\u00f3n como de regresi\u00f3n:</p> <ul> <li>Busca el hiperplano \u00f3ptimo que maximiza el margen entre clases en caso de clasificaci\u00f3n, o el tubo que contiene la mayor parte de ejemplos en caso de regresi\u00f3n.</li> <li>Permite utilizar el kernel trick para resolver problemas no lineales sin necesidad de calcular de forma expl\u00edcita transformaciones de alta dimensionalidad.</li> <li>La soluci\u00f3n se calcula a partir \u00fanicamente de un reducido subconjunto de los puntos de entrada, los conocidos como vectores de soporte que dan nombre al m\u00e9todo.</li> </ul> <p>Las SVM han sido durante d\u00e9cadas uno de los algoritmos dominantes en el campo del Machine Learning, ofreciendo excelentes resultados en problemas de dimensionalidad media y alta y con datasets de tama\u00f1o moderado. Sin embargo, presentan tambi\u00e9n una serie de limitaciones pr\u00e1cticas que comentaremos a continuaci\u00f3n.</p> <p>Una cuesti\u00f3n a tener muy en cuenta es la sensibilidad al preprocesamiento. Realizar un correcto escalado de los datos es cr\u00edtico. Supongamos que una de las caracter\u00edsticas maneja valores de orden muy superior al resto. Por ejemplo, consideremos que nuestras caracter\u00edsticas de entrada son edad y salario. La primera se situar\u00e1 normalmente en el rango de \\([0, 100]\\), mientras que la segunda tomar\u00e1 habitualmente valores en el rango de \\([1000, 5000]\\). La caracter\u00edstica con mayor rango dominar\u00e1 el c\u00e1lculo de las distancias, y esto puede causar que las de menor rango sean ignoradas. Por ello, importante realizar un escalado previo de los datos para conseguir que todas las caracter\u00edsticas tengan media \\(\\mu = 0\\) y desviaci\u00f3n t\u00edpica \\(\\sigma = 1\\), de forma que contribuyan de forma equitativa a las distancias. Con esto conseguiremos que el modelo necesite menos vectores de soporte y que generalice mejor.</p> <p>Al utilizar SVM tambi\u00e9n ser\u00e1 importante seleccionar el kernel adecuado y sus par\u00e1metros, lo cual no siempre es intuitivo. Habr\u00e1 que ajustar de forma cuidadosa los diferentes par\u00e1metros. El caso m\u00e1s com\u00fan es el encontrar el trade-off entre \\((C, \\gamma)\\) en el caso del kernel RBF, ya que es el kernel que aporta una mayor flexibilidad para adaptarse a los datos de entrada, pero no siempre ser\u00e1 la elecci\u00f3n m\u00e1s adecuada:</p> <ul> <li>El kernel lineal ser\u00e1 m\u00e1s adecuado en caso de contar con datos linealmente separables, ya que tendremos un modelo m\u00e1s sencillo, con menos vectores de soporte, y nos proporcionar\u00e1 una mayor interpretabilidad. Adem\u00e1s, solo tendremos que ajustar el par\u00e1metro \\(C\\) (y \\(\\epsilon\\) en caso de regresi\u00f3n).</li> <li>El kernel RBF, como hemos comentado, es el que proporciona una mayor flexibilidad, ya que se adapta a cualquier patr\u00f3n no lineal, siendo la  opci\u00f3n por defecto recomendada cuando no conocemos la estructura de los datos, pero tambi\u00e9n cuenta con algunas desventajas: requiere normalmente m\u00e1s vectores de soporte, por lo que es m\u00e1s lento en predicci\u00f3n, y resulta menos interpretable. Adem\u00e1s, requiere ajustar dos par\u00e1metros, \\(C\\) y \\(\\gamma\\), presentando tendencia al overfitting cuando \\(\\gamma\\) es alto.</li> <li>El kernel polinomial por otro lado, tiene como ventaja que es m\u00e1s interpretable que RBF, y resulta de utilidad para capturar relaciones polin\u00f3micas espec\u00edficas. Como inconveniente encontramos que requiere elegir el grado correcto y tambi\u00e9n es sensible al par\u00e1metro <code>coef0</code>, y puede resultar muy costoso al aumentar el grado. Si conocemos que la relaci\u00f3n de los datos es polin\u00f3mica ser\u00e1 interesante utilizar este kernel, pero si no conocemos la forma funcional de los datos entonces convendr\u00e1 utilizar RBF. </li> </ul> <p>Otra limitaci\u00f3n del m\u00e9todo es su escalabilidad computacional, con complejidades \\(O(N^3)\\) o \\(O(N^2d)\\) seg\u00fan la implementaci\u00f3n. Esto puede hacer que el coste computacional sea prohibitivo en caso de contar con datasets grandes que contengan millones de muestras. Adem\u00e1s, el coste espacial en memoria tambi\u00e9n crece cuadr\u00e1ticamente por la necesidad de almacenar el kernel.</p> <p>Tambi\u00e9n podemos encontrar una interpretabilidad limitada, especialmente con kernels no lineales, en los que el modelo se convierte en una \"caja negra\". En estos casos es complicado establecer qu\u00e9 caracter\u00edsticas son m\u00e1s importantes, y el motivo por el que el modelo ha tomado una decisi\u00f3n. </p> <p>En los pr\u00f3ximos temas vamos a abordar una familia diferente de modelos: los \u00c1rboles de Decisi\u00f3n. Mientras que Regresi\u00f3n Log\u00edsitica y SVM son algoritmos basados en geometr\u00eda (hiperplano de separaci\u00f3n), los \u00c1rboles de Decisi\u00f3n est\u00e1n basados en reglas. Como veremos, estos modelos nos dar\u00e1n una serie de ventajas como su alta interpretabilidad, la no necesidad de escalar los datos, la posibilidad de manejar datos categ\u00f3ricos de forma natural y la eficiencia computacional incluso con datasets grandes.</p> <ol> <li> <p>Cortes, C., &amp; Vapnik, V. (1995). Support-vector networks. Machine Learning, 20(3), 273--297. https://doi.org/10.1007/BF00994018 \u21a9</p> </li> <li> <p>Boyd, S., &amp; Vandenberghe, L. (2004). Convex optimization. Cambridge University Press.\u00a0\u21a9\u21a9</p> </li> <li> <p>Platt, J. C. (1998). Sequential minimal optimization: A fast algorithm for training support vector machines (Nos. MSR-TR-98-14). Microsoft Research.\u00a0\u21a9</p> </li> <li> <p>Kuhn, H. W., &amp; Tucker, A. W. (1951). Nonlinear programming. Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability, 481--492.\u00a0\u21a9</p> </li> <li> <p>Hsieh, C.-J., Chang, K.-W., Lin, C.-J., Keerthi, S. S., &amp; Sundararajan, S. (2008). A dual coordinate descent method for large-scale linear SVM. Proceedings of the 25th International Conference on Machine Learning, 408--415.\u00a0\u21a9</p> </li> <li> <p>Boser, B. E., Guyon, I. M., &amp; Vapnik, V. N. (1992). A training algorithm for optimal margin classifiers. Proceedings of the Fifth Annual Workshop on Computational Learning Theory, 144--152. https://doi.org/10.1145/130385.130401 \u21a9</p> </li> </ol>"},{"location":"03-arboles-decision/","title":"3. \u00c1rboles de decisi\u00f3n","text":""},{"location":"03-arboles-decision/#sesion-3-arboles-de-decision","title":"Sesi\u00f3n 3: \u00c1rboles de decisi\u00f3n","text":"<p>Los \u00e1rboles de decisi\u00f3n (Breiman et al., 1984; James et al., 2023)1 2 son modelos de aprendizaje supervisado que nos permiten hacer tanto tareas de clasificaci\u00f3n como de regresi\u00f3n. Se basan en una estructura jer\u00e1rquica de decisiones, y su principal ventaja es la interpretabilidad del modelo, ya que podemos interpretarlos como un diagrama de flujo en el que en cada nodo debe tomarse una decisi\u00f3n (ver -izquierda).</p>"},{"location":"03-arboles-decision/#estructura","title":"Estructura","text":"<p>El \u00e1rbol tiene un nodo ra\u00edz que abarcar\u00e1 todo el espacio de caracter\u00edsticas, conteniendo todo el conjunto de datos de entrada. </p> <p>Cada nodo interno del \u00e1rbol divide el espacio de caracter\u00edsticas mediante una  condici\u00f3n sobre los atributos. Considerando que nuestros datos tienen un conjunto de \\(d\\) atributos o features \\(\\{x_1, x_2, \\ldots, x_d \\}\\), la condici\u00f3n de cada nodo tendr\u00e1 habitualmente una forma del tipo \\(x_i \\leq \\text{valor}\\). Es decir, podemos ver cada nodo como un modelo sencillo que separa los datos en dos subramas.</p> <p>El nodo tendr\u00e1 diferentes subramas que lo conectan con sus hijos. Estas subramas representan los resultados de la condici\u00f3n. Aplicando la condici\u00f3n a cada dato de entrada, se seleccionar\u00e1 una de las subramas y llegaremos al correspondiente nodo hijo. Los hijos, a su vez, podr\u00e1n definir nuevas condiciones que vuelvan a particionar el espacio de caracter\u00edsticas. </p> <p>Llegaremos finalmente a nodos hoja que no tienen hijos y corresponden a las predicciones finales, que podr\u00e1n corresponder a las diferentes clases en problemas de clasificaci\u00f3n o valores en problemas de regresi\u00f3n. </p> <p>De este forma, para un determinado ejemplo de entrada, el \u00e1rbol se recorrer\u00e1 desde la ra\u00edz, tomando en cada nodo la subrama que corresponda al resultado de aplicar la condici\u00f3n del nodo al ejemplo, hasta llegar a un nodo hoja. El nodo hoja que alcancemos nos dar\u00e1 la predicci\u00f3n que devolver\u00e1 el modelo.</p> <p>Desde el punto de vista de dos dimensiones, este \u00e1rbol estar\u00e1 dividiendo el espacio de caracter\u00edsticas en diferentes rect\u00e1ngulos (ver -centro). </p> <p>Figura 1: Estructura del \u00e1rbol de decisi\u00f3n (izquierda) y divisi\u00f3n del espacio de caracter\u00edsticas en regiones (centro y derecha) </p> <p>Esta divisi\u00f3n generar\u00e1 \\(J\\) regiones \\(R_1, R_2, \\ldots, R_J\\) no solapadas, de forma que cada datos de entrada \\(\\mathbf{x}\\) pertenecer\u00e1 a una, y solo una de estas regiones. Cada regi\u00f3n \\(R_j\\) corresponde a uno de los nodos hoja del \u00e1rbol, y estar\u00e1 asociada a una categor\u00eda en el caso de los \u00e1rboles de clasificaci\u00f3n, o a un valor en el caso de \u00e1rboles de regresi\u00f3n. </p> <p>Figura 2: Estratificaci\u00f3n de la salida de los \u00e1rboles de regresi\u00f3n. Aproximaci\u00f3n de datos con forma de campana de Gauss </p> <p>Es importante destacar que la salida de los \u00e1rboles de decisi\u00f3n estar\u00e1 estratificada, ya que dentro de cada regi\u00f3n se generar\u00e1 siempre un valor constante (ver ). </p>"},{"location":"03-arboles-decision/#construccion","title":"Construcci\u00f3n","text":"<p>Vamos a ver en este punto c\u00f3mo construir el \u00e1rbol a partir de un conjunto de datos. Consideramos que nuestro conjunto de datos \\(\\mathcal{D}\\) contiene \\(N\\) ejemplos \\((\\mathbf{x}_i, y_i)\\) para \\(i =\u00a0\\{1, 2, \\ldots, N \\}\\), con \\(\\mathbf{x}_i = ( x_{i1}, x_{i2}, \\ldots, x_{id} )\\).</p> <p>Como hemos comentado, el \u00e1rbol dividir\u00e1 el espacio de caracter\u00edsticas en \\(J\\) regiones \\(R_1, R_2, \\ldots, R_J\\) no solapadas, de forma que cada dato \\(\\mathbf{x}_i\\) corresponder\u00e1 a una de estas regiones.</p> <p>Buscamos encontrar la divisi\u00f3n del espacio que se ajuste de forma \u00f3ptima a los datos. Por ejemplo, en caso de \u00e1rboles de regresi\u00f3n podemos tomar como criterio encontrar el particionamiento que minimice el error cuadr\u00e1tico medio (MSE) total del conjunto de datos:</p> \\[ \\frac{1}{N} \\sum_{j=1}^J \\sum_{i \\in R_j} (y_i - \\hat{y}_{R_j})^2 \\] <p>Donde \\(\\hat{y}_{R_j}\\) es la media de la salida de las observaciones pertenecientes a la regi\u00f3n \\(R_j\\). </p> <p>Dado que no es viable considerar todas las posibles particiones del espacio, se opta por un algoritmo voraz que va dividiendo el espacio recursivamente. Partiendo del conjunto de entrenamiento completo, este algoritmo funciona de la siguiente forma:</p> <ol> <li>Seleccionamos el mejor atributo y punto de corte para dividir el conjunto de datos actual.</li> <li>Creamos un nodo con una condici\u00f3n basada en los par\u00e1metros seleccionados.</li> <li>Particionamos el conjunto de datos en dos subconjuntos, en funci\u00f3n del resultado de la condici\u00f3n anterior.</li> <li>Repetimos este proceso recursivamente para cada uno de los subconjuntos anteriores hasta cumplir un criterio de parada (por ejemplo hasta conseguir regiones suficientemente homog\u00e9neas).</li> </ol> <p>Figura 3: Proceso de construcci\u00f3n de un \u00e1rbol de decisi\u00f3n paso a paso </p> <p>En la  se ilustra el proceso de construcci\u00f3n paso a paso, en el que en cada iteraci\u00f3n particionamos uno de los nodos en dos regiones.</p> <p>Una de las cuestiones m\u00e1s cr\u00edticas es establecer un criterio de divisi\u00f3n de los datos para establecer cu\u00e1les son los mejores par\u00e1metros para particionar nuestros datos. Deberemos seleccionar tanto una caracter\u00edstica \\(j \\in \\{1, 2, \\ldots, d\\}\\) como un valor de corte \\(t\\). De esta forma, la condici\u00f3n dividir\u00eda el espacio en dos subregiones y separar\u00e1 los datos en dos subconjuntos: </p> \\[ \\mathcal{D}_L = \\{ \\{\\mathbf{x}_i, y_i \\} : x_{ij} \\leq t \\} \\\\ \\mathcal{D}_R = \\{ \\{\\mathbf{x}_i, y_i \\} : x_{ij} \\gt t \\} \\] <p>Definimos una funci\u00f3n de impureza \\(H\\) que nos indicar\u00e1 la calidad de la partici\u00f3n. Buscamos minimizar esta funci\u00f3n para conseguir que la divisi\u00f3n genere regiones lo m\u00e1s homog\u00e9neas posible. Las posibles funciones alternativas de impureza diferir\u00e1n seg\u00fan si las orientamos a problemas de clasificaci\u00f3n o de regresi\u00f3n. Veremos m\u00e1s adelante las funciones utilizadas com\u00fanmente para ambos tipos de problemas.</p> <p>Con esta funci\u00f3n \\(H\\), podemos calcular la impureza de la divisi\u00f3n (split) con par\u00e1metros \\((j, t)\\) de la siguiente forma:</p> \\[ H_{split}(j,t) = \\frac{N_L}{N} H(\\mathcal{D}_L) + \\frac{N_R}{N} H(\\mathcal{D}_R) \\] <p>Donde \\(N_L = |\\mathcal{D_L}|\\) y \\(N_R = |\\mathcal{D_R}|\\) indican el n\u00famero de ejemplos que quedar\u00edan en cada una de las particiones. Es decir, la impureza de la divisi\u00f3n se calcula como la suma de las impurezas de cada una de las particiones creadas, ponderada por el n\u00famero de ejemplos de cada partici\u00f3n.</p> <p>De esta forma, se deber\u00e1n buscar los par\u00e1metros \\((j,t)\\) que minimicen la funci\u00f3n \\(H_{split}(j,t)\\). Para ello, se puede realizar una b\u00fasqueda exhaustiva para todos los pares \\((j,t)\\), o realizar un muestreo aleatorio de algunos posibles valores para reducir el coste computacional. </p> <p>Vamos a continuaci\u00f3n a ver de forma espec\u00edfica las principales funciones de impureza utilizadas en problemas de regresi\u00f3n y clasificaci\u00f3n. </p>"},{"location":"03-arboles-decision/#arboles-de-regresion","title":"\u00c1rboles de regresi\u00f3n","text":"<p>En caso de \u00e1rboles de regresi\u00f3n, dentro de cada regi\u00f3n se devolver\u00e1 como predicci\u00f3n \\(\\hat{y}_i\\) siempre el mismo valor constante, que se calcular\u00e1 habitualmente como la media de todas las observaciones \\(\\bar{y}_i\\) del conjunto de entrenamiento que pertenezcan a dicha regi\u00f3n. </p> <p>Figura 4: Construcci\u00f3n de un \u00e1rbol de regresi\u00f3n </p> <p>En la  se muestra c\u00f3mo se construye un \u00e1rbol de regresi\u00f3n conforme aumenta la profundidad del \u00e1rbol. En la parte inferior de la figura se muestra el particionamiento en regiones, mientras que en la parte superior se muestra en 3D la estratificaci\u00f3n creada en cada caso, donde cada regi\u00f3n tiene un valor constante correspondiente a la media de todas las observaciones de dicha regi\u00f3n.</p> <p>Como funci\u00f3n de impureza en \u00e1rboles de regresi\u00f3n habitualmente se utiliza el error cuadr\u00e1tico medio (MSE):</p> \\[ H_{MSE}(\\mathcal{D}) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\bar{y}_i)^2 \\] <p>Vemos que en la funci\u00f3n calculamos la diferencia entre el valor observado \\(y_i\\) y la predicci\u00f3n, que en este caso es la media \\(\\bar{y}_i\\). Podemos observar tambi\u00e9n que esta funci\u00f3n coincide con la varianza de las salidas esperadas del conjunto \\(\\mathcal{D}\\). </p> <p>En la pr\u00e1ctica se utiliza tambi\u00e9n habitualmente la suma de los cuadrados de los errores (SSE), que es equivalente a la formulaci\u00f3n anterior, ya que la \u00fanica diferencia es que el valor no est\u00e1 promediado:</p> \\[ H_{SSE}(\\mathcal{D}) = \\sum_{i=1}^N (y_i - \\bar{y}_i)^2 \\] <p>Esta es la funci\u00f3n de impureza m\u00e1s comunmente utilizada, siendo el valor por defecto en las principales librer\u00edas. Esta funci\u00f3n asume ruido gaussiano y penaliza los errores grandes.</p> <p>Tambi\u00e9n puede utilizarse el error absoluto medio (MAE), aunque en estos casos como predicci\u00f3n utilizamos la mediana \\(\\tilde{y}_i\\) en lugar de la media:</p> \\[ H_{MAE}(\\mathcal{D}) = \\frac{1}{N} \\sum_{i=1}^N |y_i - \\tilde{y}_i| \\] <p>Esta funci\u00f3n es m\u00e1s robusta frente a outliers, y no penaliza tanto grandes errores como el caso anterior, aunque el proceso de optimizaci\u00f3n resulta m\u00e1s costoso. </p> <p>Tambi\u00e9n contamos con otras funciones como Poisson deviance, que se define de la siguiente forma:</p> \\[ H_{Poisson}(\\mathcal{D}) = \\frac{2}{N} \\sum_{i=1}^N (y_i \\log \\frac{y_i}{\\bar{y}_i} - y_i + \\bar{y}_i) \\] <p>Este criterio puede ser de inter\u00e9s cuando la variable objetivo sea un conteo (por ejemplo, n\u00famero de estudiantes aprobados) o frecuencia (por ejemplo, n\u00famero de suspensos por estudiante).</p>"},{"location":"03-arboles-decision/#arboles-de-clasificacion","title":"\u00c1rboles de clasificaci\u00f3n","text":"<p>En este caso buscamos clasificar cada ejemplo de entrada en un n\u00famero \\(K\\) de clases. </p> <p>Considerando un nodo \\(m\\) del \u00e1rbol que representa un regi\u00f3n \\(R_m\\) que contiene un conjunto de \\(N_m\\) ejemplos, calculamos la proporci\u00f3n de las observaciones de la clase \\(k\\) dentro del nodo \\(m\\) de la siguiente forma:</p> \\[ p_{mk} = \\frac{1}{N_m} \\sum_{\\mathbf{x}_i \\in R_m} I(y_i = k) \\] <p>Buscamos medir c\u00f3mo de mezcladas est\u00e1n las clases dentro de cada nodo. Para ello, varias de las funciones de impureza utilizadas est\u00e1n basadas en teor\u00eda de la informaci\u00f3n. </p> <p>Una de las funciones m\u00e1s utilizadas es el \u00edndice de Gini, que se define de la siguiente forma para un nodo \\(m\\):</p> \\[ H_{Gini} = 1 - \\sum_{k=1}^K p_{mk}^2 \\] <p>En este caso, si el nodo es puro y solo contiene una \u00fanica clase tendremos \\(H_{Gini} = 0\\). El valor ser\u00e1 m\u00e1ximo cuando todas las clases est\u00e9n equilibradas. Es utilizada por el algoritmo CART y en Random Forest.</p> <p>Tambi\u00e9n se utiliza habitualmente la entrop\u00eda:</p> \\[ H_{Entrop\u00eda} =  - \\sum_{k=1}^K p_{mk} \\log_2 (p_{mk}) \\] <p>Se trata de una medida basada en teor\u00eda de la informaci\u00f3n, que mide la incertidumbre del conjunto. Nos dar\u00e1 m\u00e1xima incertidumbre cuando las clases est\u00e1n equilibradas, y penaliza m\u00e1s que Gini. Se utiliza  en los algoritmos ID3 y C4.5.</p> <p>Figura 5: Forma de las diferentes funciones de impureza para clasificaci\u00f3n </p> <p>En la  podemos observar la forma de las diferentes funciones de impureza para el caso de clasificaci\u00f3n binaria. Se muestra el valor de la funci\u00f3n de impureza en funci\u00f3n de la proporci\u00f3n de ejemplo de cada clase, pudiendo observar que es m\u00e1xima en el centro (reparto equilibrado entre las dos clases) y m\u00ednima en los extremos (todas las observaciones de una misma clase).</p>"},{"location":"03-arboles-decision/#poda-de-arboles","title":"Poda de \u00e1rboles","text":"<p>Los \u00e1rboles individuales tienen bajo sesgo pero alta varianza, ya que al construirse de forma voraz tienen una alta dependencia con los datos de entrada. Un peque\u00f1o cambio en los datos puede producir \u00e1rboles muy diferentes. </p> <p>Cuando con el algoritmo descrito el \u00e1rbol se hace crecer en exceso, tendremos tendencia al overfitting. Clasificar\u00e1 bien los datos de entrenamiento, pero dar\u00e1 malos resultados de test. Para ilustrar este efecto, en la  se muestra la aproximaci\u00f3n de una funci\u00f3n de una sola variable (1D) mediante un \u00e1rbol de regresi\u00f3n. Podemos observar la forma escalonada (estratificada) de la funci\u00f3n, y como conforme aumenta la profundidad del \u00e1rbol estos estratos se ajustan de forma m\u00e1s precisa a los datos. Sin embargo, cuando se aumenta demasiado la profundida del \u00e1rbol, como se observa a la derecha de la figura, la aproximaci\u00f3n se ajusta demasiado a los datos, teniendo estratos creados para una \u00fanica observaci\u00f3n (alta varianza).</p> <p>Figura 6: Efecto del overfitting al construir un \u00e1rbol de decisi\u00f3n </p> <p>Reduciendo el n\u00famero de regiones podemos reducir la varianza y mejorar la capacidad de generalizaci\u00f3n y la interpretabilidad. Una posible forma de abordar esta tarea es evitar que el \u00e1rbol crezca en exceso, esto es lo que se conoce como poda previa.</p>"},{"location":"03-arboles-decision/#poda-previa","title":"Poda previa","text":"<p>La poda previa consiste en detener el crecimiento del \u00e1rbol antes de que alcance su tama\u00f1o m\u00e1ximo imponiendo criterior de parada. Algunos de estos criterios pueden ser:</p> <ul> <li>Limitar la profundidad m\u00e1xima</li> <li>Establecer un n\u00famero m\u00ednimo de muestras que debe tener un nodo</li> <li>Exigir que la mejora de la impureza se encuentre por encima de un m\u00ednimo</li> </ul> <p>Aunque es una forma r\u00e1pida y sencilla de simplificar el grafo, existe riesgo de underfitting. Por ejemplo, si  establecemos como criterio de parada dejar de crecer cuando la mejora de la medida de impureza est\u00e9 por debajo de cierto umbral, puede ocurrir que en un nivel tengamos una divisi\u00f3n que no mejora apenas, pero en el siguiente encontremos una gran mejora, con lo cual, si paramos de forma anticipada, nos estaremos perdiendo esa gran mejora en la clasificaci\u00f3n.</p> <p>Por ello, es mejor estrategia dejar crecer el \u00e1rbol hasta obtener un gran \u00e1rbol \\(T_0\\), y tras ello aplicar una poda posterior. </p>"},{"location":"03-arboles-decision/#poda-posterior","title":"Poda posterior","text":"<p>La poda posterior consiste en reducir el tama\u00f1o del \u00e1rbol eliminando nodos o sub\u00e1rboles que no aportan una mejora significativa en la capacidad de generalizaci\u00f3n. </p> <p>El proceso de poda se har\u00e1 siempre desde abajo hacia arriba, es decir, desde las hojas hacia la ra\u00edz. Para cada nodo interno, tendremos la opci\u00f3n de mantener el sub\u00e1rbol que depende de \u00e9l, o bien podarlo y convertir dicho nodo en un nodo hoja.</p> <p>Tras aplicar la poda, buscaremos quedarnos con aquel sub\u00e1rbol \\(T \\subset T_0\\) que haga que no empeore, o incluso consiga que mejore, su error de generalizaci\u00f3n respecto a \\(T_0\\). Este error de generalizaci\u00f3n ser\u00e1 el error obtenido con datos no vistos durante la construcci\u00f3n del \u00e1rbol.</p> <p>Dado un subarbol, podemos estimar su error de generalizaci\u00f3n mediante un conjunto de validaci\u00f3n separado o mediante validaci\u00f3n cruzada. Sin embargo, dado que normalmente existir\u00e1 un n\u00famero muy elevado de posibles sub\u00e1rboles, no ser\u00e1 posible computacionalmente evaluar el error de todos ellos. </p> <p>Necesitaremos tener un criterio que nos permita seleccionar un peque\u00f1o conjunto de sub\u00e1rboles para tener en consideraci\u00f3n. Una forma de hacer esto es mediante la poda basada en complejidad de coste (Breiman et al., 1984)1.</p>"},{"location":"03-arboles-decision/#poda-basada-en-complejidad-de-coste","title":"Poda basada en complejidad de coste","text":"<p>La idea es generar una secuencia de sub\u00e1rboles candidatos con la siguiente forma:</p> \\[ T_0 \\supset T_1 \\supset T_2 \\supset \\ldots \\supset T_k \\] <p>Para cada uno de estos \u00e1rboles candidatos, se calcular\u00e1 su error de validaci\u00f3n y seleccionaremos aquel con menor error de validaci\u00f3n. </p>"},{"location":"03-arboles-decision/#funcion-de-complejidad-de-coste","title":"Funci\u00f3n de complejidad de coste","text":"<p>Lo fundamental ser\u00e1 encontrar un criterio que nos permita generar la secuencia de \u00e1rboles a considerar. Para ello se define la siguiente funci\u00f3n de coste:</p> \\[ R_{\\alpha}(T) = R(T)+ \\alpha |\\tilde{T}| \\] <p>Donde \\(R(T)\\) es el error emp\u00edrico obtenido con el \u00e1rbol \\(T\\) con los datos de entrenamiento, \\(\\alpha \\geq 0\\) es un par\u00e1metro para controlar la complejidad del \u00e1rbol, y \\(|\\tilde{T}|\\) es el n\u00famero de hojas del \u00e1rbol \\(T\\). </p>"},{"location":"03-arboles-decision/#generacion-de-la-secuencia-de-subarboles","title":"Generaci\u00f3n de la secuencia de sub\u00e1rboles","text":"<p>Buscamos generar \u00e1rboles que minimicen el coste anterior con diferentes valores de \\(\\alpha\\). Podemos interpretar este par\u00e1metro como un factor de penalizaci\u00f3n por el n\u00famero de hojas. En el caso de \\(\\alpha = 0\\), al no existir penalizaci\u00f3n el error m\u00ednimo nos lo dar\u00e1 el \u00e1rbol completo, pero conforme incrementemos la penalizaci\u00f3n deberemos reducir el n\u00famero de hojas. </p> <p>Para cada nodo interno del \u00e1rbol \\(t\\), consideraremos el error \\(R(t)\\) (o impureza) dentro del propio nodo \\(t\\), como si se tratara de un hoja, y la suma de los errores \\(R(T_t)\\) de todas las hojas del sub\u00e1rbol con ra\u00edz en \\(t\\). Es decir, comparamos el error emp\u00edrico \\(R(T_t)\\) del sub\u00e1rbol completo con ra\u00edz en \\(t\\), con el error emp\u00edrico \\(R(t)\\) si dicho sub\u00e1rbol se reemplazase por una hoja, aplicando la poda. </p> <p>En general, \\(R(T_t) &lt; R(t)\\), ya que de no ser as\u00ed no deber\u00edan haberse seguido generando hijos durante la construcci\u00f3n del \u00e1rbol. Sin embargo, si introducimos la penalizaci\u00f3n con el par\u00e1metro \\(\\alpha\\) entonces tenemos:</p> \\[ R_\\alpha (t) = R(t) + \\alpha \\\\ R_\\alpha (T_t) = R(T_t) + \\alpha |\\tilde{T}_t| \\] <p>En este caso podemos buscar el valor de \\(\\alpha\\) que haga \\(R_{\\alpha}(T_t) = R_\\alpha (t)\\). Para cada uno de los nodos internos calculamos:</p> \\[ \\alpha_t = \\frac{R(t) - R(T_t)}{|\\tilde{T}_t| - 1} \\] <p>Buscaremos el nodo \\(t\\) con menor \\(\\alpha_t\\), y podaremos ese nodo, obteniendo as\u00ed \\(T_1\\).</p> <p>Aplicaremos el mismo proceso sobre \\(T_1\\) para obtener \\(T_2\\), y as\u00ed iterativamente hasta quedarnos con un \u00fanico nodo. </p>"},{"location":"03-arboles-decision/#seleccion-final-del-arbol","title":"Selecci\u00f3n final del \u00e1rbol","text":"<p>Una vez generados todos los \u00e1rboles candidatos \\(T_0, T_1, \\ldots, T_k\\), los evaluaremos mediante validaci\u00f3n cruzada y nos quedaremos con aquel que obtenga un m\u00ednimo error.</p>"},{"location":"03-arboles-decision/#algoritmos","title":"Algoritmos","text":"<p>Vamos a ver a continuaci\u00f3n los principales algoritmos para la construcci\u00f3n de \u00e1rboles de decisi\u00f3n: CART, ID3 y C4.5.</p>"},{"location":"03-arboles-decision/#cart","title":"CART","text":"<p>CART es un algoritmo para construir \u00e1rboles de decisi\u00f3n binarios, aplicable tanto a clasificaci\u00f3n como a regresi\u00f3n. Fue propuesto por Breiman et al. (1984) (Breiman et al., 1984)1 y es la base de m\u00e9todos como Random Forests.</p> <p>Es el algoritmo que encontramos implementado en las clases DecisionTreeClassifier y DecisionTreeRegressor en sklearn.</p>"},{"location":"03-arboles-decision/#criterio-de-division","title":"Criterio de divisi\u00f3n","text":"<p>CART construye el \u00e1rbol mediante un proceso recursivo y voraz, donde en cada nodo se selecciona la mejor divisi\u00f3n posible seg\u00fan los siguientes criterios de impureza:</p> <ul> <li>MSE, para \u00e1rboles de regresi\u00f3n.</li> <li>\u00cdndice de Gini, para \u00e1rboles de clasificaci\u00f3n.</li> </ul> <p>Para cada nodo se consideran todos los atributos \\(j\\) y todos los posibles puntos de corte \\(t\\), y para cada split candidato \\((j,t)\\) se calcula la ganancia de la siguiente forma:</p> \\[ \\Delta H(j,t) = H(\\mathcal{D}) - \\left[  \\frac{N_L}{N} H(\\mathcal{D}_L) + \\frac{N_R}{N} H(\\mathcal{D}_R) \\right] \\] <p>Es decir, se mide cuanto mejora la pureza de los conjuntos al realizar la divisi\u00f3n, respecto a la impureza del nodo padre. Se seleccionar\u00e1 el split \\((j,t)\\) que maximice la reducci\u00f3n de impureza.</p>"},{"location":"03-arboles-decision/#criterio-de-parada","title":"Criterio de parada","text":"<p>El \u00e1rbol se construir\u00e1 recursivamente hasta que se alcance un criterio de parada:</p> <ul> <li>Se ha obtenido un nodo totalmente puro</li> <li>Se ha alcanzado la profundidad m\u00e1xima del \u00e1rbol</li> <li>Se ha alcanzado el n\u00famero m\u00ednimo de observaciones por nodo</li> <li>No se ha obtenido mejora significativa respecto al nodo padre</li> </ul>"},{"location":"03-arboles-decision/#prediccion-en-las-hojas","title":"Predicci\u00f3n en las hojas","text":"<p>En problemas de regresi\u00f3n, la predicci\u00f3n ser\u00e1 la media del valor de todas las observaciones que pertenezcan al nodo hoja:</p> \\[ \\hat{y}_i = \\bar{y}_i \\] <p>En problemas de clasificaci\u00f3n, la predicci\u00f3n ser\u00e1 o bien la clase mayoritaria del nodo hoja, o bien una distribuci\u00f3n de probabilidades:</p> \\[ \\hat{p}_k = \\frac{N_k}{N} \\] <p>Donde \\(N_k\\) es el n\u00famero de observaciones del nodo que pertenecen a la clase \\(k\\), y \\(N\\) es el n\u00famero total de observaciones en el nodo.</p>"},{"location":"03-arboles-decision/#poda-del-arbol","title":"Poda del \u00e1rbol","text":"<p>El algoritmo CART utiliza poda posterior basada en complejidad de coste. Tal como se ha comentado anteriormente, generar\u00e1 con este criterio una secuencia de sub\u00e1rboles y seleccionar\u00e1 el mejor aplicando validaci\u00f3n cruzada.</p>"},{"location":"03-arboles-decision/#id3","title":"ID3","text":"<p>ID3 (Quinlan, 1986)3 es un algoritmo hist\u00f3rico para la construcci\u00f3n de \u00e1rboles de decisi\u00f3n destinado \u00fanicamente a la construcci\u00f3n de \u00e1rboles de clasificaci\u00f3n.</p>"},{"location":"03-arboles-decision/#estructura-del-arbol","title":"Estructura del \u00e1rbol","text":"<p>Este algoritmo fue dise\u00f1ado para la clasificaci\u00f3n con atributos de entrada categ\u00f3ricos. Es decir, cada atributo de entrada puede tomar un conjunto finito de valores. Por ejemplo, podr\u00edamos tener como entrada los siguientes atributos:</p> \\[ \\begin{align*} \\text{Viento} &amp;\\rightarrow \\{ \\text{D\u00e9bil}, \\text{Fuerte} \\} \\\\ \\text{Temperatura} &amp;\\rightarrow \\{ \\text{Calor}, \\text{Templado}, \\text{Frio} \\} \\end{align*} \\] <p>En caso de contar con atributos num\u00e9ricos, deber\u00edamos discretizarlos previamente en una serie de categor\u00edas. </p> <p>Este algoritmo construye \u00e1rboles no binarios (multirama), ya que genera una subrama para cada cada posible valor del atributo seleccionado para la divisi\u00f3n. Por ejemplo, si en un nodo se selecciona el atributo \\(\\text{Temperatura}\\) como criterio de divisi\u00f3n, se crear\u00e1n \\(3\\) subramas: \\(\\text{Calor}\\), \\(\\text{Templado}\\) y \\(\\text{Frio}\\).</p> <p>Figura 7: Comparaci\u00f3n de \u00e1rboles binarios (CART) con \u00e1rboles multirama (ID3) </p> <p>En la  ilustramos la diferencia entre los \u00e1rboles binarios, que utiliza CART, y los \u00e1rboles multirama de ID3. En este \u00faltimo caso en lugar de atributos num\u00e9ricos tenemos atributos categ\u00f3ricos, y al seleccionar un atributo se crean tantas subramas como posibles valores tenga dicha atributo.</p>"},{"location":"03-arboles-decision/#criterio-de-seleccion","title":"Criterio de selecci\u00f3n","text":"<p>En este caso se utiliza la entrop\u00eda como medida de impureza. Para un conjunto de datos \\(\\mathcal{D}\\) tenemos:</p> \\[ H_{Entrop\u00eda}(\\mathcal{D}) =  - \\sum_{k=1}^K p_{k} \\log_2 (p_{k}) \\] <p>Donde \\(p_k\\) se define como la proporci\u00f3n de observaciones de la clase \\(k\\) dentro de \\(\\mathcal{D}\\).</p> <p>Como criterio de selecci\u00f3n de atributo se utiliza la ganacia de informaci\u00f3n, basada en la entrop\u00eda. Busca con ello clasificar los ejemplos de entrenamiento reduciendo la incertidumbre sobre la clase.</p> <p>Consideremos que cada atributo de entrada \\(x_j\\) puede tomar un conjunto de posibles valores \\(\\text{Valores}(x_j)\\). La ganancia de informaci\u00f3n separando el conjunto de datos \\(\\mathcal{D}\\) con el atributo \\(x_j\\)  se define como:</p> \\[ GI(\\mathcal{D},j) = H(\\mathcal{D}) - \\sum_{v \\in \\text{Valores}(x_j)} \\frac{|\\mathcal{D}_v|}{|\\mathcal{D}|} H(\\mathcal{D}_v) \\] <p>Donde \\(\\mathcal{D}_v\\) es el subconjunto de \\(\\mathcal{D}\\) en el que el atributo \\(x_j\\) toma como valor la categor\u00eda \\(v\\).</p> <p>ID3 seleccionar\u00e1 el atributo que produzca una mayor ganancia de informaci\u00f3n. </p>"},{"location":"03-arboles-decision/#pasos-del-algoritmo","title":"Pasos del algoritmo","text":"<p>A continuaci\u00f3n se muestra el algoritmo paso a paso:</p> <ol> <li>Si todas las instancias pertenecen a la misma clase se crea una hoja.</li> <li>Si no quedan atributos se crea una hoja con la clase mayoritaria.</li> <li>Se calcula la ganancia de informaci\u00f3n para cada atributo.</li> <li>Se elige el atributo con m\u00e1xima ganancia.</li> <li>Se crea  una subrama por cada valor del atributo.</li> <li>Repite el proceso recursivamente en cada subrama.</li> </ol>"},{"location":"03-arboles-decision/#c45","title":"C4.5","text":"<p>El algoritmo C4.5 (Quinlan, 1993)4 se presenta como una evoluci\u00f3n de ID3, tambi\u00e9n destinado \u00fanicamente a \u00e1rboles de clasificaci\u00f3n. Las principales mejoras introducidas sobre su predecesor son:</p> <ul> <li>Introduce la medida Gain Ratio para evitar el sesgo de ID3 hacia atributos con muchos valores. </li> <li>Permite el uso de atributos continuos.</li> <li>Aplica poda post-pruning para reducir el overfitting.</li> <li>Permine manejar valores perdidos.</li> </ul>"},{"location":"03-arboles-decision/#gain-ratio","title":"Gain Ratio","text":"<p>ID3 utiliza la ganancia de informaci\u00f3n \\(GI(\\mathcal{D}, j)\\), pero esto produce un sesgo hacia atributos con muchos valores. Por ejemplo imaginemos un atributo que tiene un valor diferente para cada ejemplo de entrada. Este atributo ser\u00eda seleccionado ya que genera nodos hoja puros, pero no generaliza. </p> <p>Para evitar esto, se introduce la medida Gain Ratio. Para calcular esta medida, primero se calcula el factor de normalizaci\u00f3n Split Info:</p> \\[ \\text{SplitInfo}(\\mathcal{D}, x_j) = - \\sum_{v \\in \\text{Valores}(x_j)} \\frac{|\\mathcal{D}_v|}{|\\mathcal{D}|} \\log_2 \\frac{|\\mathcal{D}_v|}{|\\mathcal{D}|} \\] <p>Este factor de normalizaci\u00f3n es una medida de entrop\u00eda, pero no de las clases, sino sobre c\u00f3mo estar\u00edan repartidos los tama\u00f1os de las particiones en las diferentes subramas. Ser\u00e1 \\(0\\) cuando todos los elementos del conjunto de datos tengan la misma categor\u00eda para el atributo \\(x_j\\), mientras que ser\u00e1 m\u00e1ximo cuando los diferentes valores del atributo est\u00e9n presentes de forma homog\u00e9nea. Pero adem\u00e1s, tendr\u00e1 un valor bajo cuando haya pocas subramas, y un valor alto cuando haya muchas. </p> <p>El Gain Ratio se calcular\u00e1 de la siguiente forma:</p> \\[ \\text{GainRatio}(\\mathcal{D}, x_j) = \\frac{GI(\\mathcal{D}, j)}{\\text{SplitInfo}(\\mathcal{D}, x_j)} \\] <p>De esta forma, el factor Split Info se utiliza para penalizar subdivisiones excesivas y la falta de generalizaci\u00f3n. En la  se ilustra c\u00f3mo Gain Ratio penaliza la subdivisi\u00f3n excesiva frente a Information Gain.</p> <p>Figura 8: Comparaci\u00f3n entre Information Gain (ID3) y Gain Ratio (C4.5) </p>"},{"location":"03-arboles-decision/#atributos-continuos","title":"Atributos continuos","text":"<p>Aunque internamente el algoritmo sigue manejando atributos categ\u00f3ricos, permite introducir como entrada atributos continuas. En estos casos, el algoritmo discretizar\u00e1 de forma autom\u00e1tica estos atributos. </p> <p>Para ello, dado un atributo num\u00e9rico \\(x_j\\), realiza lo siguiente:</p> <ul> <li>Ordena todos los valores de \\(x_j\\).</li> <li>Prueba diferentes splits binarios \\(t\\), para dividir en dos categor\u00edas: \\(x_j \\leq t\\) y \\(x_j &gt; t\\). </li> <li>Se elige el umbral \\(t\\) que maximiza el Gain Ratio.</li> </ul>"},{"location":"03-arboles-decision/#valores-perdidos","title":"Valores perdidos","text":"<p>En caso de que existan valores perdidos, C4.5 asigna una probabilidad a cada posible valor del atributo en funci\u00f3n de la distribuci\u00f3n de probabilidad observada. Durante el c\u00e1lculo de la entrop\u00eda de los splits cada ejemplo con un valor perdido contribuir\u00e1 proporcionalmente a cada subrama.</p>"},{"location":"03-arboles-decision/#poda-posterior_1","title":"Poda posterior","text":"<p>Otra de las mejoras que introduce el algoritmo C4.5 frente a ID3 es la poda.</p> <p>Este algoritmo utiliza poda posterior con estimaci\u00f3n del error. No utiliza un conjunto de validaci\u00f3n separado, sino que realiza de forma estad\u00edstica una estimaci\u00f3n pesimista del error a partir del error emp\u00edrico. </p> <p>El algoritmo recorre el \u00e1rbol de abajo a arriba, evaluando primero los sub\u00e1rboles m\u00e1s profundos. Para cada nodo interno, estima el error del sub\u00e1rbol, y estima el error si ese nodo fuera una hoja. Si la diferencia no es significativa, aplica la poda. </p>"},{"location":"03-arboles-decision/#pasos-del-algoritmo_1","title":"Pasos del algoritmo","text":"<p>Los pasos del algoritmo son similares a los del algoritmo ID3, con algunas modificaciones:</p> <ol> <li>Si todas las instancias pertenecen a la misma clase se crea una hoja.</li> <li>Si no quedan atributos se crea una hoja con la clase mayoritaria.</li> <li>Se calcula el Gain Ratio para cada atributo.</li> <li>Se elige el atributo con mayor Gain Ratio.</li> <li>Se crea una subrama por cada valor del atributo (o umbral si es un atributo continuo).</li> <li>Repite el proceso recursivamente en cada subrama.</li> <li>Aplica poda posterior para reducir el overfitting.</li> </ol>"},{"location":"03-arboles-decision/#limitaciones-de-los-arboles-de-decision","title":"Limitaciones de los \u00e1rboles de decisi\u00f3n","text":"<p>A pesar de su simplicidad e interpretabilidad, los \u00e1rboles de decisi\u00f3n presentan limitaciones importantes, especialmente su elevada varianza y su sensibilidad a peque\u00f1as perturbaciones en los datos de entrenamiento. Un \u00e1rbol profundo puede producir overfitting, mientras que una poda excesiva puede conducir al underfitting. </p> <p>Estas limitaciones han motivado el desarrollo de m\u00e9todos que combinan m\u00faltiples \u00e1rboles con el objetivo de mejorar la capacidad de generalizaci\u00f3n. Los m\u00e9todos de ensemble se basan precisamente en esta idea: combinar de forma adecuada un conjunto de modelos sencillos, habitualmente \u00e1rboles de decisi\u00f3n, buscando reducir tanto sesgo como varianza.</p> <ol> <li> <p>Breiman, L., Friedman, J., Stone, C. J., &amp; Olshen, R. A. (1984). Classification and regression trees. CRC Press.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>James, G., Witten, D., Hastie, T., Tibshirani, R., &amp; Taylor, J. E. (2023). An introduction to statistical learning: With applications in python. Springer. https://doi.org/10.1007/978-3-031-38747-0 \u21a9</p> </li> <li> <p>Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1(1), 81--106. https://doi.org/10.1007/BF00116251 \u21a9</p> </li> <li> <p>Quinlan, J. R. (1993). C4.5: Programs for machine learning. Morgan Kaufmann.\u00a0\u21a9</p> </li> </ol>"},{"location":"04-random-forest/","title":"4. M\u00e9todos de ensemble. Random Forest","text":""},{"location":"04-random-forest/#metodos-de-ensemble","title":"M\u00e9todos de ensemble","text":"<p>La idea tras los modelos de ensemble (Sagi &amp; Rokach, 2018)1 consiste en combinar diferentes modelos base sencillos para construir un modelo m\u00e1s robusto y preciso que cualquiera de los modelos individuales. </p> <p>Hemos visto que modelos diferentes cometen errores diferentes. Buscamos combinarlos de forma que podamos eliminar o reducir esos errores individuales. Para ello necesitaremos combinar un conjunto diverso de modelos, y cada uno de estos modelos individuales deber\u00eda proporcionar por si mismo una precisi\u00f3n que sea superior al azar.</p> <p>El aprendizaje con m\u00e9todos de ensemble puede descomponerse en dos tareas principales:</p> <ul> <li>Aprender un conjunto de modelos base a partir de los datos de entrenamiento.</li> <li>Combinarlos para construir el predictor conjunto.</li> </ul> <p>Una de las principales ventajas de los ensembles es que pueden mejorar el compromiso entre sesgo y varianza. Recordemos que el error esperado se puede descomponer como: </p> \\[E[(y - \\hat{y})^2] = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\\] <p>Existen cuatro principales tipos de enfoques para abordar la combinaci\u00f3n de modelos: Voting, Stacking, Bagging y Boosting. Diferentes enfoques abordar\u00e1n el problema de forma distinta. Algunos tipos de m\u00e9todos de ensemble se centran en reducir principalmente la varianza, como por ejemplo los m\u00e9todos de Bagging, mientras que otros se centran fundamentalmente en el sesgo, como ser\u00eda el caso del Boosting. </p> <p>Cada categor\u00eda de m\u00e9todos de ensemble tiene un enfoque diferente para generar y combinar modelos, tal como se resume en la siguiente tabla:</p> Categor\u00eda Modelos Datos de entrenamiento Combinaci\u00f3n Entrenamiento Voting Heterog\u00e9neos Iguales Fija (votos/promedio) Independiente Stacking Heterog\u00e9neos Iguales Aprendida (meta-modelo) Dos niveles Bagging Homog\u00e9neos Bootstrap (diferentes) Fija (promedio) Independiente Boosting Homog\u00e9neos Ponderados/residuos Ponderada (aprendida) Secuencial <p>A continuaci\u00f3n estudiaremos en detalle cada una de estas categor\u00edas, y los principales m\u00e9todos que existen dentro de cada una de ellas.</p>"},{"location":"04-random-forest/#voting","title":"Voting","text":"<p>La idea tras los modelos de Voting (Kittler et al., 1998)2 es la de entrenar m\u00faltiples modelos independientes y combinar sus predicciones mediante votaci\u00f3n (en el caso de clasifiaci\u00f3n) o mediante promediado (en el caso de regresi\u00f3n). </p> <p>En este caso los modelos se entrenan de forma independiente con el mismo conjunto de datos, y no hay dependencia entre modelos, por lo que pueden entrenarse en paralelo. Adem\u00e1s, podemos combinar diferentes tipos de modelos. </p> <p>Por ejemplo, podr\u00edamos combinar un modelo de Regresi\u00f3n Log\u00edstica, con KNN y SVM, obtener la predicci\u00f3n que devuelve cada uno de ellos, y devolver aquella que obtenga m\u00e1s votos. </p> <p>Encontramos diferentes formas de abordar la votaci\u00f3n, que podremos aplicar seg\u00fan se trate de un problema de clasificaci\u00f3n o de regresi\u00f3n.</p> <p>Vamos a considerar que combinamos \\(M\\) clasificadores \\(\\{h_1(\\mathbf{x}), h_2(\\mathbf{x}), \\ldots, h_M(\\mathbf{x}) \\}\\). A continuaci\u00f3n veremos c\u00f3mo combinar sus predicciones con cada enfoque de votaci\u00f3n.</p>"},{"location":"04-random-forest/#hard-voting","title":"Hard Voting","text":"<p>Se trata de un enfoque dirigido al problema de clasificaci\u00f3n. Con el enfoque Hard Voting, la clase predicha por el clasificador ser\u00e1 la que reciba m\u00e1s votos por parte de los clasificadores individuales (es decir, la moda del conjunto de predicciones). </p> \\[\\hat{y} = \\text{mode}(h_1(\\mathbf{x}), h_2(\\mathbf{x}), ..., h_M(\\mathbf{x}))\\] <p>Ejemplo: </p><pre><code>Problema: Clasificar un email como spam o no-spam\nModelo 1 (Logistic Regression): spam\nModelo 2 (Decision Tree):       spam  \nModelo 3 (SVM):                 no-spam\nModelo 4 (KNN):                 spam\n\nResultado final: spam (3 votos contra 1)\n</code></pre><p></p>"},{"location":"04-random-forest/#soft-voting","title":"Soft Voting","text":"<p>A diferencia del caso anterior, con el enfoque Soft Voting lo que tendremos en cuenta es la suma de probabilidades de predicci\u00f3n de cada clasificador individual. Aquella clase \\(k\\) cuya suma de probabilidades de predicci\u00f3n sea mayor, ser\u00e1 la seleccionada como predicci\u00f3n del modelo combinado:</p> \\[\\hat{y} = \\arg\\max_k \\frac{1}{M} \\sum_{i=1}^{M} P_i(y = k | \\mathbf{x})\\] <p>Ejemplo: </p><pre><code>Modelo 1: P(spam) = 0.9, P(no-spam) = 0.1\nModelo 2: P(spam) = 0.6, P(no-spam) = 0.4\nModelo 3: P(spam) = 0.4, P(no-spam) = 0.6\n\nPromedio: P(spam) = 0.633, P(no-spam) = 0.367\nResultado: spam\n</code></pre><p></p> <p>Es importante destacar que para poder utilizar este enfoque, los modelos individuales deben poder proporcionarnos la probabilidad de la predicci\u00f3n. Este enfoque tiene la ventaja de que considera la confianza de cada modelo en la predicci\u00f3n realizada.</p>"},{"location":"04-random-forest/#soft-voting-ponderado","title":"Soft Voting ponderado","text":"<p>Se trata de un caso similar al anterior, pero dando un peso diferente a cada predictor individual en la suma:</p> \\[\\hat{y} = \\arg\\max_k \\sum_{i=1}^{M} w_i \\cdot P_i(y = k | \\mathbf{x})\\] <p>Donde \\(\\sum w_i = 1\\) y \\(w_i\\) nos permite dar mayor peso a los modelos en los que tengamos mayor confianza. Estos valores se pueden determinar a partir de la precisi\u00f3n de los modelos individuales en la validaci\u00f3n, de m\u00e9tricas como F1-score o el inverso del error cometido.</p> <p>Ejemplo: </p><pre><code>Modelo 1 (accuracy=0.85): w1 = 0.4\nModelo 2 (accuracy=0.80): w2 = 0.35  \nModelo 3 (accuracy=0.75): w3 = 0.25\n\nLa votaci\u00f3n ponderada da m\u00e1s peso al mejor modelo\n</code></pre><p></p>"},{"location":"04-random-forest/#promediado","title":"Promediado","text":"<p>Este enfoque, a diferencia de los anteriores, est\u00e1 dirigido al problema de regresi\u00f3n. En caso de regresi\u00f3n las predicciones de promedian de la siguiente forma:</p> \\[\\hat{y} = \\frac{1}{M} \\sum_{i=1}^{M} h_i(\\mathbf{x})\\] <p>Tambi\u00e9n es posible asignar un peso a cada predictor, igual que en el caso del enfoque anterior:</p> \\[\\hat{y} = \\sum_{i=1}^{M} w_i  h_i(\\mathbf{x})\\]"},{"location":"04-random-forest/#implementacion","title":"Implementaci\u00f3n","text":"<p>En sklearn tenemos las clases VotingClassifier y VotingRegressor, para problemas de clasificaci\u00f3n y regresi\u00f3n respectivamente.</p> <p>Para utilizar estas clases deberemos pasar como par\u00e1metro <code>estimator</code> la lista de predictores individuales que queramos combinar, y nos permitir\u00e1n aplicar cualquiera de los enfoques anteriores.</p> <p>En el caso de la clasificaci\u00f3n, podemos elegir el tipo de votaci\u00f3n con el par\u00e1metro <code>voting</code>, que podr\u00e1 tomar como valores <code>hard</code> o <code>soft</code>, y en el caso del segundo tipo con <code>weights</code> podemos especificar pesos espec\u00edficos para cada predictor.</p> <p>En el caso de la regresi\u00f3n, tambi\u00e9n contamos con un par\u00e1metro <code>weights</code> para especificar los pesos de cada predicci\u00f3n.</p>"},{"location":"04-random-forest/#consideraciones-finales-sobre-voting","title":"Consideraciones finales sobre Voting","text":"<p>Se trata de un m\u00e9todo muy f\u00e1cil de entender y sencillo de implementar, que se basa en modelos existentes que no es necesario modificar. Todos los modelos base se entrenan independientemente y podr\u00eda hacerse en paralelo. </p> <p>Debemos tener en cuenta que para que sea efectivo los diferentes modelos base deben ser diversos, ya que si todos son similares no obtendremos apenas ganancia combin\u00e1ndolos. Funcionar\u00e1 mejor cuando los modelos base capturen aspectos distintos de los datos y cometan errores diferentes. </p> <p>Este m\u00e9todo tambi\u00e9n ser ver\u00e1 beneficiado cuando los modelos tengan un rendimiento similar. Si uno de los modelos base fuera muy superior, ser\u00eda recomendable utilizar \u00fanicamente dicho modelo, mientras que si tenemos un modelo peor que el azar, ese modelo perjudicar\u00e1 al ensemble y convendr\u00eda eliminarlo.</p> <p>Voting es la forma m\u00e1s b\u00e1sica de modelo de ensemble, en la que se utiliza una combinaci\u00f3n fija de modelos (votos o promedios con pesos predefinidos). Esto nos lleva a preguntarnos si podr\u00edamos aprender la mejor forma de combinar los modelos. Esto lo abordaremos con los m\u00e9todos de Stacking que veremos a continuaci\u00f3n.</p>"},{"location":"04-random-forest/#stacking","title":"Stacking","text":"<p>En el caso de Stacking (Wolpert, 1992)3, al igual que en Voting tenemos una serie de modelos base heterog\u00e9neos que entrenamos de forma independiente, pero a diferencia del caso anterior, no tendremos una combinaci\u00f3n fija, sino que utilizaremos un meta-modelo para aprender la forma de combinar los diferentes modelos base. </p> <p>De esta forma, se podr\u00e1n capturar relaciones complejas entre las diferentes predicciones. Tendremos una arquitectura en dos niveles, en la que en el nivel inferior tendremos los \\(M\\) diferentes modelos base, cada uno de los cuales producir\u00e1 una predicci\u00f3n \\(p_i\\) y en el nivel superior tendremos el meta-modelo, que recibir\u00e1 como entrada las diferentes predicciones \\(\\{ p_1, p_2, \\ldots, p_M \\}\\) y producir\u00e1 como salida la predicci\u00f3n del ensemble.</p> <pre><code>NIVEL 0 (Modelos base):\n\u251c\u2500 Modelo 1 (ej: DT)               \u2192 predicci\u00f3n p1\n\u251c\u2500 Modelo 2 (ej: SVM)              \u2192 predicci\u00f3n p2\n\u251c\u2500 Modelo 3 (ej: Logistic Reg)     \u2192 predicci\u00f3n p3\n\u2514\u2500 Modelo 4 (ej: KNN)              \u2192 predicci\u00f3n p4\n\nNIVEL 1 (Meta-modelo):\n\u2514\u2500 Recibe [p1, p2, p3, p4] como features\n   \u2192 aprende la mejor combinaci\u00f3n\n   \u2192 predicci\u00f3n final\n</code></pre>"},{"location":"04-random-forest/#algoritmo-de-entrenamiento","title":"Algoritmo de entrenamiento","text":"<p>El entrenamiento de un ensemble de tipo Stacking se har\u00e1 en varios pasos, ya que en primer lugar deberemos obtener una serie de predicciones de los modelos base para entrenar con ellas el meta-modelo. Para reducir el overfitting, utilizaremos cross-validation a la hora de generar estas predicciones, siguiendo el siguiente proceso:</p> <p>Paso 1: Obtenci\u00f3n de predicciones para el meta-modelo (Nivel 0)</p> <p>Supongamos que contamos con \\(M\\) modelos base \\(m_j\\), con \\(j=1, 2, \\ldots, M\\). Entrenaremos cada uno de estos modelos utilizando cross-validation, y para cada fold:</p> <ul> <li>Dividimos el dataset en datos de entrenamiento y datos de validaci\u00f3n. </li> <li>Cada modelo base se entrena con el conjunto de entrenamiento del fold</li> <li>Se generan predicciones para los datos del conjunto de validaci\u00f3n. Estas son predicciones out-of-fold (OOF), ya que han sido generadas para cada observaci\u00f3n utilizando un modelo que no ha sido entrenado con dicha observaci\u00f3n.</li> <li>Guardamos las predicciones OOF generadas.</li> </ul> <p>De esta forma, obtendremos un nuevo conjunto de datos compuesto por las predicciones OOF generadas por cada modelo base para cada ejemplo de entrada. </p> <p>Considerando que tenemos \\(N\\) ejemplos de entrada en nuestro dataset y \\(M\\) modelos, tendremos una matriz como la siguiente:</p> \\[ Z =  \\begin{bmatrix} \\hat{y}_1^{m_1} &amp; \\hat{y}_1^{m_2} &amp; \\ldots &amp; \\hat{y}_1^{m_M} \\\\ \\hat{y}_2^{m_1} &amp; \\hat{y}_2^{m_2} &amp; \\ldots &amp; \\hat{y}_2^{m_M} \\\\ \\vdots &amp; \\vdots &amp;   &amp; \\vdots \\\\ \\hat{y}_N^{m_1} &amp; \\hat{y}_N^{m_2} &amp; \\ldots &amp; \\hat{y}_N^{m_M} \\\\ \\end{bmatrix}  \\] <p>Esta matriz \\(Z\\) constituir\u00e1 los datos de entrada para el entrenamiento del meta-modelo. Es importante haber utilizado las predicciones OOF para construirla, ya que si hubi\u00e9ramos obtenido predicciones de los modelos base obtenidas a partir de datos vistos durante el entrenamiento, habr\u00edamos tenido un caso demasiado optimista y habr\u00edamos favorecido el overfitting.</p> <p>Paso 2: Entrenar el meta-modelo (Nivel 1)</p> <p>En este punto utilizamos la matriz \\(Z\\) de predicciones OOF generada en el paso anterior como entrada del meta-modelo. Considerando que \\(\\mathbf{z}_i\\) (fila \\(i\\)-\u00e9sima de la matriz \\(Z\\)) es una tupla con las predicciones OOF generadas por cada uno de los \\(M\\) modelos para el ejemplo de entrada \\(\\mathbf{x}_i\\), al entrenar el meta-modelo la salida esperada ser\u00e1 la etiqueta original \\(y_i\\). </p> <p>Es decir, utilizaremos para entrenar el meta-modelo un conjunto de pares \\((\\mathbf{z}_i, y_i)\\), con \\(i=1, 2, \\ldots, N\\). </p> <p>Paso 3: Entrenamiento definitivo de los modelos base (Nivel 0)</p> <p>Dado que queremos tener el mejor modelo posible, entrenaremos ahora de nuevo todos los modelos base pero utilizando el dataset completo. De esta forma, guardaremos estos modelos base reentrenados junto con el meta-modelo entrenado en el paso anterior, y con esto tendremos el modelo completo.</p> <p>Predicci\u00f3n</p> <p>Una vez entrenados de forma definitiva modelos base y meta-modelo, a la hora de obtener una predicci\u00f3n con un nuevo dato \\(\\mathbf{x}\\) seguiremos el siguiente proceso</p> <ol> <li>Cada modelo base \\(m_j\\), con \\(j=1,2,\\ldots,M\\), produce una predicci\u00f3n \\(z_j\\) para \\(\\mathbf{x}\\).</li> <li>El meta-modelo recibe como entrada la tupla \\((z_1, z_2, \\ldots, z_M)\\) de predicciones realizadas por los modelos base.</li> <li>El meta-modelo genera predicci\u00f3n final \\(y\\).</li> </ol>"},{"location":"04-random-forest/#implementacion_1","title":"Implementaci\u00f3n","text":"<p>En la librer\u00eda sklearn contamos con las clases StackingClassifier y StackingRegressor para implementar Stacking tanto en problemas de clasificaci\u00f3n como de regresi\u00f3n. </p> <p>A la hora de construir este ensemble, deberemos proporcionar tanto un conjunto de modelos base, en el par\u00e1metro <code>estimators</code>, como un meta-modelo, en <code>final_estimator</code>. A continuaci\u00f3n, mostramos un ejemplo de c\u00f3digo con 3 modelos base, y con regresi\u00f3n log\u00edstica como meta-modelo:</p> <pre><code># Definir modelos base (nivel 0)\nbase_models = [\n('rf', RandomForestClassifier(n_estimators=100)),\n('gb', GradientBoostingClassifier(n_estimators=100)),\n('svm', SVC(probability=True))\n]\n# Definir meta-modelo (nivel 1)\nmeta_model = LogisticRegression()\n# Crear stacking ensemble\nstacking = StackingClassifier(\nestimators=base_models,\nfinal_estimator=meta_model\n)\n# Entrenar\nstacking.fit(X_train, y_train)\n# Predecir\npredictions = stacking.predict(X_test)\n</code></pre>"},{"location":"04-random-forest/#variantes-de-stacking","title":"Variantes de Stacking","text":"<p>Podemos encontrar diferentes variantes de Stacking. Vamos a continuaci\u00f3n a describir algunas de ellas.</p> <p>Una de las variantes es la conocida como Blending. Se diferencia de Stacking b\u00e1sicamente en que en lugar de utilizar un K-Fold para generar predicciones OOF para el entrenamiento del meta-modelo, utiliza un \u00fanico particionamiento en conjunto de entrenamiento y conjunto de validaci\u00f3n. Por ejemplo, podemos particionar con un 80% de los datos para entrenamiento y 20% para validaci\u00f3n, con lo cual, s\u00f3lo se estar\u00edan generando predicciones con el 20% de los datos. Blending resulta m\u00e1s sencillo y r\u00e1pido, pero estaremos entrenando el meta-modelo con menos datos. Con un dataset peque\u00f1o podemos perder rendimiento y tendremos mayor varianza, pero en caso de tener un dataset grande y buscar reducir el coste computacional blending puede ser una opci\u00f3n adecuada.</p> <p>Otra variante a considerar es el Stacking multi-nivel. Este m\u00e9todo consiste en apilar m\u00faltiples niveles de meta-modelos: </p><pre><code>NIVEL 0: Modelos base \n    \u2193 (predicciones)\nNIVEL 1: Primer grupo de meta-modelos\n    \u2193 (predicciones)\nNIVEL 2: Meta-meta-modelo final\n    \u2193\nPredicci\u00f3n final\n</code></pre><p></p> <p>Esta t\u00e9cnica tiene como ventaja que puede ser capaz de capturar relaciones muy complejas, pero tiene mayor riesgo de overfitting y resulta dif\u00edcil de interpretar. </p> <p>Seg\u00fan el tipo de meta-modelo, podr\u00edamos agrupar los tipos de Stacking en tres grandes categor\u00edas:</p> <ul> <li>Linear Stacking: El meta-modelo es un modelo lineal, t\u00edpicamente Regresi\u00f3n Lineal o Regresi\u00f3n Log\u00edstica.</li> <li>Tree Stacking: El meta-modelo es un modelo basado en \u00e1rboles, como \u00c1rboles de Decisi\u00f3n o Random Forest.</li> <li>Neural Stacking: El meta-modelo es una Red Neuronal.</li> </ul> <p>Tambi\u00e9n podemos considerar varias variantes seg\u00fan el tipo de meta-features (features generadas para el meta-modelo):</p> <ul> <li>Predicciones solo: Tenemos como meta-features \u00fanicamente las predicciones de los modelos base. Por ejemplo, tendr\u00edamos \u00fanicamente \\(0\\) o \\(1\\) en caso de clasificaci\u00f3n binaria. Si queremos que sklearn utilice este tipo de predicciones, podemos proporcionar el par\u00e1metro <code>stack_method='predict'</code>. </li> <li>Probabilidad: Como meta-features tendr\u00edamos las probabilidades de pertenencia a cada clase. Para que sklearn utilice este tipo de caracter\u00edsticas, todos los modelos base deben contar con el m\u00e9todo <code>predict_proba</code>. Como por defecto tenemos <code>stack_method='auto'</code>, si todos los modelos base cuentan con <code>predict_proba</code> entonces utilizar\u00e1 de forma preferente estas probabilidades.</li> <li>Predicciones y features originales: Concatenemos las features originales del ejemplo de entrada con las predicciones realizadas por los modelos base. En sklearn esto lo conseguiremos proporcionando el par\u00e1metro <code>passthrough=True</code>.</li> </ul>"},{"location":"04-random-forest/#consideraciones-finales-sobre-stacking","title":"Consideraciones finales sobre Stacking","text":"<p>Stacking lleva la idea de ensemble un paso m\u00e1s all\u00e1 que Voting, ya que en lugar de usar combinaciones fijas, aprende la mejor forma de combinar modelos. Resulta muy flexible, pudiendo utilizar cualquier tipo de modelo en cualquier nivel y normalmente ofrecer\u00e1 mejor rendimiento que Voting.</p> <p>Sin embargo, es m\u00e1s complejo, ya que debemos ajustar un gran n\u00famero de hiper-par\u00e1metros en sus diferentes niveles. Tiene un alto coste computacional, ya que tenemos que entrenar tanto modelos base como meta-modelo, y es dif\u00edcil de interpretar.</p> <p>Adem\u00e1s, Stacking es propenso al overfitting y para mitigarlo es importante, tal como hemos comentado, utilizar cross-validation para generar las meta-features. Otras estrategias que pueden ayudar a mitigar este problema es utilizar un meta-modelo sencillo, como puede ser Regresi\u00f3n Log\u00edstica, y utilizar siempre regularizaci\u00f3n en el meta-modelo. </p> <p>Stacking ser\u00e1 un m\u00e9todo adecuado cuando contemos con modelos buenos y diversos, y un conjunto de datos suficientemente grande como para evitar el overfitting. Sin embargo, con conjuntos de datos peque\u00f1os tendremos un alto riesgo de overfitting. </p> <p>Hasta ahora hemos visto m\u00e9todos que combinan modelos heterog\u00e9neos entrenados en los mismos datos, pero, \u00bfy si en lugar de esto buscamos la diversidad entrenando un mismo modelo con diferentes muestras de datos?. Esta pregunta nos lleva a explorar el siguiente tipo de m\u00e9todos de ensemble, el conocido como Bagging.</p>"},{"location":"04-random-forest/#bagging-bootstrap-aggregating","title":"Bagging (Bootstrap Aggregating)","text":"<p>A diferencia de Voting y Stacking, los m\u00e9todos de Bagging (Breiman, 1996)4 se basan en entrenar m\u00faltiples modelos homog\u00e9neos en diferentes muestras bootstrap (con reemplazo) del conjunto de entrenamiento, promediando sus predicciones.</p> <p>Con esta t\u00e9cnica se busca principalmente reducir la varianza, con lo que va a ser efectiva principalmente cuando se aplique a modelos con alta varianza, como es el caso de los \u00e1rboles de decisi\u00f3n. Los modelos pueden entrenarse en paralelo, cada uno de ellos con una muestra diferente de datos. Utiliza muestreo con reemplazo (bootstrap sampling) para obtener la muestra con la que se entrenar\u00e1 cada modelo. Es tambi\u00e9n importante destacar que se le da la misma importancia a todas las predicciones. Estas se combinar\u00e1n mediante promedio en caso de regresi\u00f3n, y mediante votaci\u00f3n en caso de clasificaci\u00f3n.</p>"},{"location":"04-random-forest/#bootstrap-sampling","title":"Bootstrap Sampling","text":"<p>El t\u00e9rmino Bootstrap se refiere a una t\u00e9cnica estad\u00edstica de remuestreo con reemplazo. A partir de un dataset original \\(\\mathcal{D}\\) con \\(N\\) ejemplos obtenemos \\(B\\) bootstrap samples \\(\\mathcal{D}_b\\), con \\(b=1, 2, \\ldots, B\\). Cada bootstrap sample tendr\u00e1 tambi\u00e9n \\(N\\) ejemplos, pero podr\u00e1 haber ejemplos del conjunto original repetidos o ausentes (out-of-bag). </p> <p>Por ejemplo, considerando \\(N=10\\), podr\u00edamos tener:</p> \\[ \\begin{align*} \\mathcal{D} &amp;= \\{ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\\} \\\\ \\Downarrow \\\\ \\mathcal{D}_1 &amp;= [1, 3, 3, 5, 7, 8, 8, 9, 10, 10] \\\\  \\mathcal{D}_2 &amp;= [1, 2, 2, 4, 4, 5, 6, 8, 9, 9] \\\\  &amp; \\ldots \\\\ \\mathcal{D}_B &amp;= [2, 3, 4, 5, 5, 6, 7, 7, 8, 10] \\end{align*} \\] <p>Podemos calcular la probabilidad de que un ejemplo sea out-of-bag como \\((1-\\frac{1}{N})^N\\), y con un valor alto de \\(N\\) est\u00e1 probabilidad converge a \\(0.368\\). Por lo tanto, aproximadamente el \\(37 \\%\\) de los ejemplos quedar\u00e1n fuera de cada muestra.</p>"},{"location":"04-random-forest/#algoritmo-de-bagging","title":"Algoritmo de Bagging","text":"<p>A continuaci\u00f3n se muestra el algoritmo de entrenamiento de Bagging:</p> \\[ \\begin{align*} &amp; \\text{Entrada: } \\mathcal{D} \\\\ &amp; \\text{Para } b=1 \\text{ hasta } B:\\\\ &amp; \\quad \\mathcal{D}_b \\leftarrow \\text{Seleccionar, con reemplazo } N \\text{ ejemplos del conjunto } \\mathcal{D} \\\\ &amp; \\quad h_b \\leftarrow \\text{Entrenar un modelo con }\\mathcal{D}_b \\\\ &amp; \\text{Devuelve: } H = \\{ h_1, h_2, \\ldots, h_B \\} \\end{align*} \\] <p>Una vez entrenado el ensemble \\(H\\), la predicci\u00f3n en caso de regresi\u00f3n se calcular\u00e1 como el promedio de las predicciones de cada modelo del ensemble:</p> \\[ \\hat{y} = \\frac{1}{B} \\sum_{b=1}^B h_b(\\mathbf{x}) \\] <p>La predicci\u00f3n en caso de clasificaci\u00f3n se obtendr\u00e1 mediante votaci\u00f3n (moda del conjunto de predicciones):</p> \\[ \\hat{y} = \\text{moda}(h_1(\\mathbf{x}), h_2(\\mathbf{x}), \\ldots, h_B(\\mathbf{x})) \\] <p>Si cada modelo individual \\(h_b\\) proporciona una probabilidad \\(P_b(y=k|\\mathbf{x})\\) de que el ejemplo de entrada \\(\\mathbf{x}\\) pertenezca a la clase \\(k\\), entonces tambi\u00e9n podr\u00edamos calcular la probabilidad global del ensemble mediante promediado:</p> \\[ P(y=k|\\mathbf{x}) = \\frac{1}{B} \\sum_{b=1}^B P_b(y=k|\\mathbf{x}) \\]"},{"location":"04-random-forest/#out-of-bag-oob-error","title":"Out-of-Bag (OOB) Error","text":"<p>Como hemos comentado anteriormente, aproximadamente el 37% de las muestras no aparecen en cada bootstrap sample. Estas muestras out-of-bag se pueden usar para validaci\u00f3n sin necesidad de un conjunto separado.</p> <p>A continuaci\u00f3n se muestra el algoritmo para el c\u00e1lculo del OOB error:</p> \\[ \\begin{align*} &amp; \\text{Entrada: } \\mathcal{D} \\\\ &amp; \\text{Para cada } (\\mathbf{x}_i, y_i) \\in \\mathcal{D}\\\\ &amp; \\quad \\mathcal{B}_i \\leftarrow \\{b: (\\mathbf{x}_i, y_i) \\notin \\mathcal{D}_b  \\} \\quad \\text{(Modelos que no usaron } \\mathbf({x}_i, y_i) \\text{ en el entrenamiento)} \\\\ &amp; \\quad \\text{Si } |\\mathcal{B}_i| &gt; 0: \\quad \\text{(Calcula predicciones para estos modelos) }\\\\ &amp; \\quad\\quad  \\hat{y}_i^{\\text{OOB}} = \\begin{cases}     \\text{moda}\\{h_b(\\mathbf{x}_i) : b \\in \\mathcal{B}_i\\} \\quad \\text{(Clasificaci\u00f3n)} \\\\     \\frac{1}{|\\mathcal{B}_i|} \\sum_{b \\in \\mathcal{B}_i} h_b(\\mathbf{x}_i) \\quad \\text{(Regresi\u00f3n)} \\end{cases}  \\\\ &amp; \\mathcal{I}_{OOB} \\leftarrow \\{ i: |\\mathcal{B}_i| &gt; 0 \\} \\quad \\text{(\u00cdndices de las muestras que fueron OOB para al menos un modelo)} \\\\  &amp;OOBError \\leftarrow \\begin{cases}     \\frac{1}{|\\mathcal{I}_{OOB}|} \\sum_{i \\in \\mathcal{I}_{OOB}} \\mathbb{1}(\\hat{y}_i^{\\text{OOB}} \\neq y_i)  \\quad \\text{(Clasificaci\u00f3n)} \\\\     \\frac{1}{|\\mathcal{I}_{OOB}|} \\sum_{i \\in \\mathcal{I}_{OOB}} (y_i - \\hat{y}_i^{\\text{OOB}})^2 \\quad \\text{(Regresi\u00f3n)} \\end{cases} \\end{align*} \\] <p>De esta forma podemos estimar el error de generalizaci\u00f3n sin necesitar un conjunto de validaci\u00f3n separado y sin suponer un coste computacional adicional</p>"},{"location":"04-random-forest/#analisis-del-metodo","title":"An\u00e1lisis del m\u00e9todo","text":"<p>Vamos a analizar a continuaci\u00f3n los motivos por los que la t\u00e9cnica de Bagging ayuda a reducir la varianza. Vamos a empezar haciendo un estudio a nivel te\u00f3rico.</p> <p>Supongamos que tenemos B modelos independientes (sin correlaci\u00f3n entre ellos) con varianza \\(\\sigma^2\\) cada uno. Si los promediamos:</p> \\[\\text{Var}(\\text{promedio}) = \\text{Var}\\left(\\frac{1}{B}\\sum_{i=1}^B h_i\\right) = \\frac{1}{B^2} \\sum_{i=1}^B \\text{Var}(h_i) = \\frac{\\sigma^2}{B}\\] <p>La varianza se reduce por un factor de B. Sin embargo, en la pr\u00e1ctica los modelos no son completamente independientes, ya que est\u00e1n entrenados con muestras correlacionadas, pero a\u00fan as\u00ed la reducci\u00f3n de varianza es significativa. Si consideramos que tenemos una correlaci\u00f3n \\(\\rho\\) entre modelos, entonces tendr\u00edamos:</p> \\[\\text{Var}(\\text{ensemble}) = \\rho\\sigma^2 + \\frac{1-\\rho}{B}\\sigma^2\\] <p>Si los modelos predicen exactamente lo mismo entonces tendremos correlaci\u00f3n \\(\\rho=1\\), y no obtendremos ninguna ganancia con el ensemble. Sin embargo, conforme consigamos reducir la correlaci\u00f3n podremos reducir la varianza del modelo. Vemos que el primer t\u00e9rmino de la funci\u00f3n anterior solo se puede reducir reduciendo la correlaci\u00f3n, mientras que el segundo t\u00e9rmino se podr\u00eda reducir aumentando el n\u00famero de modelos. El caso ideal te\u00f3rico ser\u00eda conseguir \\(\\rho=0\\), pero es dif\u00edcil conseguirlo en la pr\u00e1ctica. Lo principal a tener en cuenta es que a mayor correlaci\u00f3n, la ganancia ser\u00e1 menor, y por ello es importante la diversidad de los modelos.</p>"},{"location":"04-random-forest/#implementacion-de-bagging-generico","title":"Implementaci\u00f3n de Bagging Gen\u00e9rico","text":"<p>La librer\u00eda sklearn incluye las clase <code>BaggingClassifier</code> y <code>BaggingRegressor</code> que nos permiten utilizar de forma gen\u00e9rica est\u00e1 t\u00e9cnica, tanto para problemas de clasificaci\u00f3n como de regresi\u00f3n.</p> <p>Deberemos proporcionar el modelo base que queremos entrenar en el par\u00e1metro <code>estimator</code>. Por defecto utilizar\u00e1 como clasificador base \u00c1rboles de Decisi\u00f3n. Podemos tambi\u00e9n indicar la cantidad \\(B\\) de clasificadores a entrenar en el par\u00e1metro <code>num_estimators</code>. </p> <p>Es interesante observar que esta implementaci\u00f3n nos da gran flexibilidad para la implementaci\u00f3n del m\u00e9todo. Por ejemplo, con <code>max_samples</code> podemos indicar el n\u00famero de ejemplos que tendr\u00e1 cada muestra generada (por defecto tendr\u00e1 \\(N\\) ejemplos, tantos como el conjunto de entrada original), y tambi\u00e9n podemos indicar con el par\u00e1metro <code>bootstrap</code> si queremos que muestree con reemplazo o sin reemplazo. Es recomendable que este par\u00e1metro tenga siempre valor <code>True</code>, que es la opci\u00f3n por defecto. Tambi\u00e9n tenemos el par\u00e1metro <code>oob_score</code> que nos permite indicar si queremos utilizar los ejemplos out-of-bag para estimar el error de generalizaci\u00f3n (esto solo es posible si se utiliza muestre con reemplazo).</p> <p>Adem\u00e1s, no solo nos permite muestrear con reemplazo los ejemplos de entrada, sino que tambi\u00e9n nos permite hacer lo mismo con las features (esto como veremos a continuaci\u00f3n es algo que incorporan los Randon Forest). Con <code>max_features</code> y <code>bootstrap_feature</code> podemos indicar el n\u00famero de features que queremos seleccionar en cada muestra y si queremos que se puedan seleccionar con reemplazo, respectivamente. En este caso por defecto est\u00e1 establecido que no se realice muestreo con reemplazo de las features. </p> <pre><code># Bagging con \u00e1rboles de decisi\u00f3n profundos\nbagging_tree = BaggingClassifier(\nestimator=DecisionTreeClassifier(max_depth=None),  # \u00c1rbol sin restricciones\nn_estimators=100,           # N\u00famero de modelos\nmax_samples=1.0,            # Usar 100% de muestras (por defecto)\nmax_features=1.0,           # Usar 100% de features (por defecto)\nbootstrap=True,             # Con reemplazo (por defecto)\noob_score=True              # Calcular OOB error\n)\nbagging_tree.fit(X_train, y_train)\nprint(f\"OOB Score: {bagging_tree.oob_score_:.4f}\")\n</code></pre>"},{"location":"04-random-forest/#consideraciones-finales-sobre-bagging","title":"Consideraciones finales sobre Bagging","text":"<p>Las t\u00e9cnicas de Bagging est\u00e1n enfocadas a la reducci\u00f3n de la varianza, y por lo tanto son muy efectivas con modelos con alta varianza, como es el caso de los \u00c1rboles de Decisi\u00f3n profundos. El ensemble es m\u00e1s robusto frente a outliers que un modelo individual.</p> <p>Es paralelizable, ya que todos los modelos se pueden entrenar independientemente, y resulta sencillo de implementar y entender. Adem\u00e1s, nos permite validar sin coste adicional mediante la evaluaci\u00f3n OOB. </p> <p>Sin embargo, entre sus limitaciones encontramos que no reduce sesgo. Adem\u00e1s, es menos interpretable que un modelo invidual y computacionalmente m\u00e1s cosoto. </p> <p>Ser\u00e1 interesante utilizar Bagging con modelos con alta varianza, si queremos reducir el overfitting y si el modelo base es r\u00e1pido de entrenar. Por estos motivos, podemos encontrar un mayor beneficio al aplicar Bagging con modelos como \u00c1rboles de Decisi\u00f3n sin poda o KNN con \\(K\\) peque\u00f1o, o en general cualquier modelo con alta varianza y bajo sesgo. Sin embargo, otros modelos como Regresi\u00f3n Log\u00edstica que ya tienen baja varianza obtendr\u00e1n normalmente poco beneficio.</p> <p>Centr\u00e1ndonos en el caso de Bagging con \u00c1rboles de Decisi\u00f3n, como hemos comentado, Bagging reduce efectivamente la varianza al promediar m\u00faltiples \u00e1rboles entrenados con diferentes muestras bootstrap. Sin embargo, los \u00e1rboles resultantes tienden a ser similares entre s\u00ed. Dado que todos consideran las mismas caracter\u00edsticas en cada divisi\u00f3n, suelen seleccionar las variables m\u00e1s informativas en los nodos superiores, generando estructuras correlacionadas. Esta correlaci\u00f3n limita la reducci\u00f3n de varianza que puede conseguir el ensemble. Random Forest introduce una modificaci\u00f3n sencilla pero efectiva: en cada divisi\u00f3n del \u00e1rbol, en lugar de considerar todas las caracter\u00edsticas disponibles, selecciona un subconjunto aleatorio de ellas. Esta aleatoriedad adicional reduce sustancialmente la correlaci\u00f3n entre \u00e1rboles, permitiendo una mayor reducci\u00f3n de varianza y mejorando significativamente el rendimiento del ensemble.</p>"},{"location":"04-random-forest/#random-forest","title":"Random Forest","text":"<p>Random Forest (Breiman, 2001)5 es el m\u00e9todo de Bagging m\u00e1s popular, espec\u00edficamente dise\u00f1ado y optimizado para \u00e1rboles de decisi\u00f3n. La innovaci\u00f3n fundamental que introducen es combinar Bagging con selecci\u00f3n aleatoria de caracter\u00edsticas. </p> <p>Para aumentar la diversidad entre los \u00e1rboles, Random Forest introduce dos fuentes de aletoriedad:</p> <ol> <li> <p>Bootstrap sampling (como Bagging est\u00e1ndar):: Cada \u00e1rbol se entrena en una muestra bootstrap diferente, con lo que aproximadamente el 37% de ejemplos quedan out-of-bag en cada \u00e1rbol.</p> </li> <li> <p>Random feature selection:: En el split de cada nodo solo se considera un subconjunto aleatorio de \\(m\\) features, ayudando a reducir la correlaci\u00f3n entre \u00e1rboles.</p> </li> </ol>"},{"location":"04-random-forest/#algoritmo-de-random-forest","title":"Algoritmo de Random Forest","text":"<p>Detallamos a continuaci\u00f3n el algoritmo para el entrenamiento de un modelo de tipo Random Forest:</p> \\[ \\begin{align*} &amp; \\text{Entrada: Dataset } \\mathcal{D} \\text{, n\u00famero de \u00e1rboles } B \\text{, n\u00famero de feature } m \\\\ &amp; \\text{Para } b=1 \\text{ hasta } B:\\\\ &amp; \\quad \\mathcal{D}_b \\leftarrow \\text{Seleccionar, con reemplazo } N \\text{ ejemplos del conjunto } \\mathcal{D} \\\\ &amp; \\quad T_b \\leftarrow \\text{Crear \u00e1rbol} \\\\ &amp; \\quad \\text{Para cada nodo en } T_b: \\\\ &amp; \\quad \\quad \\text{Seleccionar } m \\text{ features al azar del total } d  \\\\ &amp; \\quad \\quad \\text{Encontrar el mejor split utilizando solo esas features} \\\\ &amp; \\quad \\quad \\text{Dividir el nodo con el mejor split} \\\\ &amp; \\quad \\quad \\text{Hacer crecer el \u00e1rbol hasta profundidad m\u00e1xima (sin poda)} \\\\ &amp; \\text{Devuelve: } \\text{Random Forest} \\{ T_1, T_2, \\ldots, T_B \\} \\end{align*} \\] <p>Podemos observar que la principal diferencia con Bagging puro de \u00e1rboles es que con Bagging en cada split se estar\u00edan considerando siempre todas las \\(d\\) features,  mientras que con Random Forest en cada split se considera un conjunto aleatorio de features, siendo \\(m \\ll d\\).</p>"},{"location":"04-random-forest/#hiperparametros","title":"Hiperpar\u00e1metros","text":"<p>En sklearn contamos con las implementaciones RandomForestClassifier y RandomForestRegressor de este modelo. A continuaci\u00f3n enumeramos los principales hiperpar\u00e1metros a tener en cuenta:</p> <ul> <li> <p><code>n_estimators</code> (n\u00famero de \u00e1rboles \\(B\\)): Obtendremos mayor rendimiento cuantos m\u00e1s \u00e1rboles utilicemos (hasta la convergencia), sin llegar a causar overfitting, pero cuanto mayor sea este n\u00famero mayor coste computacional tendremos. T\u00edpicamente toma valores entre \\(100\\) (por defecto) y \\(500\\).</p> </li> <li> <p><code>max_features</code> (features por split \\(m\\)): El valor por defecto cambia seg\u00fan estemos en un problema de clasificaci\u00f3n (\\(m = \\sqrt{d}\\)) o de regresi\u00f3n (\\(m=d\\)). Valores menores de este par\u00e1metro reducir\u00e1n la correlaci\u00f3n, pero pueden aumentar el sesgo.</p> </li> <li> <p><code>bootstrap</code> (usar bootstrap sampling): Por defecto toma valor <code>True</code>. En caso de cambiar a <code>False</code> utilizar\u00eda siempre el dataset completo.</p> </li> <li> <p><code>oob_score</code> (calcular error OOB): Por defecto es <code>False</code>. El c\u00e1lculo del error OOB solo est\u00e1 disponible si <code>bootstrap=True</code>. </p> </li> </ul> <p>Los siguientes par\u00e1metros nos permiten ajustar c\u00f3mo se construyen los \u00e1rboles individuales:</p> <ul> <li> <p><code>max_depth</code> (profundidad m\u00e1xima de cada \u00e1rbol). Por defecto es <code>None</code> (sin l\u00edmite), con lo que los \u00e1rboles crecen completamente. Sin introducimos valores menores tendremos menos overfitting, pero mayor sesgo.</p> </li> <li> <p><code>min_samples_split</code> (m\u00ednimo de muestras para dividir un nodo): Por defecto toma el valor \\(2\\). Con valores mayores tendremos \u00e1rboles m\u00e1s simples. </p> </li> <li> <p><code>min_samples_leaf</code> (m\u00ednimo de muestras en una hoja). Por defecto toma el valor \\(1\\). Solo se dividir\u00e1 un nodo si en cada una de las hojas resultantes hay al menos este n\u00famero de muestras. Con valores m\u00e1s altos tendremos una regularizaci\u00f3n m\u00e1s fuerte que producir\u00e1 modelos m\u00e1s suaves, especialmente en el caso de regresi\u00f3n.</p> </li> <li> <p><code>max_leaf_nodes</code> (m\u00e1ximo n\u00famero de hojas). Por defecto toma valor <code>None</code> (sin l\u00edmite). Nos permite controlar la complejidad del \u00e1rbol. </p> </li> </ul>"},{"location":"04-random-forest/#feature-importance","title":"Feature Importance","text":"<p>Una vez entrenado el modelo de Random Forest, podemos calcular la importancia de cada caracter\u00edstica. Esta informaci\u00f3n nos ser\u00e1 de utilidad para:</p> <ul> <li> <p>Interpretabilidad del modelo: Nos permite identificar qu\u00e9 caracter\u00edsticas son m\u00e1s relevantes para las predicciones. Esta informaci\u00f3n nos ayudar\u00e1 a explicar el comportamiento del modelo y validar que se est\u00e9n utilizando caracter\u00edsticas con sentido. </p> </li> <li> <p>Selecci\u00f3n de caracter\u00edsticas: Nos permite simplificar el modelo eliminando caracter\u00edsticas no relevantes. Esto reducir\u00e1 el coste computacional, puede mejorar la generalizaci\u00f3n, eliminando ruido, y facilitar\u00e1 el despliegue del modelo al requerir menos datos de entrada. </p> </li> </ul> <p>Podemos calcular la importancia de cada feature de dos formas diferentes.</p>"},{"location":"04-random-forest/#gini-importance","title":"Gini Importance","text":"<p>Esta primera forma de medir la importancia, tambi\u00e9n conocida como Mean Decrease in Impurity (MDI) est\u00e1 basada en la mejora promedio de la impureza cuando se usa esa feature:</p> \\[\\text{Importance}(x_j) = \\frac{1}{B} \\sum_{b=1}^{B} \\sum_{t \\in T_b} \\Delta I_t \\cdot \\mathbb{1}(v_t = j)\\] <p>Donde \\(\\Delta I_t\\) es la reducci\u00f3n en impureza en el nodo \\(t\\) y \\(v_t\\) es la feature usada en el split del nodo \\(t\\). Esta medida suma toda la reducci\u00f3n de impureza de los nodos en los que se utiliza \\(j\\) como feature para dividir, y promedia sobre todos los \u00e1rboles \\(B\\). </p> <p>A continuaci\u00f3n podemos ver c\u00f3mo obtener este valor de impureza con sklearn:</p> <pre><code># Calcular importancia\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\n# Ordena por importancia y muestra un ranking de caracter\u00edsticas\nindices = np.argsort(importances)[::-1]\nprint(\"Ranking de caracteristicas:\")\nfor i in range(X.shape[1]):\nprint(f\"{i+1}. Feature {indices[i]}: {importances[indices[i]]:.4f}\")\n</code></pre> <p>Esta forma de obtener la importancia es muy r\u00e1pida de calcular, ya que se obtiene a partir de las medidas de impureza obtenidas a partir del entrenamiento. </p> <p>Sin embargo, tiene un sesgo hacia caracter\u00edsticas de alta cardinalidad, es decir, caracter\u00edsticas con muchos valores \u00fanicos, como por ejemplo un ID que es \u00fanico para cada ejemplo de entrada, ya que dividen artificialmente el espacio, pero no generalizan. </p> <p>Tambi\u00e9n es problem\u00e1tico con caracter\u00edsticas correlacionadas, ya que la importancia se diluye entre ellas, y no se sabe cu\u00e1l es realmente importante.</p>"},{"location":"04-random-forest/#permutation-importance","title":"Permutation Importance","text":"<p>Esta segunda forma mide la degradaci\u00f3n del rendimiento cuando se permutan los valores de una feature. </p> <p>Para ello, primer evaluaremos en modelo con un dataset \\(\\mathcal{D}\\), obteniendo el error baseline. Una vez hecho esto, en el conjunto \\(\\mathcal{D}\\) permutamos los valores de la feature (columna) \\(j\\), corrompiendo as\u00ed los datos para dicha feature, y calculamos el error del modelo con la feature corrupta. Repetiremos la permutaci\u00f3n varias veces y promediamos el error obtenido al corromper \\(j\\). Con esto, podemos calcular la importancia de permutaci\u00f3n de \\(j\\) calculando la diferencia entre el error baseline y el error promedio obtenido al corromper \\(j\\).</p> <p>A continuaci\u00f3n podemos ver la implementaci\u00f3n en sklearn:</p> <pre><code>from sklearn.inspection import permutation_importance\n# Calcular permutation importance\nresult = permutation_importance(rf, X_test, y_test, n_repeats=10)\nimportances = result.importances_mean\n# Mostramos ranking de caracteristicas\nfor i in np.argsort(importances)[::-1]:\nprint(f\"Feature {i}: {importances[i]:.4f} +/- {result.importances_std[i]:.4f}\")\n</code></pre> <p>Este tipo de c\u00e1lculo de la importancia es m\u00e1s fiable cuando contamos con caracter\u00edsticas correlacionadas, y no existe sesgo por la cardinalidad, pero tiene un mayor coste computacional y requiere un conjunto de test o validaci\u00f3n.</p>"},{"location":"04-random-forest/#out-of-bag-oob-error-en-random-forest","title":"Out-of-Bag (OOB) Error en Random Forest","text":"<p>Como Random Forest usa bootstrap, hereda la estimaci\u00f3n del error OOB. A continuaci\u00f3n podemos ver c\u00f3mo obtener este valor con sklearn:</p> <pre><code>rf = RandomForestClassifier(\nn_estimators=100,\noob_score=True  # Activar OOB\n)\nrf.fit(X_train, y_train)\nprint(f\"OOB Score: {rf.oob_score_:.4f}\")\nprint(f\"Test Score: {rf.score(X_test, y_test):.4f}\")\n</code></pre> <p>El OOB score es t\u00edpicamente una buena aproximaci\u00f3n del error de generalizaci\u00f3n.</p>"},{"location":"04-random-forest/#extra-trees-extremely-randomized-trees","title":"Extra Trees (Extremely Randomized Trees)","text":"<p>Los Extra Trees son una variante de Random Forest en la que se introduce a\u00fan m\u00e1s aleatoriedad. Si bien en Random Forest en cada nodo de los \u00e1rboles se elige el mejor split entre las features seleccionadas, en Extra Trees se elige un split de forma aleatoria para cada feature, y nos quedamos con aquella que proporciona una mayor ganancia.   </p> <p>En la siguiente tabla se resumen las principales diferencias estre estos modelos:</p> Aspecto Random Forest Extra Trees Sampling Bootstrap (por defecto con reemplazo) Todo el dataset (por defecto sin bootstrap) Splits Mejor split entre \\(m\\) features Split aleatorio entre \\(m\\) features Varianza Bajo A\u00fan menor Sesgo Bajo Ligeramente mayor Velocidad M\u00e1s lento M\u00e1s r\u00e1pido (splits aleatorios) <p>En sklearn tenemos las clase ExtraTreesClassifier y ExtraTreesRegressor. A continuaci\u00f3n vemos un ejemplo de implementaci\u00f3n:</p> <pre><code>from sklearn.ensemble import ExtraTreesClassifier\n# Extra Trees\net = ExtraTreesClassifier(\nn_estimators=100,\nmax_features='sqrt',\nbootstrap=False  # No usa bootstrap (default)\n)\net.fit(X_train, y_train)\n</code></pre> <p>Ser\u00e1 conveniente utilizar Extra Trees cuando busquemos reducir a\u00fan m\u00e1s la varianza, aunque sea con un ligero aumento del sesgo, y cuando necesitemos una mayor velocidad de entrenamiento. </p>"},{"location":"04-random-forest/#consideraciones-finales","title":"Consideraciones finales","text":"<p>Random Forest es uno de los mejores algoritmos out-of-the-box. Es decir, es capaz de funcionar correctamente sin necesidad de configuraci\u00f3n o ajustes adicionales, funcionando bien con los hiperpar\u00e1metros por defecto.</p> <p>Es un m\u00e9todo robusto, que puede manejar bien datos con ruido, outliers y features irrelevantes. Puede manejar adem\u00e1s valores faltantes (missing values). Mediante la evaluaci\u00f3n OOB podemos validar el modelo sin requerir un conjunto de validaci\u00f3n por separado. Una limitaci\u00f3n que encontramos es que el modelo no extrapola, es decir, no puede predecir fuera del rango de entrenamiento.</p> <p>Es tambi\u00e9n vers\u00e1til, dando buenos resultados tanto en problemas de clasificaci\u00f3n como de regresi\u00f3n. Permite tambi\u00e9n tener features mixtas, num\u00e9ricas y categ\u00f3ricas, y permite capturar relaciones complejas (no lineales) de los datos de entrada. Tambi\u00e9n es invariante a la escala de las features, por lo que no necesita normalizaci\u00f3n. Sin embargo, con datasets peque\u00f1os puede haber algo de overfitting, y puede tener un peor rendimiento cuando tenemos features muy correlacionadas.</p> <p>En cuanto a la interpretabilidad, tenemos la posibilidad de obtener una medida de la importancia de caracter\u00edsticas, pudiendo destacar las m\u00e1s relevantes. Sin embargo, es menos interpretable que un \u00e1rbol individual.</p> <p>Respecto al coste, el entrenamiento es muy r\u00e1pido y es paralelizable, pero cuando tenemos muchos \u00e1rboles la predicci\u00f3n puede ser m\u00e1s lenta. Adem\u00e1s, tiene un alto coste espacial en memoria, ya que debe almacenar muchos \u00e1rboles profundos. El tama\u00f1o de los modelos puede ser grande. </p> <p>En resumen, Random Forest es robusto y vers\u00e1til, siendo el m\u00e9todo de Bagging m\u00e1s popular y uno de los algoritmos m\u00e1s importantes en Machine Learning. Combina: - Bootstrap sampling (como Bagging) - Random feature selection - \u00c1rboles profundos sin poda - Promediado/votaci\u00f3n</p> <p>Normalmente Random Forest funcionar\u00e1 mejor que Bagging porque existe menos correlaci\u00f3n entre los \u00e1rboles y esta mayor diversidad produce una reducci\u00f3n de la varianza. Con Random Forest podemos obtener de forma r\u00e1pida un baseline robusto para tratar datos tabulares. </p> <p>Sin embargo, en algunos casos deber\u00edamos considerar otros modelos. Si necesitamos mayor rendimiento podr\u00edamos considerar m\u00e9todos como XGBoost o LightGBM. Si buscamos una alta interpretabilidad ser\u00e1 m\u00e1s adecuado utilizar \u00e1rboles individuales o modelos lineales. Si tenemos datos de tipo imagen o texto, ser\u00e1 m\u00e1s adecuado utilizar redes neuronales profundas. En caso de tener datasets peque\u00f1os, con solo decenas o unos pocos cientos de ejemplos, deber\u00edamos considerar modelos m\u00e1s simples. </p> <p>Hasta ahora hemos visto m\u00e9todos que combinan modelos entrenados independientemente (en paralelo), ya sea en los mismos datos (Voting, Stacking) o en diferentes muestras (Bagging). La siguiente pregunta es: \u00bfy si los modelos se entrenan secuencialmente, aprendiendo cada uno de los errores del anterior?. En esto se basar\u00e1n los m\u00e9todos de Boosting.</p> <ol> <li> <p>Sagi, O., &amp; Rokach, L. (2018). Ensemble learning: A survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8(4), e1249.\u00a0\u21a9</p> </li> <li> <p>Kittler, J., Hatef, M., Duin, R. P., &amp; Matas, J. (1998). On combining classifiers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(3), 226--239.\u00a0\u21a9</p> </li> <li> <p>Wolpert, D. H. (1992). Stacked generalization. Neural Networks, 5(2), 241--259.\u00a0\u21a9</p> </li> <li> <p>Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2), 123--140.\u00a0\u21a9</p> </li> <li> <p>Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5--32.\u00a0\u21a9</p> </li> </ol>"},{"location":"05-adaboost/","title":"5. Boosting. Adaboost","text":""},{"location":"05-adaboost/#boosting","title":"Boosting","text":"<p>Boosting es un meta-algoritmo de aprendizaje autom\u00e1tico, que busca reducir principalmente el sesgo, aunque tambi\u00e9n algo la varianza. </p> <p>A diferencia de los m\u00e9todos de ensemble estudiados hasta el momento (Voting, Stacking, Bagging), en los que cont\u00e1bamos con modelos independientes que se entrenaban en paralelo, Boosting entrena los modelos secuencialmente, de forma que cada nuevo modelo se centra en corregir los errores cometidos por el ensemble actual.</p>"},{"location":"05-adaboost/#combinacion-de-clasificadores","title":"Combinaci\u00f3n de clasificadores","text":"<p>La idea que buscamos con Boosting es combinar una serie de clasificadores d\u00e9biles para construir un clasificador fuerte. Los definimos de la siguiente forma:</p> <ul> <li> <p>Clasificador d\u00e9bil (weak learner): Se trata de clasificadores simples que funcionan mejor que la clasificaci\u00f3n aleatoria. Es decir, su accuracy debe ser mayor que 50% (mejor que lanzar una moneda). Formalmente, para clasificaci\u00f3n binaria, un clasificador d\u00e9bil debe cumplir que su tasa de error sea \\(\\epsilon &lt; \\frac{1}{2}\\).</p> </li> <li> <p>Clasificador fuerte (strong learner): Se trata de un clasificador con alta precisi\u00f3n, capaz de aproximarse arbitrariamente bien a la funci\u00f3n objetivo. Su tasa de error puede ser tan peque\u00f1a como se desee con suficientes datos y suficiente capacidad del modelo.</p> </li> </ul>"},{"location":"05-adaboost/#clasificadores-debiles","title":"Clasificadores d\u00e9biles","text":"<p>Algunos de los ejemplos de clasificadores d\u00e9biles t\u00edpicos son los siguientes:</p> <ul> <li> <p>Decision stumps: \u00c1rboles de decisi\u00f3n con una \u00fanica divisi\u00f3n (profundidad \\(1\\)). Equivale a una regla simple, como por ejemplo, \"si \\(x_1 &gt; 5\\) entonces clase positiva, si no, clase negativa\".</p> </li> <li> <p>\u00c1rboles poco profundos: \u00c1rboles con profundidad m\u00e1xima \\(2\\) o \\(3\\).</p> </li> <li> <p>Clasificadores lineales en problemas no lineales.</p> </li> </ul>"},{"location":"05-adaboost/#clasificadores-fuertes","title":"Clasificadores fuertes","text":"<p>A continuaci\u00f3n mostramos algunos ejemplos de clasificadores fuertes t\u00edpicos:</p> <ul> <li>Redes neuronales profundas</li> <li>\u00c1rboles de decisi\u00f3n muy profundos</li> <li>SVMs con kernels complejos</li> <li>Random Forests</li> <li>Gradient Boosting</li> </ul>"},{"location":"05-adaboost/#base-fundacional","title":"Base fundacional","text":"<p>En 1988 y 1989 Kearns y Valiant (Kearns &amp; Valiant, 1988, 1989)1 2 plantearon la pregunta que constituye la fundaci\u00f3n te\u00f3rica del Boosting: \"\u00bfPuede un conjunto de clasificadores d\u00e9biles crear un \u00fanico clasificador fuerte?\". </p> <p>En 1990 Schapire (Schapire, 1990)3 demuestra que esto es posible, lo cual supone uno de los resultados m\u00e1s importantes en teor\u00eda de aprendizaje autom\u00e1tico. Lo que nos dice el teorema que plantea es que si existe un clasificador d\u00e9bil que puede lograr error menor que \\(1/2 - \\gamma\\) (donde \\(\\gamma &gt; 0\\)), entonces existe un algoritmo de Boosting que puede combinarlo para lograr un error arbitrariamente peque\u00f1o en el conjunto de entrenamiento.</p> <p>Este teorema es importante porque:</p> <ol> <li>Demuestra que la \"debilidad\" es suficiente para el aprendizaje.</li> <li>Proporciona una garant\u00eda matem\u00e1tica sobre los m\u00e9todos de Boosting.</li> <li>Nos muestra c\u00f3mo construir el clasificador fuerte.</li> </ol> <p>En este punto, nos podemos plantear la pregunta \"\u00bfPor qu\u00e9 usar clasificadores d\u00e9biles?\". Aunque pueda parecer contraintuitivo usar modelos \"d\u00e9biles\", hay varios motivos para hacerlo:</p> <ol> <li>Prevenci\u00f3n del overfitting: </li> <li>Los modelos d\u00e9biles tienen baja varianza</li> <li>Son menos propensos a aprender el ruido</li> <li> <p>La combinaci\u00f3n de modelos reduce el overfitting</p> </li> <li> <p>Eficiencia computacional:</p> </li> <li>Modelos sencillos como los decision stumps son extremadamente r\u00e1pidos.</li> <li> <p>Podemos entrenar cientos o miles de ellos de forma eficiente.</p> </li> <li> <p>Interpretabilidad:</p> </li> <li>Cada modelo d\u00e9bil es f\u00e1cil de entender.</li> <li> <p>La combinaci\u00f3n de modelos mantiene cierta trazabilidad.</p> </li> <li> <p>Teor\u00eda s\u00f3lida:</p> </li> <li>Existen garant\u00edas matem\u00e1ticas de convergencia.</li> <li>El error de generalizaci\u00f3n puede ser acotado.</li> </ol> <p>Para que un clasificador d\u00e9bil sea \u00fatil en un ensemble debe cumplir:</p> <ol> <li>Mejor que el azar: Su error debe ser \\(\\epsilon &lt; 0.5\\).</li> <li>Diversidad: Debe cometer errores diferentes a los que cometen otros clasificadores.</li> <li>Eficiencia: Debe ser r\u00e1pido de entrenar.</li> <li>Estabilidad: No debe ser extremadamente sensible a peque\u00f1os cambios en los datos.</li> </ol> <p>Respecto a la relaci\u00f3n con el problema sesgo-varianza, los clasificadores d\u00e9biles tendr\u00e1n un alto sesgo, pero baja varianza (deben ser estables ante cambios en los datos). Por el contrario, los clasificadores fuertes tendr\u00e1n bajo sesgo, ya que pueden aprender patrones complejos, pero una alta varianza, ya que ser\u00e1n m\u00e1s sensibles a los datos de entrenamiento.</p> <p>Un ensemble de clasificadores d\u00e9biles reducir\u00e1 el sesgo, al combinar de forma secuencial los clasificadores, manteniendo la baja varianza de los modelos individuales, obteniendo as\u00ed lo mejor de cada tipo de clasificador. </p>"},{"location":"05-adaboost/#muestreo-y-votos-ponderados","title":"Muestreo y votos ponderados","text":"<p>Como hemos comentado, una de las principales diferencias de Boosting con los m\u00e9todos de ensemble vistos anteriormente es que en lugar de entrenar los modelos en paralelo, Boosting realiza el entrenamiento secuencialmente, buscando que los nuevos clasificadores d\u00e9biles corrijan los principales errores del ensemble actual. Para ello introduce ponderaci\u00f3n a dos niveles: en el muestreo de ejemplos de entrenamiento y en la importancia de cada clasificador.</p> <p>Si recordamos las caracter\u00edsticas de los m\u00e9todos de Bagging, tenemos:</p> <ul> <li>Bagging realiza un muestreo aleatorio de los ejemplos de entrada para entrenar cada clasificador, pero todos estos ejemplos de entrada reciben el mismo peso.</li> <li>Bagging da la misma importancia a todos los clasificadores. El voto de cada clasificador vale lo mismo.</li> </ul> <p>A diferencia de esto, Boosting introduce:</p> <ul> <li> <p>Muestreo ponderado: No se da el mismo peso a todos los ejemplos de entrenamiento. El entrenamiento se concentrar\u00e1 en los ejemplos m\u00e1s dif\u00edciles. Intuitivamente, podr\u00edamos considerar que aquellos ejemplos cercanos a la frontera de decisi\u00f3n son m\u00e1s dif\u00edciles de clasificar, y deber\u00edan por lo tanto recibir pesos m\u00e1s altos. Podemos establecer una relaci\u00f3n entre esto y los vectores de soporte en SVM, ya que en ambos casos buscamos basarnos en los ejemplos m\u00e1s dif\u00edciles de clasificar para obtener el clasificador. </p> </li> <li> <p>Votos ponderados: Se da diferente peso a los diferentes clasificadores, que al combinarlos se obtiene un \"voto ponderado\". Esto, junto a la estrategia de muestreo anterior, ayudar\u00e1 a producir un clasificador m\u00e1s fuerte.</p> </li> </ul> <p>Considerando que contamos con un dataset \\(\\mathcal{D}\\) con \\(N\\) ejemplos de entrenamiento \\((\\mathbf{x}_i, y_i)\\), con \\(i = 1, 2, \\ldots, N\\), y \\(T\\) clasificadores d\u00e9biles \\(h_1, h_2, \\ldots, h_T\\), definimos:</p> <ul> <li> <p>\\(w_i^{(t)}\\): Peso del ejemplo de entrenamiento \\((\\mathbf{x}_i, y_i)\\) para entrenar el clasificador \\(h_t\\).</p> </li> <li> <p>\\(\\alpha_t\\): Peso del clasificador \\(h_t\\) en la votaci\u00f3n del ensemble.</p> </li> </ul> <p>De esta forma, para el caso de clasificaci\u00f3n binaria con \\(y_i \\in \\{-1, 1\\}\\), podemos definir el ensemble como:</p> \\[ F(\\mathbf{x}) = \\sum_{t=1}^T \\alpha_t h_t(\\mathbf{x}) \\] <p>Es decir, la predicci\u00f3n de cada clasificador \\(h_t\\) estar\u00e1 ponderada por el peso \\(\\alpha_t\\), pudiendo as\u00ed dar mayor peso a los clasificadores con menor error. La predicci\u00f3n final vendr\u00eda dada por el signo de la funci\u00f3n anterior:</p> \\[ H(\\mathbf{x}) = \\text{signo} \\left( F(\\mathbf{x}) \\right) \\]"},{"location":"05-adaboost/#adaboost-adaptive-boosting","title":"AdaBoost (Adaptive Boosting)","text":"<p>AdaBoost (Freund &amp; Schapire, 1997)4 fue desarrollado por Freund y Schapire en 1997, y constituye el primer algoritmo de Boosting exitoso.</p> <p>La idea central de este algoritmo es:</p> <ol> <li>Entrenar un clasificador d\u00e9bil.</li> <li>Aumentar el peso de los ejemplos mal clasificados.</li> <li>Entrenar el siguiente clasificador con los datos ponderados.</li> <li>Repetir el proceso.</li> <li>Combinar todos los clasificadores ajustando sus pesos seg\u00fan su accuracy.</li> </ol> <p>Considerando el caso de clasificaci\u00f3n binaria, donde \\(y_i \\in \\{ -1, 1\\}\\), el algoritmo AdaBoost buscar\u00e1 minimizar la p\u00e9rdida exponencial, que se define como:</p> \\[ L(y, F) = \\sum_{i=1}^N e^{-y_i F(\\mathbf{x}_i)}  \\] <p>Podemos observar que esta funci\u00f3n de p\u00e9rdida penaliza aquellos ejemplos en los que \\(y_i\\) y \\(F(\\mathbf{x}_i)\\) tengan distinto signo, es decir, aquellos que est\u00e1n mal clasificados. Adem\u00e1s, la p\u00e9rdida no crece linealmente con el error, sino de forma exponencial. Un ejemplo muy mal clasificado contribuir\u00e1 a la p\u00e9rdida de forma desproporcionada.</p> <p>A nivel general, el algoritmo AdaBoost realizar\u00e1 los siguientes pasos:</p> \\[ \\begin{align*} &amp; \\text{Entrada: } \\text{Conjunto de entrenamiento } \\mathcal{D}= \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N \\text{ con } y_i \\in \\{-1, +1\\} \\\\ &amp; w_i^{(1)} \\leftarrow \\frac{1}{N} \\quad \\forall i \\in 1, 2, \\ldots, N \\quad \\text{(Inicializa todos los ejemplos con peso uniforme)} \\\\ &amp; \\text{Para  } t = 1, \\ldots, T \\\\ &amp; \\quad h_t \\leftarrow \\text{Entrenar un clasificador d\u00e9bil con los pesos } \\mathbf{w}^{(t)} \\\\ &amp; \\quad \\alpha_t \\leftarrow \\text{Calcular el peso del clasificador } h_t \\\\ &amp; \\quad w_i^{(t+1)} \\leftarrow \\text{Actualiza los pesos de los ejemplos para el siguiente clasificador } \\forall i \\\\ &amp; \\text{Devuelve: } H(\\mathbf{x}) = \\text{signo} \\left( F(\\mathbf{x}) \\right) = \\text{signo} \\left( \\sum_{t=1}^T \\alpha_t h_t(\\mathbf{x}) \\right) \\end{align*} \\] <p>El algoritmo construye el ensemble de forma voraz, iteraci\u00f3n a iteraci\u00f3n. En cada iteraci\u00f3n \\(t\\) a\u00f1ade al ensemble un nuevo clasificador \\(h_t\\). Para entrenarlo presta especial atenci\u00f3n a los ejemplos de entrenamiento que tengan un mayor peso \\(w_i^{(t)}\\) en dicha iteraci\u00f3n.</p> <p>Vamos a continuaci\u00f3n a detallar cada uno de los pasos de este algoritmo.</p>"},{"location":"05-adaboost/#actualizacion-de-los-pesos","title":"Actualizaci\u00f3n de los pesos","text":"<p>Como hemos comentado, al final de cada iteraci\u00f3n deber\u00e1n ajustarse los pesos de cada ejemplo de entrenamiento, de forma que se le d\u00e9 mayor peso a los ejemplos m\u00e1s dif\u00edciles (los peor clasificados hasta el momento), para que el siguiente clasificador se centre en mejorar su clasificaci\u00f3n. </p> <p>Siguiendo el proceso del algoritmo anterior, podemos considerar que hasta la iteraci\u00f3n \\(t-1\\) tendremos un clasificador fuerte \\(F_{t-1}\\), que se obtiene como:</p> \\[ F_{t-1}(\\mathbf{x}) = \\alpha_1 h_1(\\mathbf{x}) + \\alpha_2 h_2(\\mathbf{x}) + \\ldots + \\alpha_{t-1} h_{t-1}(\\mathbf{x}) \\] <p>En la iteraci\u00f3n \\(t\\) buscaremos mejorarlo a\u00f1adiendo un nuevo clasificador d\u00e9bil \\(h_t\\):</p> \\[ F_{t}(\\mathbf{x}) = F_{t-1}(\\mathbf{x}) + \\alpha_t h_t(\\mathbf{x})  \\] <p>Buscamos que el nuevo clasificador d\u00e9bil minimice la p\u00e9rdida \\(L_t(y, F_t)\\):</p> \\[ \\begin{align*} L_t(y, F_t) &amp;= \\sum_{i=1}^N e^{-y_i F_t(\\mathbf{x}_i)} = \\\\ &amp;=  \\sum_{i=1}^N e^{-y_i (F_{t-1}(\\mathbf{x}_i) + \\alpha_t h_t(\\mathbf{x}_i))} = \\\\ &amp;= \\sum_{i=1}^N e^{-y_i F_{t-1}(\\mathbf{x}_i)} e^{ - \\alpha_t y_i h_t(\\mathbf{x}_i)} \\end{align*} \\] <p>El primer factor \\(e^{-y_i F_{t-1}(\\mathbf{x}_i)}\\) no depende de lo que se est\u00e1 optimizando en este paso (\\(\\alpha_t\\) y \\(h_t\\)), sino que act\u00faa como una constante multiplicativa para cada ejemplo de entrada \\(i\\). De esta forma, consideramos esa constante como un peso \\(w_i^{(t)}\\) que se define como:</p> \\[ w_i^{(t)} = e^{-y_i F_{t-1}(\\mathbf{x}_i)} \\] <p>Este peso ser\u00e1 mayor cuanto peor est\u00e9 clasificado el ejemplo \\(i\\) en el ensemble \\(t-1\\).</p> <p>Reemplaz\u00e1ndolo en la funci\u00f3n de p\u00e9rdida anterior tendr\u00edamos:</p> \\[ \\begin{align*} L_t(y, F_t) &amp;=  \\sum_{i=1}^N w_i^{(t)} e^{ - \\alpha_t y_i h_t(\\mathbf{x}_i)} \\end{align*} \\] <p>Es decir, en la funci\u00f3n de p\u00e9rdida se le dar\u00e1 mayor peso a los ejemplos peor clasificados por el ensemble anterior \\(t-1\\).</p> <p>Vamos a ver ahora la forma de actualizar los pesos. El peso para la siguiente iteraci\u00f3n \\(t+1\\) ser\u00eda:</p> \\[ w_i^{(t+1)} = e^{-y_i F_{t}(\\mathbf{x}_i)} =  e^{-y_i F_{t-1}(\\mathbf{x}_i)} e^{ -  \\alpha_t y_i h_t(\\mathbf{x}_i)} = w_i^{(t)} e^{ - \\alpha_t y_i h_t(\\mathbf{x}_i)} \\] <p>Por lo tanto, tenemos la forma en la que se actualizar\u00e1n los pesos tras cada iteraci\u00f3n:</p> \\[ \\quad w_i^{(t+1)} = w_i^{(t)} e^{-\\alpha_t y_i h_t(\\mathbf{x}_i)}  \\quad \\forall i  \\] <p>A partir de esta forma de actualizar los pesos, podemos observar que los ejemplos mal clasificados tendr\u00e1n signo positivo en la exponencial, por lo que aumentar\u00e1 su peso, mientras que los bien clasificados tendr\u00e1n signo negativo y en ese caso disminuir\u00e1 su peso.</p> <p>Adem\u00e1s, deberemos normalizar los pesos para posteriormente poder calcular correctamente el error del clasificador a partir de ellos:</p> \\[ w_i^{(t+1)} = \\frac{w_i^{(t+1)}}{\\sum_{j=1}^N w_j^{(t+1)}}  \\quad \\forall i  \\] <p>Relaci\u00f3n con la derivada: Si tratamos \\(F(\\mathbf{x}_i)\\) como una variable independiente para cada ejemplo de entrada \\(\\mathbf{x}_i\\), y evaluamos cu\u00e1nto afecta a la p\u00e9rdida total calculando la derivada parcial, tenemos:</p> \\[ \\frac{\\partial L_t(y, F_{t})}{\\partial F_{t}(\\mathbf{x}_i)} = -y_i e^{-y_i F_{t}} \\] <p>Si atendemos al gradiente negativo, podemos identificar dos partes:</p> <ul> <li>\\(y_i\\) nos indica la direcci\u00f3n, es decir, hacia d\u00f3nde hay que mover \\(F(\\mathbf{x}_i)\\) para reducir la p\u00e9rdida.</li> <li>\\(e^{-y_i F_{t}}\\) nos indicar\u00e1 la magnitud, lo cual nos da una medida de cu\u00e1nto importa el ejemplo \\(\\mathbf{x}_i\\) en este momento. Por este motivo, definiremos este segundo t\u00e9rmino como el peso que le daremos a cada ejemplo de entrada. </li> </ul>"},{"location":"05-adaboost/#entrenar-un-clasificador-debil","title":"Entrenar un clasificador d\u00e9bil","text":"<p>Buscamos minimizar la funci\u00f3n de p\u00e9rdida exponencial \\(L(y, F_t)\\), que a partir de la definici\u00f3n de los pesos expuesta en el apartado anterior, podemos escribir como:</p> \\[ L(y, F_t)  = \\sum_{i=1}^N w_i^{(t)} e^{- \\alpha_t y_i h_t(\\mathbf{x}_i)} \\] <p>Si en la funci\u00f3n separamos los ejemplos que se clasifican correcta e incorrectamente, tendr\u00edamos:</p> \\[ L(y, F_t) = \\sum_{y_i = h_t(\\mathbf{x}_i)} w_i^{(t)} e^{-\\alpha_t } + \\sum_{y_i \\neq h_t(\\mathbf{x}_i)} w_i^{(t)} e^{\\alpha_t } \\] <p>De forma equivalente, podr\u00eda expresarse como:</p> \\[ L(y, F_t) = \\sum_{i=1}^N w_i^{(t)} e^{-\\alpha_t } + \\sum_{y_i \\neq h_t(\\mathbf{x}_i)} w_i^{(t)} (e^{\\alpha_t } - e^{-\\alpha_t }) \\] <p>En la ecuaci\u00f3n anterior, vemos que s\u00f3lo el segundo t\u00e9rmino depende de \\(h_t\\). Por lo tanto, el clasificador que minimizar\u00e1 el error ser\u00e1 aquel que minimice \\(\\sum_{y_i \\neq h_t(\\mathbf{x}_i)} w_i^{(t)}\\), es decir, la suma de los pesos de los ejemplos de entrada mal clasificados. Tendremos que entrenar los modelos base dando a cada ejemplo de entrada \\((\\mathbf{x}_i, y_i)\\) su correspondiente peso \\(w_i^{(t)}\\).</p> <p>A nivel pr\u00e1ctico, en sklearn, esto lo podremos hacer mediante el par\u00e1metro <code>sample_weight</code> de la funci\u00f3n <code>fit</code>. Cada tipo de modelo tratar\u00e1 estos pesos de forma distinta. Por ejemplo, en el caso de \u00c1rboles de Decisi\u00f3n se tendr\u00e1n en cuenta los pesos en el c\u00e1lculo de la impureza de cada nodo, mientras que otros modelos como Regresi\u00f3n Log\u00edstica o SVM incorporar\u00e1n los pesos en su propia funci\u00f3n de p\u00e9rdida, haciendo que cada ejemplo contribuya a la p\u00e9rdida con un factor proporcional a \\(w_i^{(t)}\\). Esto tiene la implicaci\u00f3n de que solo podremos usar como modelos base en AdaBoost aquellos estimadores que soporten ese par\u00e1metro. </p>"},{"location":"05-adaboost/#calcular-el-peso-del-clasificador","title":"Calcular el peso del clasificador","text":"<p>Deberemos buscar el valor del peso de cada clasificador que minimice el error del ensemble. Para hacer esto, en primer lugar derivaremos la funci\u00f3n de p\u00e9rdida respecto al peso \\(\\alpha_t\\) de cada clasificador:</p> \\[ \\begin{align*} \\frac{\\partial L(y, F_t)}{\\partial \\alpha_t} &amp;= \\frac{\\partial \\left( \\sum_{y_i = h_t(\\mathbf{x}_i)} w_i^{(t)} e^{-\\alpha_t } + \\sum_{y_i \\neq h_t(\\mathbf{x}_i)} w_i^{(t)} e^{\\alpha_t } \\right)}{\\partial \\alpha_t} =  \\\\ &amp;= -\\sum_{y_i = h_t(\\mathbf{x}_i)} w_i^{(t)} e^{-\\alpha_t } + \\sum_{y_i \\neq h_t(\\mathbf{x}_i)} w_i^{(t)} e^{\\alpha_t } \\end{align*} \\] <p>Teniendo en cuenta que la funci\u00f3n de error es una funci\u00f3n convexa, la igualaremos a \\(0\\) para buscar el punto en la que es m\u00ednima:</p> \\[ -\\sum_{y_i = h_t(\\mathbf{x}_i)} w_i^{(t)} e^{-\\alpha_t } + \\sum_{y_i \\neq h_t(\\mathbf{x}_i)} w_i^{(t)} e^{\\alpha_t } = 0 \\] \\[ \\Downarrow \\] \\[ \\sum_{y_i = h_t(\\mathbf{x}_i)} w_i^{(t)} e^{-\\alpha_t } = \\sum_{y_i \\neq h_t(\\mathbf{x}_i)} w_i^{(t)} e^{\\alpha_t } \\] \\[ \\Downarrow \\] \\[ \\frac{\\sum_{y_i = h_t(\\mathbf{x}_i)} w_i^{(t)}}{\\sum_{y_i \\neq h_t(\\mathbf{x}_i)} w_i^{(t)}} = \\frac{e^{\\alpha_t }}{e^{-\\alpha_t }} = e^{2\\alpha_t} \\] <p>Por lo tanto, tenemos:</p> \\[ \\alpha_t = \\frac{1}{2} \\ln \\frac{\\sum_{y_i = h_t(\\mathbf{x}_i)} w_i^{(t)}}{\\sum_{y_i \\neq h_t(\\mathbf{x}_i)} w_i^{(t)}} \\] <p>Considerando que los pesos est\u00e1n normalizados, podemos definir el error \\(\\epsilon_t\\) del clasificador d\u00e9bil \\(h_t\\), de la siguiente forma:</p> \\[ \\epsilon_t = \\sum_{y_i \\neq h_t(\\mathbf{x}_i)} w_i^{(t)} \\] <p>Observamos que calculamos el error de \\(h_t\\) como la suma de los pesos de los ejemplos mal clasificados por dicho clasificador d\u00e9bil. De esta forma, el clasificador tendr\u00e1 error bajo cuando el peso total de los ejemplos mal clasificados sea bajo, y los ejemplos de mayor peso hayan sido correctamente clasificados. </p> <p>Teniendo esta definici\u00f3n en cuenta, y considerando que los pesos est\u00e1n normalizados y que por lo tanto \\(\\sum_{i=1}^N w_i^{(t)} = 1\\), tenemos:</p> <p>$$ \\begin{align*} \\alpha_t &amp;= \\frac{1}{2} \\ln \\frac{\\sum_{y_i = h_t(\\mathbf{x}i)} w_i^{(t)}}{\\sumi)} w_i^{(t)}} = \\ &amp;= \\frac{1}{2} \\ln \\frac{\\sum^N w_i^{(t)} - \\sum_{y_i \\neq h_t(\\mathbf{x}i)} w_i^{(t)}}{\\sum_i)} w_i^{(t)}} = \\</p> <p>&amp;= \\frac{1}{2} \\ln \\frac{1-\\epsilon_t}{\\epsilon_t} \\end{align*} $$</p> <p>Con esto podemos observar que \\(\\alpha_t\\) ser\u00e1 mayor cuanto menor error \\(\\epsilon_t\\) tenga el clasificador. Lo peor que pueda ocurrir es que \\(\\epsilon_t = 0.5\\), ya que en ese caso el clasificador equivale a lanzar una moneda al aire, y en tal caso tendremos un peso \\(\\alpha_t = 0\\). Podemos observar tambi\u00e9n que si \\(\\epsilon_t &gt; 0.5\\) (peor que el azar), el peso pasar\u00e1 a ser negativo, es decir, se invierte el clasificador para que as\u00ed pase a ser algo mejor que el azar.  </p>"},{"location":"05-adaboost/#algoritmo-detallado","title":"Algoritmo detallado","text":"<p>Con todo lo anterior, podemos escribir de forma completa el algoritmo AdaBoost detallando en cada paso la forma en la que se calculan los errores y los pesos:</p> \\[ \\begin{align*} &amp; \\text{Entrada: } \\text{Conjunto de entrenamiento } \\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N \\text{ con } y_i \\in \\{-1, +1\\} \\\\ &amp; w_i^{(1)} \\leftarrow \\frac{1}{N} \\quad \\forall i \\in 1, 2, \\ldots, N \\quad \\text{(Inicializa todos los ejemplos con peso uniforme)} \\\\ &amp; \\text{Para  } t = 1, \\ldots, T \\\\ &amp; \\quad h_t \\leftarrow \\text{Entrenar un clasificador d\u00e9bil que minimice } \\sum_{y_i \\neq h_t(\\mathbf{x}_i)} w_i^{(t)} \\\\ &amp; \\quad \\epsilon_t \\leftarrow \\sum_{y_i \\neq h_t(\\mathbf{x}_i)} w_i^{(t)} \\quad \\text{(Calcular el error del clasificador d\u00e9bil)} \\\\ &amp; \\quad \\alpha_t \\leftarrow \\frac{1}{2} \\ln \\left( \\frac{1-\\epsilon_t}{\\epsilon_t} \\right) \\quad \\text{(Calcular el peso del clasificador)}  \\\\ &amp; \\quad w_i^{(t+1)} \\leftarrow w_i^{(t)} e^{-\\alpha_t y_i h_t(\\mathbf{x}_i)}  \\quad \\forall i \\quad \\text{(Actualiza los pesos de los ejemplos para el siguiente clasificador)} \\\\ &amp; \\quad w_i^{(t+1)} \\leftarrow \\frac{w_i^{(t+1)}}{\\sum_{j=1}^N w_j^{(t+1)}}  \\quad \\forall i \\quad \\text{(Normaliza los nuevos pesos)} \\\\ &amp; \\text{Devuelve: } H(\\mathbf{x}) = \\text{signo} \\left( F(\\mathbf{x}) \\right) = \\text{signo} \\left( \\sum_{t=1}^T \\alpha_t h_t(\\mathbf{x}) \\right) \\end{align*} \\]"},{"location":"05-adaboost/#propiedades-de-adaboost","title":"Propiedades de AdaBoost","text":""},{"location":"05-adaboost/#adaptatividad","title":"Adaptatividad","text":"<p>Una propiedad importante de AdaBoost es su adaptatividad, lo cual le da el nombre (Adaptative Boosting). El t\u00e9rmino \"adaptativo\" tiene adem\u00e1s dos significados:</p> <ul> <li> <p>Adaptaci\u00f3n a los datos de entrada: Los pesos \\(w_i\\) se ajustan en funci\u00f3n de los ejemplos que se clasifican mal. El algoritmo es capaz de identificar estos ejemplos a partir de los errores observados, y se concentra en ellos.</p> </li> <li> <p>Adaptaci\u00f3n a la calidad de los clasificadores base: El peso \\(\\alpha_t\\) que recibe cada clasificador no se fija de antemano, sino que el algoritmo lo calcula de forma autom\u00e1tica a partir del error \\(\\epsilon_t\\). A diferencia de propuestas previas de m\u00e9todos de Boosting, no es necesario conocer de antemano la calidad de los clasificadores base ni su error, sino que se adaptar\u00e1 autom\u00e1ticamente a partir del error observado.</p> </li> </ul>"},{"location":"05-adaboost/#cota-del-error-de-entrenamiento","title":"Cota del error de entrenamiento","text":"<p>AdaBoost tiene una garant\u00eda te\u00f3rica sobre el error de entrenamiento (Schapire &amp; Freund, 2012)5 tras \\(T\\) iteraciones:</p> \\[\\frac{1}{N} \\sum_{i=1}^N 1(H(\\mathbf{x}_i) \\neq y_i) \\leq \\prod_{t=1}^{T} 2 \\sqrt{\\epsilon_t(1-\\epsilon_t)} \\leq \\exp\\left(-2\\sum_{t=1}^{T}\\gamma_t^2\\right)\\] <p>Donde \\(\\gamma_t = \\frac{1}{2} - \\epsilon_t\\) es el margen o ventaja del clasificador d\u00e9bil \\(h_t\\) sobre el azar. Podemos ver que si cada clasificador supera al azar, aunque sea por poco (\\(\\gamma_t &gt; 0\\)), el error de entrenamiento decrece.</p> <p>Podemos simplificar la cota anterior, considerando que todos los clasificadores tienen al menos una ventaja m\u00ednima uniforme \\(\\gamma_t \\geq \\gamma \\gt 0\\), de la siguiente forma:</p> \\[\\text{Training Error} \\leq \\exp\\left(-2 T \\gamma^2\\right)\\] <p>Aqu\u00ed podemos ver m\u00e1s claramente la implicaci\u00f3n de esta cota, y es que el error de entrenamiento decrece exponencialmente con el n\u00famero de iteraciones \\(T\\). Basta que cada clasificador sea ligeramente mejor que el azar para que el error de entrenamiento se reduzca a cero.</p>"},{"location":"05-adaboost/#variantes-de-adaboost","title":"Variantes de AdaBoost","text":"<p>El algoritmo AdaBoost original (Freund &amp; Schapire, 1997)4 est\u00e1 dise\u00f1ado para clasificaci\u00f3n binaria, pero desde su publicaci\u00f3n se han propuesto numerosas variantes que ampl\u00edan su aplicabilidad a clasificaci\u00f3n multiclase, regresi\u00f3n, y escenarios con distintos requisitos de robustez. </p> <p>Encontramos diferentes variantes para clasificaci\u00f3n binaria, como Real AdaBoost (Schapire &amp; Singer, 1999)6, Gentle AdaBoost y LogitAdaBoost (Friedman et al., 2000)7. Tambi\u00e9n tenemos extensiones para clasificaci\u00f3n multiclase, como AdaBoost.M1 / AdaBoost.M2 (Freund &amp; Schapire, 1996)8 y SAMME / SAMME.R (Zhu et al., 2009)9. Por otro lado, tambi\u00e9n tenemos AdaBoost.R2 (Drucker, 1997)10, una adaptaci\u00f3n al caso continuo enfocada a problemas de regresi\u00f3n.</p> <p>Vamos a centrarnos en las m\u00e1s relevantes: SAMME para problemas de clasificaci\u00f3n multiclase y AdaBoost.R2 para regresi\u00f3n. </p>"},{"location":"05-adaboost/#clasificacion-multiclase-con-samme","title":"Clasificaci\u00f3n multiclase con SAMME","text":"<p>SAMME (Stagewise Additive Modeling using a Multi-class Exponential loss) (Zhu et al., 2009)9 es la generalizaci\u00f3n m\u00e1s com\u00fanmente utilizada de AdaBoost al caso multiclase, y cuenta con una s\u00f3lida base te\u00f3rica. </p> <p>El problema fundamental al pasar a clasificaci\u00f3n multiclase es que la condici\u00f3n de debilidad \\(\\epsilon_t &lt; 0.5\\) es demasiado estricta cuando el n\u00famero de clases aumenta. El azar uniforme entre \\(K\\) clases corresponde a un error de \\(1 - \\frac{1}{K}\\), que para \\(K \\geq 3\\) supera \\(0.5\\). Exigir \\(\\epsilon_t &lt; 0.5\\) equivale entonces a pedir que el clasificador base supere al azar binario en un problema que no lo es, lo que resulta cada vez m\u00e1s dif\u00edcil a medida que \\(K\\) crece.</p> <p>Si consideramos el caso del azar para cada \\(K\\) clases, tendr\u00edamos \\(\\epsilon_t = 1 - \\frac{1}{K}\\). En tal caso, si no introducimos ninguna correcci\u00f3n en el c\u00e1lculo de los pesos quedar\u00eda lo siguiente:</p> \\[ \\begin{align*} \\alpha_t &amp;= \\ln\\frac{1 - \\epsilon_t}{\\epsilon_t} = \\ln\\frac{1 - 1 + \\frac{1}{K}}{1-\\frac{1}{K}} = \\ln\\frac{\\frac{1}{K}}{1-\\frac{1}{K}} =\\\\ &amp; = \\ln\\frac{1}{K-1} = -\\ln (K-1) \\end{align*} \\] <p>Para un n\u00famero de clases \\(K &gt; 2\\) nos estar\u00e1 dando un peso negativo del clasificador, cuando realmente deber\u00eda darnos un peso \\(0\\), ya que esta situaci\u00f3n corresponde al azar. Por este motivo, la principal modificaci\u00f3n clave que introduce SAMME est\u00e1 en modificar la forma de calcular el peso de cada clasificador como se muestra a continuaci\u00f3n:</p> \\[\\alpha_t = \\ln\\frac{1 - \\epsilon_t}{\\epsilon_t} + \\ln(K - 1)\\] <p>El t\u00e9rmino adicional \\(\\ln(K-1)\\) tiene dos consecuencias importantes:</p> <ul> <li> <p>En primer lugar, la condici\u00f3n de debilidad pasa a ser \\(\\epsilon_t &lt; 1 - \\frac{1}{K}\\), que corresponde a superar al azar uniforme entre \\(K\\) clases, lo cual es la condici\u00f3n te\u00f3ricamente correcta. Cuando un clasificador funcione igual que el azar, su peso ser\u00e1 \\(0\\).</p> </li> <li> <p>En segundo lugar, para \\(K = 2\\) se tiene \\(\\ln(K-1) = \\ln 1 = 0\\) y el algoritmo producir\u00e1 las mismas predicciones que AdaBoost binario, confirmando que es una generalizaci\u00f3n consistente.</p> </li> </ul> <p>El algoritmo completo de entrenamiento es el siguiente:</p> \\[ \\begin{align*} &amp; \\text{Entrada: } \\text{Conjunto de entrenamiento } \\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N \\text{ con } y_i \\in \\{1, \\ldots, K\\} \\\\ &amp; w_i^{(1)} \\leftarrow \\frac{1}{N} \\quad \\forall i \\in 1, 2, \\ldots, N \\quad \\text{(Inicializa todos los ejemplos con peso uniforme)} \\\\ &amp; \\text{Para  } t = 1, \\ldots, T \\\\ &amp; \\quad h_t \\leftarrow \\text{Entrenar un clasificador d\u00e9bil que minimice } \\sum_{y_i \\neq h_t(\\mathbf{x}_i)} w_i^{(t)} \\\\ &amp; \\quad \\epsilon_t \\leftarrow \\sum_{y_i \\neq h_t(\\mathbf{x}_i)} w_i^{(t)} \\quad \\text{(Calcular el error del clasificador d\u00e9bil)} \\\\ &amp; \\quad \\text{Si } \\epsilon_t \\geq 1 - \\frac{1}{K} \\text{: detener} \\\\ &amp; \\quad \\alpha_t \\leftarrow  \\ln \\left( \\frac{1-\\epsilon_t}{\\epsilon_t} \\right) + \\ln(K-1) \\quad \\text{(Calcular el peso del clasificador)}  \\\\ &amp; \\quad w_i^{(t+1)} \\leftarrow w_i^{(t)} e^{\\alpha_t \\cdot \\mathbb{1}(h_t(\\mathbf{x}_i) \\neq y_i)}  \\quad \\forall i \\quad \\text{(Actualiza los pesos de los ejemplos para el siguiente clasificador)} \\\\ &amp; \\quad w_i^{(t+1)} \\leftarrow \\frac{w_i^{(t+1)}}{\\sum_{j=1}^N w_j^{(t+1)}}  \\quad \\forall i \\quad \\text{(Normaliza los nuevos pesos)} \\\\ &amp; \\text{Devuelve: } H(\\mathbf{x}) = \\arg \\max_k  \\sum_{t: h_t(\\mathbf{x})=k} \\alpha_t  \\end{align*} \\] <p>La predicci\u00f3n final asigna a cada clase la suma de pesos de los clasificadores que la predicen, eligiendo la clase con mayor suma acumulada.</p>"},{"location":"05-adaboost/#adaboostr2-para-regresion","title":"AdaBoost.R2 para regresi\u00f3n","text":"<p>AdaBoost.R2 (Drucker, 1997)10 adapta AdaBoost al problema de regresi\u00f3n. La dificultad principal es que en regresi\u00f3n no existe un concepto de \"azar\" tan claro como en clasificaci\u00f3n, y el error no puede definirse como una proporci\u00f3n de ejemplos mal clasificados.</p> <p>Lo que plantea esta variante es normalizar el error de cada ejemplo al rango \\([0,1]\\), para as\u00ed poder recuperar la condici\u00f3n de debilidad \\(\\epsilon_t &lt; 0.5\\). Para cada ejemplo \\(i\\) y cada iteraci\u00f3n \\(t\\), se define la p\u00e9rdida normalizada de la siguiente forma:</p> \\[L_i^{(t)} = \\frac{|y_i - h_t(\\mathbf{x}_i)|}{\\max_j |y_j - h_t(\\mathbf{x}_j)|}\\] <p>Esta p\u00e9rdida normalizada toma valores en \\([0,1]\\) y mide el error relativo al peor error cometido por el regresor base en esa iteraci\u00f3n. El ejemplo con peor error tendr\u00e1 \\(L_i^{(t)}=1\\), y el que mejor se ajuste tendr\u00e1 un valor cercano a \\(0\\). Hay variantes que en lugar de utilizar error lineal utilizan error cuadr\u00e1tico o exponencial.  </p> <p>A partir de la p\u00e9rdida normalizada calculamos el error ponderado del regresor base:</p> \\[\\epsilon_t = \\sum_{i=1}^{N} w_i^{(t)} L_i^{(t)}\\] <p>Dado que la condici\u00f3n de debilidad es \\(\\epsilon_t &lt; 0.5\\), si obtenemos \\(\\epsilon_t \\geq 0.5\\) el algoritmo se detendr\u00e1. Podemos interpretar este error ponderado como una esperanza. Es decir, \\(\\epsilon_t\\) ser\u00eda el valor esperado del error normalizado bajo la distribuci\u00f3n de pesos actual. </p> <p>Como pesos para los estimadores \\(h_t\\), en regresi\u00f3n en lugar de utilizar \\(\\alpha_t\\) utilizaremos \\(\\beta_t\\), que se calcula de la siguiente forma:</p> \\[ \\beta_t = \\frac{\\epsilon_t}{1 - \\epsilon_t} \\] <p>El valor de este peso estar\u00e1 dentro del rango \\(\\beta_t \\in (0,1)\\) cuando se cumpla la condici\u00f3n \\(\\epsilon_t &lt; 0.5\\). Tendr\u00edamos \\(\\beta_t = 1\\) en caso de tener un regresor muy malo, justo en el l\u00edmite \\(\\epsilon_t = 0.5\\), y el valor ser\u00e1 menor conforme mejor sea el regresor, aproxim\u00e1ndose a \\(0\\) en los mejores casos.</p> <p>La actualizaci\u00f3n de los pesos se har\u00e1 de la siguiente forma:</p> \\[ w_i^{(t+1)} = w_i^{(t)} \\beta_t^{1-L_i^{(t)}} \\] <p>Interpretando el exponente de la actualizaci\u00f3n anterior tenemos:</p> <ul> <li> <p>Si el error del ejemplo \\(i\\) es peque\u00f1o (\\(L_i^{(t)}\\) es cercano a \\(0\\)), entonces el exponente ser\u00e1 cercano a \\(1\\) y el peso se multiplica por \\(\\beta_t &lt; 1\\), lo cual har\u00e1 que se reduzca.</p> </li> <li> <p>Si por el contrario el error del ejemplo \\(i\\) es m\u00e1ximo (\\(L_i^{(t)}\\) es \\(1\\)), entonces el exponente ser\u00e1  \\(0\\) y el peso se multiplica por \\(\\beta_t^0 = 1\\), manteniendo su valor.</p> </li> </ul> <p>Al igual que en el caso de clasificaci\u00f3n, esto har\u00e1 que el peso de los ejemplos dif\u00edciles crezca, mientras que los f\u00e1ciles perder\u00e1n peso.</p> <p>AdaBoost.R2 difiere considerablemente respecto a AdaBoost en la forma de obtener la predicci\u00f3n final. En lugar de una suma ponderada de predicciones, se obtendr\u00e1 mediante una mediana ponderada. La elecci\u00f3n de la mediana en lugar de la media se debe a que la mediana ser\u00e1 m\u00e1s robusta frente a predicciones extremas de regresores de baja calidad.</p> <p>A continuaci\u00f3n se muestra el algoritmo completo:</p> \\[ \\begin{align*} &amp; \\text{Entrada: } \\text{Conjunto de entrenamiento } \\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N \\\\ &amp; w_i^{(1)} \\leftarrow \\frac{1}{N} \\quad \\forall i \\in 1, 2, \\ldots, N \\quad \\text{(Inicializa todos los ejemplos con peso uniforme)} \\\\ &amp; \\text{Para  } t = 1, \\ldots, T \\\\ &amp; \\quad h_t \\leftarrow \\text{Entrenar regresor base con pesos } w_i^{(t)} \\\\ &amp; \\quad L_i^{(t)} \\leftarrow \\frac{|y_i - h_t(\\mathbf{x}_i)|}{\\max_j |y_j - h_t(\\mathbf{x}_j)|} \\quad \\forall i \\quad \\text{(P\u00e9rdida normalizada)}  \\\\ &amp; \\quad \\epsilon_t \\leftarrow \\sum_{i=1}^{N} w_i^{(t)}  L_i^{(t)} \\quad \\text{(Error ponderado del regresor base)}  \\\\ &amp; \\quad \\beta_t \\leftarrow \\frac{\\epsilon_t}{1 - \\epsilon_t} \\quad \\text{(Calcula el peso del regresor)}  \\\\ &amp; \\quad w_i^{(t+1)} \\leftarrow w_i^{(t)}  \\beta_t^{1 - L_i^{(t)}} \\quad \\forall i \\quad \\text{(Actualizaci\u00f3n de pesos de los ejemplos)}  \\\\ &amp; \\quad w_i^{(t+1)} \\leftarrow \\frac{w_i^{(t+1)}}{\\sum_{j=1}^N w_j^{(t+1)}}  \\quad \\forall i \\quad \\text{(Normaliza los nuevos pesos)} \\\\ &amp; \\text{Devuelve: } H(\\mathbf{x}) = \\text{mediana ponderada de } \\{h_t(\\mathbf{x})\\}_{t=1}^T \\text{ con pesos } \\ln\\frac{1}{\\beta_t}  \\end{align*} \\] <p>Observamos que la actualizaci\u00f3n de pesos act\u00faa de forma inversa a la clasificaci\u00f3n: los ejemplos con error peque\u00f1o (\\(L_i^{(t)} \\approx 0\\)) reducen su peso, ya que \\(\\beta_t^{1-L_i^{(t)}} \\approx \\beta_t &lt; 1\\), mientras que los ejemplos dif\u00edciles conservan o aumentan su importancia relativa.</p> <p>La predicci\u00f3n final es la mediana ponderada de los regresores base, donde el peso de cada regresor es \\(\\ln(1/\\beta_t)\\): los regresores con menor error ponderado \\(\\epsilon_t\\) tienen mayor \\(\\ln(1/\\beta_t)\\) y por tanto contribuyen m\u00e1s.</p>"},{"location":"05-adaboost/#implementacion","title":"Implementaci\u00f3n","text":"<p>En sklearn contamos con las implementaciones AdaBoostClassifier y AdaBoostRegressor para problemas de clasificaci\u00f3n y regresi\u00f3n, respectivamente.</p> <p>En este caso, <code>AdaBoostClassifier</code> implementa el algoritmo SAMME, mientras que <code>AdaBoostRegressor</code> implementa AdaBoost.R2. </p> <p>A continuaci\u00f3n podemos ver un ejemplo de c\u00f3digo que utiliza AdaBoost para clasificaci\u00f3n con decision stumps, lo cual es la opci\u00f3n por defecto.</p> <pre><code>ada = AdaBoostClassifier(\nestimator=DecisionTreeClassifier(max_depth=1),  # Decision stump\nn_estimators=50         # N\u00famero de clasificadores d\u00e9biles\n)\nada.fit(X_train, y_train)\n# Inspeccionar clasificadores y pesos\nprint(\"Pesos de clasificadores:\", ada.estimator_weights_)\nprint(\"Errores de clasificadores:\", ada.estimator_errors_)\n</code></pre> <p>De igual forma, a continuaci\u00f3n incluimos un ejemplo de regresi\u00f3n:</p> <pre><code>reg = AdaBoostRegressor(\nestimator=DecisionTreeRegressor(max_depth=4),\nn_estimators=200,\nloss='linear'   \n)\nreg.fit(X_train, y_train)\n</code></pre> <p>En este caso contamos con el par\u00e1metro <code>loss</code> que permite elegir c\u00f3mo se calcula la p\u00e9rdida normalizada \\(L_i^{(t)}\\). Con <code>'linear'</code> se usa el error absoluto normalizado (el descrito en el algoritmo anterior), con <code>'square'</code> el error cuadr\u00e1tico normalizado, y con <code>'exponential'</code> una p\u00e9rdida exponencial normalizada. La opci\u00f3n <code>'linear'</code> es la correspondiente al algoritmo AdaBoost.R2 original y suele ser la m\u00e1s robusta.</p>"},{"location":"05-adaboost/#consideraciones-finales","title":"Consideraciones finales","text":"<p>AdaBoost tiene varias ventajas. Es f\u00e1cil de entender e implementar, puede funcionar con cualquier clasificador d\u00e9bil y tiene pocos hiperpar\u00e1metros. No necesitamos conocer los errores \\(\\epsilon_t\\), el algoritmo se adapta autom\u00e1ticamente. Puede alcanzar buenos resultados con clasificadores muy simples.</p> <p>Sin embargo, tambi\u00e9n encontramos una serie de desventajas. No es paralelizable como otros tipos de ensembles y puede ser costoso si contamos con clasificadores d\u00e9biles lentos. Debemos tener en cuenta tambi\u00e9n que no funciona con clasificadores d\u00e9biles peores que el azar.</p> <p>La principal desventaja que deberemos tener en cuenta es que es muy sensible al ruido y a los outliers. Hemos visto que al utilizar la funci\u00f3n de p\u00e9rdida exponencial, se da un peso desmesurado a ejemplos muy mal clasificados, dominando as\u00ed la optimizaci\u00f3n. Esto har\u00e1 que AdaBoost acabar\u00e1 dedicando casi toda su capacidad a intentar clasificar correctamente un ejemplo imposible, degradando el rendimiento en el resto. </p> <p>Pero si AdaBoost equivale a minimizar la p\u00e9rdida exponencial, \u00bfqu\u00e9 ocurrir\u00eda si cambiamos esa funci\u00f3n de p\u00e9rdida? Si utilizamos p\u00e9rdida log\u00edstica tendremos la variante LogitBoost que ser\u00e1 m\u00e1s robusta frente al ruido al crecer linealmente para errores grandes. Si generalizamos para cualquier funci\u00f3n de p\u00e9rdida diferenciable obtendremos Gradient Boosting, que estudiaremos en la siguiente sesi\u00f3n.</p> <p>En el caso de regresi\u00f3n encontramos que AdaBoost.R2 presenta dos debilidades estructurales:</p> <ul> <li> <p>La primera es que la normalizaci\u00f3n por el m\u00e1ximo error hace el algoritmo muy sensible a outliers, al igual que ocurre en el caso de la clasificaci\u00f3n. Un \u00fanico ejemplo con un error enorme hace que el denominador sea muy grande y que todos los dem\u00e1s errores normalizados sean casi cero, lo que distorsiona completamente el c\u00e1lculo de \\(\\epsilon_t\\) y de los pesos.</p> </li> <li> <p>La segunda es que la elecci\u00f3n de la funci\u00f3n de p\u00e9rdida no viene de un marco te\u00f3rico coherente como en el caso de la clasificaci\u00f3n. En clasificaci\u00f3n, la actualizaci\u00f3n de pesos se deriva directamente de la p\u00e9rdida exponencial. En AdaBoost.R2, la normalizaci\u00f3n y la actualizaci\u00f3n son heur\u00edsticas razonables pero no tienen una justificaci\u00f3n como minimizadores de ninguna funci\u00f3n de p\u00e9rdida concreta.</p> </li> </ul> <p>Gradient Boosting resuelve ambos problemas: permite elegir expl\u00edcitamente la funci\u00f3n de p\u00e9rdida en funci\u00f3n de las necesidades del problema, y est\u00e1 te\u00f3ricamente bien fundamentado. Por ello, en la pr\u00e1ctica AdaBoost.R2 ha sido ampliamente superado por Gradient Boosting.</p> <p>Tanto si tenemos un problema de regresi\u00f3n, como si en el caso de clasificaci\u00f3n tenemos mucho ruido y outliers o buscamos m\u00e1ximo rendimiento, convendr\u00e1 considerar como alternativa m\u00e9todos de Gradient Boosting.  </p> <ol> <li> <p>Kearns, M., &amp; Valiant, L. G. (1988). Learning Boolean formulae or finite automata is as hard as factoring (Nos. TR-14-88). Harvard University Aiken Computation Laboratory.\u00a0\u21a9</p> </li> <li> <p>Kearns, M., &amp; Valiant, L. G. (1989). Cryptographic limitations on learning Boolean formulae and finite automata. Proceedings of the 21st Annual ACM Symposium on Theory of Computing (STOC'89), 433--444. https://doi.org/10.1145/73007.73049 \u21a9</p> </li> <li> <p>Schapire, R. E. (1990). The strength of weak learnability. Machine Learning, 5(2), 197--227.\u00a0\u21a9</p> </li> <li> <p>Freund, Y., &amp; Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1), 119--139.\u00a0\u21a9\u21a9</p> </li> <li> <p>Schapire, R. E., &amp; Freund, Y. (2012). Boosting: Foundations and algorithms. MIT Press.\u00a0\u21a9</p> </li> <li> <p>Schapire, R. E., &amp; Singer, Y. (1999). Improved boosting algorithms using confidence-rated predictions. Machine Learning, 37(3), 297--336. https://doi.org/10.1023/A:1007614523901 \u21a9</p> </li> <li> <p>Friedman, J., Hastie, T., &amp; Tibshirani, R. (2000). Additive logistic regression: A statistical view of boosting (Special Invited Paper). The Annals of Statistics, 28(2), 337--407. https://doi.org/10.1214/aos/1016218223 \u21a9</p> </li> <li> <p>Freund, Y., &amp; Schapire, R. E. (1996). Experiments with a new boosting algorithm. In L. Saitta (Ed.), Proceedings of the 13th international conference on machine learning (ICML'96) (pp. 148--156). Morgan Kaufmann.\u00a0\u21a9</p> </li> <li> <p>Zhu, J., Rosset, S., Zou, H., &amp; Hastie, T. (2009). Multi-class AdaBoost. Statistics and Its Interface, 2(3), 349--360. https://doi.org/10.4310/SII.2009.v2.n3.a8 \u21a9\u21a9</p> </li> <li> <p>Drucker, H. (1997). Improving regressors using boosting techniques. Proceedings of the 14th International Conference on Machine Learning (ICML'97), 107--115.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"06-gradient-boosting/","title":"Gradient Boosting","text":""},{"location":"06-gradient-boosting/#gradient-boosting","title":"Gradient Boosting","text":""},{"location":"06-gradient-boosting/#xgboost","title":"XGBoost","text":""},{"location":"06-gradient-boosting/#42-gradient-boosting","title":"4.2. Gradient Boosting","text":"<p>Gradient Boosting generaliza la idea de boosting mediante una perspectiva de optimizaci\u00f3n num\u00e9rica. En lugar de ajustar pesos de ejemplos (como AdaBoost), cada nuevo modelo se ajusta al gradiente negativo de la funci\u00f3n de p\u00e9rdida.</p> <p>Desarrollado por Jerome Friedman (1999-2001), Gradient Boosting es la base te\u00f3rica de m\u00e9todos modernos como XGBoost, LightGBM y CatBoost.</p>"},{"location":"06-gradient-boosting/#marco-conceptual","title":"Marco Conceptual","text":"<p>Idea clave: Ver boosting como un problema de optimizaci\u00f3n en el espacio de funciones.</p> <p>Queremos encontrar una funci\u00f3n \\(F(x)\\) que minimice la p\u00e9rdida esperada:</p> \\[F^* = \\arg\\min_F \\mathbb{E}_{x,y}[L(y, F(x))]\\] <p>Donde \\(L\\) es una funci\u00f3n de p\u00e9rdida diferenciable.</p> <p>Aproximaci\u00f3n: Construir \\(F\\) como una suma de funciones m\u00e1s simples:</p> \\[F(x) = \\sum_{m=0}^{M} f_m(x)\\] <p>Donde cada \\(f_m\\) es un modelo d\u00e9bil (t\u00edpicamente un \u00e1rbol).</p>"},{"location":"06-gradient-boosting/#analogia-con-gradient-descent","title":"Analog\u00eda con Gradient Descent","text":"<p>Gradient Descent (optimizaci\u00f3n en espacio de par\u00e1metros): </p><pre><code>\u03b8 = \u03b8\u2080  # Inicializaci\u00f3n\nfor t in range(T):\n    g = \u2207L(\u03b8)  # Gradiente respecto a par\u00e1metros\n    \u03b8 = \u03b8 - \u03b7 * g  # Actualizaci\u00f3n\n</code></pre><p></p> <p>Gradient Boosting (optimizaci\u00f3n en espacio de funciones): </p><pre><code>F = F\u2080  # Inicializaci\u00f3n\nfor m in range(M):\n    g = \u2207L(F)  # Gradiente respecto a predicciones\n    f_m = fit_model(g)  # Ajustar modelo a gradientes\n    F = F + \u03b7 * f_m  # Actualizaci\u00f3n\n</code></pre><p></p>"},{"location":"06-gradient-boosting/#algoritmo-general-de-gradient-boosting","title":"Algoritmo General de Gradient Boosting","text":"<p>Inicializaci\u00f3n: </p><pre><code>F\u2080(x) = arg min_\u03b3 \u03a3\u1d62 L(y\u1d62, \u03b3)\n\nT\u00edpicamente:\n- Regresi\u00f3n: F\u2080(x) = mean(y)\n- Clasificaci\u00f3n binaria: F\u2080(x) = log(p/(1-p)) donde p = mean(y)\n</code></pre><p></p> <p>Para m = 1 hasta M:</p> <p>Paso 1: Calcular pseudo-residuos (gradientes negativos) </p><pre><code>r\u1d62\u2098 = -[\u2202L(y\u1d62, F(x\u1d62))/\u2202F(x\u1d62)]_{F=F_{m-1}}\n\nEstos son los \"residuos generalizados\"\n</code></pre><p></p> <p>Paso 2: Ajustar modelo d\u00e9bil a los residuos </p><pre><code>h\u2098 = arg min_h \u03a3\u1d62 (r\u1d62\u2098 - h(x\u1d62))\u00b2\n\nT\u00edpicamente h\u2098 es un \u00e1rbol de decisi\u00f3n poco profundo\n</code></pre><p></p> <p>Paso 3: Encontrar multiplicador \u00f3ptimo (line search) </p><pre><code>\u03b3\u2098 = arg min_\u03b3 \u03a3\u1d62 L(y\u1d62, F_{m-1}(x\u1d62) + \u03b3\u00b7h\u2098(x\u1d62))\n\nEn la pr\u00e1ctica, a menudo se fija \u03b3\u2098 = 1\n</code></pre><p></p> <p>Paso 4: Actualizar el ensemble </p><pre><code>F\u2098(x) = F_{m-1}(x) + \u03bd\u00b7\u03b3\u2098\u00b7h\u2098(x)\n\nDonde \u03bd \u2208 (0,1] es el learning rate\n</code></pre><p></p> <p>Predicci\u00f3n final: </p><pre><code>\u0177 = F\u2098(x)\n</code></pre><p></p>"},{"location":"06-gradient-boosting/#funciones-de-perdida-comunes","title":"Funciones de P\u00e9rdida Comunes","text":""},{"location":"06-gradient-boosting/#regresion","title":"Regresi\u00f3n","text":"<p>1. Squared Error (L2): </p><pre><code>L(y, F) = (y - F)\u00b2/2\n\nGradiente: \u2202L/\u2202F = F - y\nResiduo: r = y - F  (residuo ordinario)\n</code></pre><p></p> <p>2. Absolute Error (L1): </p><pre><code>L(y, F) = |y - F|\n\nGradiente: \u2202L/\u2202F = sign(F - y)\nResiduo: r = sign(y - F)\nM\u00e1s robusto a outliers que L2\n</code></pre><p></p> <p>3. Huber Loss: </p><pre><code>L(y, F) = {\n    (y - F)\u00b2/2           si |y - F| \u2264 \u03b4\n    \u03b4|y - F| - \u03b4\u00b2/2      si |y - F| &gt; \u03b4\n}\n\nCombina robustez de L1 cerca de 0 con suavidad de L2\n</code></pre><p></p> <p>4. Quantile Loss: </p><pre><code>L(y, F) = \u03a3\u1d62 \u03c1_\u03b1(y\u1d62 - F(x\u1d62))\n\nDonde \u03c1_\u03b1(u) = u\u00b7(\u03b1 - \ud835\udfd9(u &lt; 0))\n\nPermite predecir cuantiles (ej: mediana con \u03b1=0.5)\n</code></pre><p></p>"},{"location":"06-gradient-boosting/#clasificacion-binaria","title":"Clasificaci\u00f3n Binaria","text":"<p>Logistic Loss (Deviance): </p><pre><code>y \u2208 {-1, +1}\nL(y, F) = log(1 + exp(-2yF))\n\nGradiente: \u2202L/\u2202F = -2y/(1 + exp(2yF))\nResiduo: r = 2y/(1 + exp(2yF))\n\nPredicci\u00f3n: P(y=1|x) = 1/(1 + exp(-2F(x)))\n</code></pre><p></p> <p>Exponential Loss: </p><pre><code>L(y, F) = exp(-yF)\n\nGradiente: \u2202L/\u2202F = -y\u00b7exp(-yF)\n\nNota: Esta es la p\u00e9rdida que AdaBoost minimiza impl\u00edcitamente\n</code></pre><p></p>"},{"location":"06-gradient-boosting/#clasificacion-multiclase","title":"Clasificaci\u00f3n Multiclase","text":"<p>Softmax (Multinomial Deviance): </p><pre><code>K clases, entrenar K funciones F_k(x)\n\nL(y, F) = -\u03a3\u2096 y\u2096\u00b7log(p\u2096)\n\nDonde p\u2096 = exp(F\u2096)/\u03a3\u2c7c exp(F\u2c7c)\n</code></pre><p></p>"},{"location":"06-gradient-boosting/#ejemplo-con-squared-loss","title":"Ejemplo con Squared Loss","text":"<p>Para entender mejor, veamos un ejemplo completo con squared loss: </p><pre><code>import numpy as np\n# Datos sint\u00e9ticos\nnp.random.seed(42)\nX = np.linspace(0, 10, 100).reshape(-1, 1)\ny = np.sin(X).ravel() + np.random.normal(0, 0.1, 100)\n# Inicializaci\u00f3n: F\u2080 = mean(y)\nF = np.full(100, y.mean())\n# Par\u00e1metros\nlearning_rate = 0.1\nn_estimators = 100\n# Boosting manual\nfor m in range(n_estimators):\n# Paso 1: Calcular residuos\nresiduals = y - F\n# Paso 2: Ajustar \u00e1rbol a residuos\n# (aqu\u00ed simplificado, en pr\u00e1ctica se usa un \u00e1rbol real)\ntree = fit_tree(X, residuals)  # \u00c1rbol de profundidad limitada\n# Paso 3: Predecir con el nuevo \u00e1rbol\npredictions = tree.predict(X)\n# Paso 4: Actualizar F\nF = F + learning_rate * predictions\nif m % 10 == 0:\nmse = np.mean((y - F)**2)\nprint(f\"Iteraci\u00f3n {m}: MSE = {mse:.4f}\")\n</code></pre><p></p>"},{"location":"06-gradient-boosting/#learning-rate-shrinkage","title":"Learning Rate (Shrinkage)","text":"<p>El learning rate \\(\\nu\\) controla la contribuci\u00f3n de cada \u00e1rbol:</p> \\[F_m(x) = F_{m-1}(x) + \\nu \\cdot h_m(x)\\] <p>Efecto: - \u03bd grande (ej: 1.0): Convergencia r\u00e1pida, riesgo de overfitting - \u03bd peque\u00f1o (ej: 0.01-0.1): Convergencia lenta, mejor generalizaci\u00f3n</p> <p>Trade-off learning rate vs n\u00famero de \u00e1rboles: </p><pre><code>\u03bd = 1.0 \u2192 50 \u00e1rboles suficientes, pero puede overfit\n\u03bd = 0.1 \u2192 500 \u00e1rboles necesarios, mejor generalizaci\u00f3n\n\u03bd = 0.01 \u2192 5000 \u00e1rboles necesarios, excelente generalizaci\u00f3n\n</code></pre><p></p> <p>Recomendaci\u00f3n pr\u00e1ctica: - Usar \u03bd peque\u00f1o (0.01-0.1) - Aumentar n_estimators proporcionalmente - Usar early stopping para encontrar el n\u00famero \u00f3ptimo</p>"},{"location":"06-gradient-boosting/#arboles-de-decision-en-gradient-boosting","title":"\u00c1rboles de Decisi\u00f3n en Gradient Boosting","text":"<p>Caracter\u00edsticas t\u00edpicas: - Poco profundos: max_depth = 3-8 (vs Random Forest: sin l\u00edmite) - Capturam interacciones: Profundidad k \u2192 interacciones de orden k - R\u00e1pidos de entrenar: \u00c1rboles peque\u00f1os - Complementarios: Cada \u00e1rbol captura un patr\u00f3n residual diferente</p> <p>Ejemplo: </p><pre><code>from sklearn.tree import DecisionTreeRegressor\n# \u00c1rbol t\u00edpico en Gradient Boosting\ntree = DecisionTreeRegressor(\nmax_depth=3,           # Poco profundo\nmin_samples_split=20,  # Evitar splits en pocas muestras\nmin_samples_leaf=10    # Hojas razonablemente grandes\n)\n</code></pre><p></p>"},{"location":"06-gradient-boosting/#implementacion-en-scikit-learn","title":"Implementaci\u00f3n en scikit-learn","text":"<pre><code>from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n# Gradient Boosting para clasificaci\u00f3n\ngb_clf = GradientBoostingClassifier(\nn_estimators=100,         # N\u00famero de \u00e1rboles\nlearning_rate=0.1,        # Shrinkage\nmax_depth=3,              # Profundidad de \u00e1rboles\nmin_samples_split=20,     # M\u00ednimo para split\nmin_samples_leaf=10,      # M\u00ednimo en hojas\nsubsample=0.8,            # Stochastic GB (fracci\u00f3n de muestras)\nmax_features='sqrt',      # Features por split\nloss='log_loss',          # Funci\u00f3n de p\u00e9rdida (default)\nrandom_state=42\n)\ngb_clf.fit(X_train, y_train)\n# Gradient Boosting para regresi\u00f3n\ngb_reg = GradientBoostingRegressor(\nn_estimators=100,\nlearning_rate=0.1,\nmax_depth=3,\nloss='squared_error',     # Para regresi\u00f3n (default)\nrandom_state=42\n)\ngb_reg.fit(X_train, y_train)\n</code></pre>"},{"location":"06-gradient-boosting/#stochastic-gradient-boosting","title":"Stochastic Gradient Boosting","text":"<p>Introducido por Friedman (1999), a\u00f1ade subsample de datos en cada iteraci\u00f3n: </p><pre><code>Para m = 1 hasta M:\n    1. Muestrear fracci\u00f3n f de datos (sin reemplazo)\n    2. Calcular residuos en subsample\n    3. Entrenar \u00e1rbol en subsample\n    4. Actualizar F en todo el dataset\n</code></pre><p></p> <p>Ventajas: - \u2705 Reduce overfitting (m\u00e1s regularizaci\u00f3n) - \u2705 M\u00e1s r\u00e1pido (menos datos por \u00e1rbol) - \u2705 Introduce diversidad entre \u00e1rboles</p> <p>Subsample t\u00edpico: 0.5-0.8 </p><pre><code>gb = GradientBoostingClassifier(\nsubsample=0.8,  # Usar 80% de datos por \u00e1rbol\nn_estimators=100,\nlearning_rate=0.1\n)\n</code></pre><p></p>"},{"location":"06-gradient-boosting/#hiperparametros-importantes","title":"Hiperpar\u00e1metros Importantes","text":""},{"location":"06-gradient-boosting/#control-de-complejidad","title":"Control de complejidad:","text":"<p>1. n_estimators: N\u00famero de \u00e1rboles - M\u00e1s \u00e1rboles \u2192 m\u00e1s capacidad - Usar con learning_rate bajo y early stopping - T\u00edpico: 100-1000</p> <p>2. learning_rate: Tasa de aprendizaje - Controla contribuci\u00f3n de cada \u00e1rbol - T\u00edpico: 0.01-0.1 - Trade-off con n_estimators</p> <p>3. max_depth: Profundidad m\u00e1xima de \u00e1rboles - Controla interacciones capturadas - T\u00edpico: 3-8 - Profundidad k \u2192 interacciones de orden k</p> <p>4. min_samples_split / min_samples_leaf: - Previene splits en pocas muestras - Regularizaci\u00f3n importante - T\u00edpico: 20/10</p> <p>5. subsample: Fracci\u00f3n de muestras por \u00e1rbol - Stochastic GB - T\u00edpico: 0.8 - Reduce overfitting</p> <p>6. max_features: Features consideradas por split - Similar a Random Forest - T\u00edpico: 'sqrt' o None - A\u00f1ade diversidad</p>"},{"location":"06-gradient-boosting/#early-stopping","title":"Early Stopping","text":"<p>Detener el entrenamiento cuando el rendimiento en validaci\u00f3n deja de mejorar: </p><pre><code>gb = GradientBoostingClassifier(\nn_estimators=1000,        # N\u00famero grande\nlearning_rate=0.01,       # Learning rate bajo\nvalidation_fraction=0.2,  # Fracci\u00f3n para validaci\u00f3n\nn_iter_no_change=50,      # Parar si no mejora en 50 iteraciones\ntol=1e-4,                 # Tolerancia de mejora\nrandom_state=42\n)\ngb.fit(X_train, y_train)\n# N\u00famero \u00f3ptimo de \u00e1rboles encontrado\nprint(f\"N\u00famero \u00f3ptimo de \u00e1rboles: {gb.n_estimators_}\")\n</code></pre><p></p>"},{"location":"06-gradient-boosting/#ventajas-de-gradient-boosting","title":"Ventajas de Gradient Boosting","text":"<ul> <li>Muy efectivo: Estado del arte en muchos problemas</li> <li>Flexible: Cualquier funci\u00f3n de p\u00e9rdida diferenciable</li> <li>Reduce sesgo y varianza: Mejor que bagging o boosting b\u00e1sico</li> <li>Maneja features mixtas: Num\u00e9ricas y categ\u00f3ricas</li> <li>Robusto: Con funciones de p\u00e9rdida apropiadas</li> <li>Interpretable: Feature importance disponible</li> </ul>"},{"location":"06-gradient-boosting/#limitaciones-de-gradient-boosting","title":"Limitaciones de Gradient Boosting","text":"<ul> <li>Propenso a overfitting: Requiere tuning cuidadoso</li> <li>No paralelizable: Entrenamiento secuencial</li> <li>Lento: Comparado con Random Forest</li> <li>Sensible a hiperpar\u00e1metros: M\u00e1s que Random Forest</li> <li>Sensible a outliers: Con squared loss</li> </ul>"},{"location":"06-gradient-boosting/#prevencion-de-overfitting","title":"Prevenci\u00f3n de Overfitting","text":"<p>Estrategias m\u00faltiples: </p><pre><code>gb = GradientBoostingClassifier(\n# 1. Reduce learning rate, aumenta \u00e1rboles\nlearning_rate=0.01,\nn_estimators=1000,\n# 2. Limita complejidad de \u00e1rboles\nmax_depth=3,\nmin_samples_split=20,\nmin_samples_leaf=10,\n# 3. Stochastic GB\nsubsample=0.8,\nmax_features='sqrt',\n# 4. Early stopping\nvalidation_fraction=0.2,\nn_iter_no_change=50,\nrandom_state=42\n)\n</code></pre><p></p>"},{"location":"06-gradient-boosting/#comparacion-adaboost-vs-gradient-boosting","title":"Comparaci\u00f3n: AdaBoost vs Gradient Boosting","text":"Aspecto AdaBoost Gradient Boosting Actualizaci\u00f3n Pesos de ejemplos Ajuste a residuos/gradientes Funci\u00f3n de p\u00e9rdida Exponential (fija) Cualquiera (flexible) Clasificador d\u00e9bil Cualquiera T\u00edpicamente \u00e1rboles Robustez a outliers Baja Media-Alta (depende de loss) Flexibilidad Menor Mayor Teor\u00eda M\u00e1s simple M\u00e1s general Rendimiento Bueno Excelente"},{"location":"06-gradient-boosting/#ejemplo-completo","title":"Ejemplo Completo","text":"<pre><code>from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport numpy as np\n# Datos\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Modelo\ngb = GradientBoostingClassifier(\nn_estimators=100,\nlearning_rate=0.1,\nmax_depth=3,\nsubsample=0.8,\nrandom_state=42,\nverbose=1  # Mostrar progreso\n)\n# Entrenar\ngb.fit(X_train, y_train)\n# Evaluar\nprint(f\"Train score: {gb.score(X_train, y_train):.4f}\")\nprint(f\"Test score: {gb.score(X_test, y_test):.4f}\")\n# Feature importance\nimportances = gb.feature_importances_\ntop_features = np.argsort(importances)[::-1][:5]\nprint(\"\\nTop 5 features:\")\nfor i in top_features:\nprint(f\"Feature {i}: {importances[i]:.4f}\")\n# Learning curves\ntrain_scores = []\ntest_scores = []\nfor i, (train_pred, test_pred) in enumerate(zip(\ngb.staged_predict(X_train),\ngb.staged_predict(X_test)\n)):\ntrain_scores.append(np.mean(train_pred == y_train))\ntest_scores.append(np.mean(test_pred == y_test))\n# El error converge gradualmente\nprint(f\"\\nScores cada 25 \u00e1rboles:\")\nfor i in [24, 49, 74, 99]:\nprint(f\"\u00c1rboles {i+1}: Train={train_scores[i]:.4f}, Test={test_scores[i]:.4f}\")\n</code></pre>"},{"location":"06-gradient-boosting/#43-histgradientboosting-scikit-learn","title":"4.3. HistGradientBoosting (scikit-learn)","text":"<p>HistGradientBoosting es la implementaci\u00f3n moderna de Gradient Boosting en scikit-learn, inspirada en LightGBM y disponible desde la versi\u00f3n 0.21.</p> <p>Innovaci\u00f3n principal: Usar histogramas para discretizar features continuas, acelerando enormemente la b\u00fasqueda de splits.</p>"},{"location":"06-gradient-boosting/#diferencias-con-gradientboosting-tradicional","title":"Diferencias con GradientBoosting tradicional","text":"Aspecto GradientBoosting HistGradientBoosting B\u00fasqueda de splits Exacta (todos los valores) Histogramas (valores discretizados) Velocidad Lenta 10-100x m\u00e1s r\u00e1pida Escalabilidad Dataset mediano Dataset grande Missing values No soportado Soporte nativo Features categ\u00f3ricas Manual encoding Soporte experimental API n_estimators max_iter"},{"location":"06-gradient-boosting/#algoritmo-basado-en-histogramas","title":"Algoritmo basado en Histogramas","text":"<p>Idea: En lugar de considerar todos los posibles valores para splits, discretizar features en bins (histogramas). </p><pre><code>Feature continua: [0.1, 0.3, 0.5, 0.7, 0.9, 1.2, 1.5, ...]\n\nTradicional: Considerar todos los valores como candidatos a split\n\u2192 O(n * p) comparaciones por nivel\n\nHistogramas: Discretizar en 255 bins\n\u2192 O(255 * p) comparaciones por nivel (independiente de n!)\n</code></pre><p></p> <p>Construcci\u00f3n de histogramas: </p><pre><code>1. Para cada feature:\n   - Calcular quantiles para crear bins\n   - Mapear valores continuos \u2192 \u00edndices de bins (0-254)\n\n2. Para encontrar mejor split:\n   - Solo considerar los 255 bins como candidatos\n   - Acumulaci\u00f3n eficiente de gradientes por bin\n</code></pre><p></p> <p>Ventaja masiva: Complejidad independiente del n\u00famero de ejemplos.</p>"},{"location":"06-gradient-boosting/#implementacion-en-scikit-learn_1","title":"Implementaci\u00f3n en scikit-learn","text":"<pre><code>from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n# Clasificaci\u00f3n\nhgb = HistGradientBoostingClassifier(\nmax_iter=100,              # N\u00famero de \u00e1rboles (equivalente a n_estimators)\nlearning_rate=0.1,         # Tasa de aprendizaje\nmax_depth=None,            # Sin l\u00edmite (usa max_leaf_nodes)\nmax_leaf_nodes=31,         # M\u00e1ximo n\u00famero de hojas (default)\nmin_samples_leaf=20,       # M\u00ednimo en hojas\nl2_regularization=0.0,     # Regularizaci\u00f3n L2\nmax_bins=255,              # N\u00famero de bins (default)\ncategorical_features=None, # \u00cdndices de features categ\u00f3ricas\nearly_stopping='auto',     # Early stopping autom\u00e1tico\nscoring='loss',            # M\u00e9trica para early stopping\nvalidation_fraction=0.1,   # Fracci\u00f3n para validaci\u00f3n\nn_iter_no_change=10,       # Parar si no mejora\nrandom_state=42\n)\nhgb.fit(X_train, y_train)\n</code></pre>"},{"location":"06-gradient-boosting/#diferencias-de-api-importantes","title":"Diferencias de API importantes","text":"<pre><code># GradientBoosting tradicional\nfrom sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(\nn_estimators=100,  # \u2190 Nombre del par\u00e1metro\nmax_depth=3\n)\n# HistGradientBoosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nhgb = HistGradientBoostingClassifier(\nmax_iter=100,      # \u2190 Diferente nombre!\nmax_leaf_nodes=31  # \u2190 Controla complejidad diferente\n)\n</code></pre>"},{"location":"06-gradient-boosting/#manejo-nativo-de-missing-values","title":"Manejo Nativo de Missing Values","text":"<p>Una de las grandes ventajas: </p><pre><code>import numpy as np\n# Datos con valores faltantes\nX_train[0, 3] = np.nan\nX_train[5, 7] = np.nan\n# GradientBoosting: ERROR\n# gb = GradientBoostingClassifier()\n# gb.fit(X_train, y_train)  # ValueError: Input contains NaN\n# HistGradientBoosting: Funciona!\nhgb = HistGradientBoostingClassifier()\nhgb.fit(X_train, y_train)  # OK, aprende direcci\u00f3n \u00f3ptima para NaN\n</code></pre><p></p> <p>C\u00f3mo maneja NaN: - Trata NaN como un valor especial en el histograma - Aprende la mejor direcci\u00f3n (izquierda o derecha) para NaN en cada split - No requiere imputaci\u00f3n manual</p>"},{"location":"06-gradient-boosting/#soporte-experimental-para-features-categoricas","title":"Soporte Experimental para Features Categ\u00f3ricas","text":"<pre><code># Marcar features categ\u00f3ricas\ncategorical_features = [0, 3, 5]  # \u00cdndices de columnas categ\u00f3ricas\nhgb = HistGradientBoostingClassifier(\ncategorical_features=categorical_features\n)\n# O pasar un boolean mask\ncategorical_mask = [True, False, False, True, False, True, False, ...]\nhgb = HistGradientBoostingClassifier(\ncategorical_features=categorical_mask\n)\nhgb.fit(X_train, y_train)\n</code></pre> <p>Ventaja: No requiere one-hot encoding manual, maneja categor\u00edas de forma eficiente.</p>"},{"location":"06-gradient-boosting/#early-stopping-automatico","title":"Early Stopping Autom\u00e1tico","text":"<pre><code>hgb = HistGradientBoostingClassifier(\nmax_iter=1000,            # N\u00famero m\u00e1ximo de iteraciones\nearly_stopping='auto',    # Activar early stopping\nvalidation_fraction=0.1,  # 10% para validaci\u00f3n\nn_iter_no_change=10,      # Parar si no mejora en 10 iteraciones\ntol=1e-4                  # Tolerancia de mejora\n)\nhgb.fit(X_train, y_train)\n# N\u00famero real de iteraciones usadas\nprint(f\"Iteraciones usadas: {hgb.n_iter_}\")\n</code></pre>"},{"location":"06-gradient-boosting/#comparacion-de-velocidad","title":"Comparaci\u00f3n de Velocidad","text":"<pre><code>import time\nfrom sklearn.datasets import make_classification\n# Dataset grande\nX, y = make_classification(n_samples=100000, n_features=50, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n# GradientBoosting tradicional\nstart = time.time()\ngb = GradientBoostingClassifier(n_estimators=100, max_depth=3)\ngb.fit(X_train, y_train)\ngb_time = time.time() - start\n# HistGradientBoosting\nstart = time.time()\nhgb = HistGradientBoostingClassifier(max_iter=100)\nhgb.fit(X_train, y_train)\nhgb_time = time.time() - start\nprint(f\"GradientBoosting: {gb_time:.2f}s, Score: {gb.score(X_test, y_test):.4f}\")\nprint(f\"HistGradientBoosting: {hgb_time:.2f}s, Score: {hgb.score(X_test, y_test):.4f}\")\nprint(f\"Speedup: {gb_time/hgb_time:.1f}x\")\n</code></pre> <p>Resultado t\u00edpico:</p> <p>GradientBoosting: 45.23s, Score: 0.8756 HistGradientBoosting: 2.14s, Score: 0.8812 Speedup: 21.1x</p>"},{"location":"06-gradient-boosting/#hiperparametros-clave","title":"Hiperpar\u00e1metros Clave","text":"<p>Control de iteraciones: - <code>max_iter</code>: N\u00famero de boosting iterations (100-1000) - <code>learning_rate</code>: Shrinkage (0.01-0.1)</p> <p>Control de complejidad: - <code>max_leaf_nodes</code>: M\u00e1ximo n\u00famero de hojas (31 default) - <code>max_depth</code>: Profundidad m\u00e1xima (None = sin l\u00edmite) - <code>min_samples_leaf</code>: M\u00ednimo de muestras en hojas (20 default)Regularizaci\u00f3n:</p> <p>l2_regularization: Regularizaci\u00f3n L2 en hojas (0.0 default) Histogramas:</p> <p>max_bins: N\u00famero de bins para histogramas (255 default, m\u00e1ximo) Early stopping:</p> <p>early_stopping: 'auto', True, False validation_fraction: Fracci\u00f3n para validaci\u00f3n (0.1 default) n_iter_no_change: Paciencia para early stopping (10 default) Ventajas de HistGradientBoosting \u2705 Mucho m\u00e1s r\u00e1pido que GradientBoosting tradicional (10-100x) \u2705 Escala a datasets grandes (millones de ejemplos) \u2705 Manejo nativo de missing values \u2705 Soporte para categ\u00f3ricas (experimental) \u2705 Early stopping incorporado \u2705 Incluido en sklearn (sin instalaci\u00f3n adicional) \u2705 API similar a GradientBoosting tradicional \u2705 Rendimiento comparable a XGBoost/LightGBM Limitaciones \u26a0\ufe0f Features categ\u00f3ricas a\u00fan experimental \u26a0\ufe0f API ligeramente diferente (max_iter vs n_estimators) \u26a0\ufe0f Menos features que XGBoost/LightGBM/CatBoost \u26a0\ufe0f Sin soporte GPU \u26a0\ufe0f Comunidad m\u00e1s peque\u00f1a que XGBoost Cu\u00e1ndo usar HistGradientBoostingUsar HistGradientBoosting cuando:</p> <p>\u2705 Quieres rapidez de LightGBM sin instalaci\u00f3n extra \u2705 Dataset grande (&gt;10k ejemplos, &gt;50 features) \u2705 Tienes missing values (soporte nativo) \u2705 Quieres quedarte dentro de sklearn (sin dependencias) \u2705 No necesitas features avanzadas de XGBoost Considerar alternativas cuando:</p> <p>\u274c Dataset peque\u00f1o \u2192 GradientBoosting tradicional est\u00e1 bien \u274c Necesitas features categ\u00f3ricas robustas \u2192 CatBoost \u274c M\u00e1ximo rendimiento en competici\u00f3n \u2192 XGBoost/LightGBM \u274c Necesitas GPU \u2192 XGBoost/LightGBM/CatBoost</p>"},{"location":"07-clustering/","title":"Clustering","text":""},{"location":"07-clustering/#clustering","title":"Clustering","text":""},{"location":"07-clustering/#introduccion-al-clustering","title":"Introducci\u00f3n al clustering","text":"<ul> <li>Concepto y objetivos</li> <li>Tipos de clustering: particionamiento, jer\u00e1rquico, basado en densidad, probabil\u00edstico</li> <li>M\u00e9tricas de distancia y similitud</li> <li>Evaluaci\u00f3n de clustering (silhouette score, \u00edndice Davies-Bouldin, etc.)</li> </ul>"},{"location":"07-clustering/#clustering-jerarquico","title":"Clustering Jer\u00e1rquico","text":"<ul> <li>Aglomerativo vs. divisivo</li> <li>M\u00e9tricas de linkage: single, complete, average, Ward</li> <li>Dendrogramas: interpretaci\u00f3n y corte</li> <li>Complejidad computacional</li> <li>Ventajas e inconvenientes</li> </ul>"},{"location":"07-clustering/#dbscan","title":"DBSCAN","text":"<ul> <li>Conceptos: core points, border points, noise</li> <li>Par\u00e1metros: epsilon, MinPts</li> <li>Ventajas: formas arbitrarias, detecci\u00f3n de ruido, no requiere especificar K</li> <li>Limitaciones: clusters con diferentes densidades</li> <li>Selecci\u00f3n de par\u00e1metros (k-distance plot)</li> <li>Aplicaciones pr\u00e1cticas</li> </ul>"},{"location":"07-clustering/#gaussian-mixture-models-gmm","title":"Gaussian Mixture Models (GMM)","text":"<ul> <li>De hard clustering a soft clustering</li> <li>Modelo probabil\u00edstico: mezcla de gaussianas</li> <li> <p>EM Algorithm (Expectation-Maximization)</p> <ul> <li>E-step: asignaci\u00f3n probabil\u00edstica</li> <li>M-step: actualizaci\u00f3n de par\u00e1metros</li> <li>Convergencia y garant\u00edas</li> </ul> </li> <li> <p>Selecci\u00f3n del n\u00famero de componentes (BIC, AIC)</p> </li> <li>Ventajas sobre K-Means: clusters el\u00edpticos, asignaci\u00f3n probabil\u00edstica</li> <li>Aplicaciones: densidad de probabilidad, inicializaci\u00f3n</li> </ul> <p>El EM algorithm es conceptualmente importante porque aparece en muchos otros contextos de ML. Est\u00e1 relacionado con K-Means, que es un caso especial de EM con varianzas fijas. La visualizaci\u00f3n de elipses vs. c\u00edrculos ayuda a la comprensi\u00f3n</p>"},{"location":"07-clustering/#clustering-espectral","title":"Clustering Espectral","text":"<ul> <li>Motivaci\u00f3n: limitaciones de K-Means con estructuras complejas</li> <li> <p>Conceptos de teor\u00eda de grafos</p> <ul> <li>Grafo de similitud (KNN graph, epsilon-neighborhood, fully connected)</li> <li>Matriz de adyacencia</li> <li>Matriz de grado</li> <li>Matriz laplaciana (unnormalized, normalized)</li> </ul> </li> <li> <p>Algoritmo de clustering espectral</p> <ul> <li>Construcci\u00f3n del grafo de similitud</li> <li>C\u00e1lculo de autovectores de la laplaciana</li> <li>Aplicaci\u00f3n de K-Means en el espacio transformado</li> </ul> </li> <li> <p>Intuici\u00f3n geom\u00e9trica: proyecci\u00f3n a espacio donde clusters son linealmente separables</p> </li> <li>Relaci\u00f3n con graph cuts (normalized cut, ratio cut)</li> <li>Ventajas: formas arbitrarias, fundamento te\u00f3rico s\u00f3lido</li> <li>Limitaciones: escalabilidad, selecci\u00f3n de par\u00e1metros del grafo</li> <li> <p>Aplicaciones: segmentaci\u00f3n de im\u00e1genes, an\u00e1lisis de redes sociales</p> </li> <li> <p>Ejemplos donde K-Means falla pero espectral funciona (c\u00edrculos conc\u00e9ntricos, espirales)</p> </li> <li>Relaci\u00f3n con graph neural networks </li> </ul>"},{"location":"07-clustering/#isolation-forest-y-deteccion-de-anomalias","title":"Isolation Forest y detecci\u00f3n de anomal\u00edas","text":"<ul> <li>Concepto: anomal\u00edas son f\u00e1ciles de aislar</li> <li>Construcci\u00f3n de \u00e1rboles aleatorios</li> <li>Path length como score de anomal\u00eda</li> <li>Ventajas: eficiente, no requiere asumir distribuci\u00f3n</li> <li>Relaci\u00f3n con Random Forest</li> </ul> <p>Uso de clustering para detecci\u00f3n de anomal\u00edas</p> <ul> <li>Puntos lejanos de centroides (K-Means)</li> <li>Noise points (DBSCAN)</li> <li>Baja probabilidad (GMM)</li> </ul> <p>Isolation Forest est\u00e1 relacionado con Random Forest, pero con una filosof\u00eda completamente diferente. Ambos utilizan conjuntos (ensemble) de \u00e1rboles de decisi\u00f3n, pero con objetivos y mecanismos distintos.</p> <p>Coinciden en que ambos usan ensemble de \u00e1rboles:</p> <ul> <li>Construyen m\u00faltiples \u00e1rboles de decisi\u00f3n</li> <li>Combinan sus resultados para la predicci\u00f3n final</li> <li>Usan aleatorizaci\u00f3n para crear diversidad entre \u00e1rboles</li> </ul> <p>Construcci\u00f3n aleatoria:</p> <ul> <li>Muestreo aleatorio de datos</li> <li>Selecci\u00f3n aleatoria de caracter\u00edsticas</li> </ul> <p>Las diferencias fundamentales son:</p> <ol> <li>Objetivo</li> </ol> <p>Random Forest:</p> <ul> <li>Aprendizaje supervisado (clasificaci\u00f3n o regresi\u00f3n)</li> <li>Objetivo: maximizar la precisi\u00f3n de predicci\u00f3n</li> <li>Reduce varianza mediante promediado</li> </ul> <p>Isolation Forest:</p> <ul> <li>Aprendizaje no supervisado (detecci\u00f3n de anomal\u00edas)</li> <li>Objetivo: identificar puntos an\u00f3malos</li> <li> <p>No hay etiquetas, no predice clases</p> </li> <li> <p>Filosof\u00eda de los \u00e1rboles</p> </li> </ul> <p>Random Forest:</p> <ul> <li>Construye \u00e1rboles profundos hasta conseguir hojas puras (o casi puras)</li> <li>Busca separar bien las clases en el espacio de caracter\u00edsticas</li> <li>Usa criterios de divisi\u00f3n basados en impureza (Gini, entrop\u00eda)</li> </ul> <p>Isolation Forest:</p> <ul> <li>Construye \u00e1rboles poco profundos (altura limitada)</li> <li>No intenta separar clases (no hay etiquetas)</li> <li>Hace divisiones aleatorias del espacio</li> <li>El objetivo NO es pureza, sino aislar puntos</li> </ul>"},{"location":"practicas/00-datos-y-visualizacion/","title":"0. Datos y visualizaci\u00f3n","text":""},{"location":"practicas/00-datos-y-visualizacion/#clase-0-flujo-completo-de-machine-learning-de-datos-a-modelo","title":"Clase 0: Flujo Completo de Machine Learning - De Datos a Modelo","text":"<p>Contexto</p> <p>Esta es la clase inaugural de Aprendizaje Avanzado. Cubriremos el flujo end-to-end de un problema de ML: desde datos crudos hasta un modelo desplegado y rastreado. No nos enfocamos en implementaci\u00f3n, sino en por qu\u00e9 cada paso importa.</p>"},{"location":"practicas/00-datos-y-visualizacion/#1-preprocesamiento-de-datos","title":"1. Preprocesamiento de Datos","text":"<p>El preprocesamiento es donde invertir\u00e1s el 60-70% del tiempo en un proyecto real de ML. Es tentador salt\u00e1rselo, pero un buen preprocesamiento es la diferencia entre un modelo mediocre y uno excelente.</p>"},{"location":"practicas/00-datos-y-visualizacion/#11-exploracion-inicial-y-tipos-de-datos","title":"1.1 Exploraci\u00f3n Inicial y Tipos de Datos","text":"<p>\u00bfQu\u00e9 hacemos aqu\u00ed?</p> <p>Lo primero es entender qu\u00e9 tenemos entre manos. Necesitas responder:</p> <ul> <li>\u00bfCu\u00e1ntas muestras y caracter\u00edsticas tengo?</li> <li>\u00bfQu\u00e9 tipo de datos son? (num\u00e9ricas, categ\u00f3ricas, de fecha, texto)</li> <li>\u00bfHay valores faltantes? \u00bfCu\u00e1ntos?</li> <li>\u00bfCu\u00e1l es mi variable objetivo?</li> </ul> <p>\u00bfPor qu\u00e9 importa?</p> <p>Diferentes tipos de datos requieren diferentes tratamientos. Una variable categ\u00f3rica no se puede alimentar directamente a sklearn. Los valores faltantes pueden sesgar tu modelo. Los tipos incorrectos causan errores silenciosos.</p> <p>En la pr\u00e1ctica:</p> <pre><code>import pandas as pd\nimport numpy as np\n# Cargar datos\ndf = pd.read_csv('./datasets/Iris.csv')\n# Dimensiones\nprint(\"*\" * 40)\nprint(f\"Shape: {df.shape}\")  # (150, 5)\nprint(\"*\" * 40)\n# Tipos de datos\nprint(df.dtypes)\nprint(\"*\" * 40)\n# Primeras filas\nprint(df.head())\nprint(\"*\" * 40)\n# Resumen r\u00e1pido\nprint(df.info())\nprint(\"*\" * 40)\n# Valores faltantes\nprint(df.isnull().sum())\n</code></pre> <p>Output esperado para Iris: </p><pre><code>****************************************\nShape: (150, 6)\n****************************************\nId                 int64\nSepalLengthCm    float64\nSepalWidthCm     float64\nPetalLengthCm    float64\nPetalWidthCm     float64\nSpecies           object\ndtype: object\n****************************************\n   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n0   1            5.1           3.5            1.4           0.2  Iris-setosa\n1   2            4.9           3.0            1.4           0.2  Iris-setosa\n2   3            4.7           3.2            1.3           0.2  Iris-setosa\n3   4            4.6           3.1            1.5           0.2  Iris-setosa\n4   5            5.0           3.6            1.4           0.2  Iris-setosa\n****************************************\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 6 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   Id             150 non-null    int64  \n 1   SepalLengthCm  150 non-null    float64\n 2   SepalWidthCm   150 non-null    float64\n 3   PetalLengthCm  150 non-null    float64\n 4   PetalWidthCm   150 non-null    float64\n 5   Species        150 non-null    object \ndtypes: float64(4), int64(1), object(1)\nmemory usage: 7.2+ KB\nNone\n****************************************\nId               0\nSepalLengthCm    0\nSepalWidthCm     0\nPetalLengthCm    0\nPetalWidthCm     0\nSpecies          0\ndtype: int64\n</code></pre><p></p> <p>Nota importante: Iris no tiene valores faltantes (es por eso que es un dataset \"juguete\"). En la vida real, rara vez ver\u00e1s datos tan limpios.</p>"},{"location":"practicas/00-datos-y-visualizacion/#12-manejo-de-valores-faltantes","title":"1.2 Manejo de Valores Faltantes","text":"<p>\u00bfQu\u00e9 son y por qu\u00e9 aparecen?</p> <p>Valores faltantes <code>(NaN, None)</code> aparecen por:</p> <ul> <li>Errores en la recopilaci\u00f3n</li> <li>Equipos que no registraron datos</li> <li>Confidencialidad (datos intencionalmente omitidos)</li> <li>Procesos de limpieza anteriores</li> </ul> <p>Estrategias de manejo:</p> Estrategia Cu\u00e1ndo usar Ventajas Desventajas Eliminaci\u00f3n de fila &lt;5% de filas con datos faltantes Simple, limpio Pierdes informaci\u00f3n Eliminaci\u00f3n de columna &gt;50% de valores faltantes Limpio Pierdes feature Imputaci\u00f3n por media/mediana Datos num\u00e9ricos, MCAR* R\u00e1pido Reduce varianza, sesgado Imputaci\u00f3n por forward fill Series temporales Preserva contexto Solo para datos ordenados M\u00e9todos avanzados (KNN, IterativeImputer) Datos con patrones M\u00e1s preciso Computacionalmente caro <p>*MCAR = Missing Completely At Random</p> <p>Ejemplo conceptual:</p> <pre><code># Chequear valores faltantes\nmissing = df.isnull().sum()\nprint(missing)\n# Estrategia 1: Eliminar filas con cualquier faltante\ndf_clean = df.dropna()\n# Estrategia 2: Eliminar columnas con &gt;50% faltantes\ndf_clean = df.dropna(thresh=len(df)*0.5, axis=1)\n# Estrategia 3: Imputaci\u00f3n\ndf_imputed = df.fillna(df.mean())  # num\u00e9ricas\ndf_imputed = df.fillna(df.mode()[0])  # categ\u00f3ricas\n# Estrategia 4: Imputaci\u00f3n avanzada (sklearn)\nfrom sklearn.impute import SimpleImputer, KNNImputer\nimputer = KNNImputer(n_neighbors=5)\ndf_imputed = imputer.fit_transform(df)\n</code></pre> <p>Decisi\u00f3n cr\u00edtica: La estrategia que elijas introduce sesgo. Documentalo siempre.</p>"},{"location":"practicas/00-datos-y-visualizacion/#13-tratamiento-de-outliers","title":"1.3 Tratamiento de Outliers","text":"<p>\u00bfQu\u00e9 son outliers?</p> <p>Valores que se desv\u00edan significativamente del resto de la distribuci\u00f3n. Pueden ser: - Errores reales: Tipogr\u00e1ficos, sensores rotos - Valores leg\u00edtimos: Comportamientos extremos pero reales (ej: un cliente que compra 1000 veces)</p> <p>M\u00e9todos de detecci\u00f3n:</p> M\u00e9todo F\u00f3rmula/Criterio Cu\u00e1ndo usar Desviaci\u00f3n est\u00e1ndar \u03bc \u00b1 3\u03c3 Datos normales IQR Q1 - 1.5\u00d7IQR, Q3 + 1.5\u00d7IQR Datos sesgados, robusta Z-score |z| &gt; 3 Datos normalizados Visualizaci\u00f3n Box plots, scatter plots Siempre, como primer paso <p>Ejemplo:</p> <p></p><pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n# Detecci\u00f3n por IQR\nQ1 = df['SepalLengthCm'].quantile(0.25)\nQ3 = df['SepalLengthCm'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = (df['SepalLengthCm'] &lt; Q1 - 1.5*IQR) | (df['SepalLengthCm'] &gt; Q3 + 1.5*IQR)\nprint(f\"Outliers detectados: {outliers.sum()}\")\n# Visualizaci\u00f3n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n# Box plot\naxes[0].boxplot(df['SepalLengthCm'])\naxes[0].set_title('Box Plot - Sepal Length')\naxes[0].set_ylabel('Valor')\n# Scatter plot\naxes[1].scatter(range(len(df)), df['SepalLengthCm'].sort_values())\naxes[1].set_title('Distribuci\u00f3n de valores')\naxes[1].set_ylabel('Sepal Length (ordenado)')\nplt.tight_layout()\nplt.savefig('./images/outliers_detection.png', dpi=300, bbox_inches='tight')\nplt.close()\n</code></pre> Output esperado:  <p></p> <p>\u00bfEliminar o transformar?</p> <ul> <li>Elimina si: Es claramente un error (ej: edad = -5 a\u00f1os)</li> <li>Transforma si: Es extremo pero leg\u00edtimo. Usa transformaciones (log, ra\u00edz cuadrada) para reducir su impacto sin perder informaci\u00f3n</li> </ul>"},{"location":"practicas/00-datos-y-visualizacion/#14-transformacion-de-variables","title":"1.4 Transformaci\u00f3n de Variables","text":"<p>Normalizaci\u00f3n vs Estandarizaci\u00f3n:</p> <p>Ambas escalan los datos, pero de formas diferentes:</p> T\u00e9cnica F\u00f3rmula Rango Cu\u00e1ndo usar Normalizaci\u00f3n (Min-Max) (x - min) / (max - min) [0, 1] Cuando sabes el rango y quieres limites fijos Estandarizaci\u00f3n (Z-score) (x - \u03bc) / \u03c3 T\u00edpicamente [-3, 3], sin l\u00edmites Por defecto para modelos lineales, SVM, NN Robust Scaling (x - Q2) / IQR - Cuando hay outliers (menos sensible) <p>\u00bfPor qu\u00e9 es cr\u00edtico?</p> <p>Algoritmos como regresi\u00f3n log\u00edstica, SVM y redes neuronales son sensibles a la escala. Una caracter\u00edstica con rango [0, 10000] domina sobre otra en [0, 1]. El modelo aprender\u00e1 m\u00e1s lentamente o converger\u00e1 a soluciones sub\u00f3ptimas.</p> <p>Ejemplo:</p> <pre><code>from sklearn.preprocessing import StandardScaler, MinMaxScaler\n# Valores originales\nprint(\"*\" * 40)\nprint(\"Valores originales:\")\nprint(f\"Min: {df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].min().values}\")\nprint(f\"Max: {df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].max().values}\")\nprint(\"*\" * 40)\n# Estandarizaci\u00f3n (recomendado para modelos lineales)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']])\n# Despu\u00e9s de estandarizar, cada feature tiene media=0 y std=1\nprint(f\"Media: {X_scaled.mean(axis=0)}\")  # ~0\nprint(f\"Std: {X_scaled.std(axis=0)}\")     # ~1\nprint(\"*\" * 40)\n# Normalizaci\u00f3n (alternativa)\nnormalizer = MinMaxScaler()\nX_normalized = normalizer.fit_transform(df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']])\n# Rango: [0, 1]\nprint(f\"Min: {X_normalized.min(axis=0)}\")\nprint(f\"Max: {X_normalized.max(axis=0)}\")\nprint(\"*\" * 40)\n</code></pre>"},{"location":"practicas/00-datos-y-visualizacion/#15-codificacion-de-variables-categoricas","title":"1.5 Codificaci\u00f3n de Variables Categ\u00f3ricas","text":"<p>El problema: Los modelos num\u00e9ricos no entienden \"setosa\", \"versicolor\", \"virginica\".</p> <p>Dos enfoques principales:</p> T\u00e9cnica Ejemplo Cu\u00e1ndo usar Label Encoding setosa \u2192 0, versicolor \u2192 1, virginica \u2192 2 Variables ordinales (peque\u00f1o &lt; medio &lt; grande) One-Hot Encoding setosa \u2192 [1,0,0], versicolor \u2192 [0,1,0], virginica \u2192 [0,0,1] Variables nominales (sin orden natural) <p>\u00bfPor qu\u00e9 importa la elecci\u00f3n?</p> <p>Con Label Encoding, le dices al modelo \"2 &gt; 1 &gt; 0\". Eso introduce un orden falso en variables sin orden natural, sesgando el aprendizaje.</p> <p>Ejemplo:</p> <pre><code>from sklearn.preprocessing import LabelEncoder\nimport pandas as pd\n# Label Encoding (solo si es ordinal)\nle = LabelEncoder()\ndf['species_encoded'] = le.fit_transform(df['Species'])\n# setosa \u2192 0, versicolor \u2192 1, virginica \u2192 2\n# One-Hot Encoding (para nominales - RECOMENDADO para Iris)\ndf_encoded = pd.get_dummies(df, columns=['Species'], drop_first=True)\n# Crea: species_versicolor, species_virginica (drop_first evita multicolinealidad)\nprint(df_encoded.tail())\n</code></pre> <p>Output: </p><pre><code>Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n145  146            6.7           3.0            5.2           2.3   \n146  147            6.3           2.5            5.0           1.9   \n147  148            6.5           3.0            5.2           2.0   \n148  149            6.2           3.4            5.4           2.3   \n149  150            5.9           3.0            5.1           1.8   \n\n     species_encoded  Species_encoded  Species_Iris-versicolor  \\\n145                2                2                    False   \n146                2                2                    False   \n147                2                2                    False   \n148                2                2                    False   \n149                2                2                    False   \n\n     Species_Iris-virginica  \n145                    True  \n146                    True  \n147                    True  \n148                    True  \n149                    True\n</code></pre><p></p> <p>One-Hot Encoding: drop_first=True</p> <p>Con 3 clases, necesitas solo 2 variables dummy. La tercera es redundante (si no es versicolor ni virginica, es setosa). Omitirla evita multicolinealidad perfecta.</p>"},{"location":"practicas/00-datos-y-visualizacion/#2-analisis-exploratorio-de-datos-eda","title":"2. An\u00e1lisis Exploratorio de Datos (EDA)","text":"<p>El EDA es donde empiezas a entender tus datos y a generar hip\u00f3tesis. Es detective work.</p>"},{"location":"practicas/00-datos-y-visualizacion/#21-estadistica-descriptiva","title":"2.1 Estad\u00edstica Descriptiva","text":"<p>\u00bfQu\u00e9 n\u00fameros resumidos nos dicen los datos?</p> <p></p><pre><code># Resumen r\u00e1pido\nprint(df.describe())\n</code></pre> Output para Iris: <pre><code>Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\ncount  150.000000     150.000000    150.000000     150.000000    150.000000\nmean    75.500000       5.843333      3.054000       3.758667      1.198667\nstd     43.445368       0.828066      0.433594       1.764420      0.763161\nmin      1.000000       4.300000      2.000000       1.000000      0.100000\n25%     38.250000       5.100000      2.800000       1.600000      0.300000\n50%     75.500000       5.800000      3.000000       4.350000      1.300000\n75%    112.750000       6.400000      3.300000       5.100000      1.800000\nmax    150.000000       7.900000      4.400000       6.900000      2.500000\n</code></pre><p></p> <p>\u00bfQu\u00e9 significa cada l\u00ednea?</p> <ul> <li>count: N\u00famero de valores no-nulos (verificar valores faltantes)</li> <li>mean: Promedio. Si es muy diferente de la mediana, hay sesgos</li> <li>std: Desviaci\u00f3n est\u00e1ndar. Mide dispersi\u00f3n. Alto = datos heterog\u00e9neos</li> <li>min/max: Rango. Busca valores imposibles o sospechosos</li> <li>25%, 50%, 75%: Cuartiles. Son robustos a outliers, a diferencia de media</li> </ul>"},{"location":"practicas/00-datos-y-visualizacion/#22-distribuciones-univariadas","title":"2.2 Distribuciones Univariadas","text":"<p>Un gr\u00e1fico vale m\u00e1s que 1000 n\u00fameros.</p> <p>Visualizar c\u00f3mo se distribuye cada variable te muestra: - Forma (normal, sesgada, bimodal) - Concentraci\u00f3n de valores - Presencia de outliers</p> <p></p><pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nfeatures = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\ncolors = ['skyblue', 'lightgreen', 'salmon', 'plum']\nfor idx, feature in enumerate(features):\nax = axes[idx // 2, idx % 2]\n# Histograma con KDE\nsns.histplot(data=df, x=feature, kde=True, bins=20, ax=ax, color=colors[idx])\nax.set_title(f'Distribuci\u00f3n de {feature}')\nax.set_xlabel('Valor')\nax.set_ylabel('Frecuencia')\nplt.tight_layout()\nplt.savefig('./images/distribuciones_univariadas.png', dpi=300, bbox_inches='tight')\nplt.close()\n</code></pre> Output esperado: <p></p> <ul> <li>Forma normal (campana): Indicador de datos bien comportados</li> <li>Sesgada (skewed): Cola larga en un lado. Petal length est\u00e1 sesgada a la izquierda (muchos flores peque\u00f1as)</li> <li>Bimodal: Dos picos. Sugiere dos subgrupos (en Iris: especies con flores grandes vs peque\u00f1as)</li> </ul>"},{"location":"practicas/00-datos-y-visualizacion/#23-box-plots","title":"2.3 Box Plots","text":"<p>Resumen r\u00e1pido de distribuci\u00f3n y outliers:</p> <pre><code>fig, axes = plt.subplots(1, 4, figsize=(14, 4))\nfor idx, feature in enumerate(features):\naxes[idx].boxplot(df[feature])\naxes[idx].set_title(feature)\naxes[idx].set_ylabel('Valor')\nplt.tight_layout()\nplt.savefig('images/boxplots.png', dpi=300, bbox_inches='tight')\nplt.close()\n</code></pre> <p>Output esperado: Figura 1: Box Plots</p> <p>Interpretaci\u00f3n: - L\u00ednea central: Mediana (Q2) - Caja: IQR (50% central de datos) - Bigotes: L\u00edmites (Q1 - 1.5\u00d7IQR hasta Q3 + 1.5\u00d7IQR) - Puntos: Outliers</p> <p>Para Iris, notar\u00e1s que petal_length y petal_width tienen distribuciones claramente bimodales \u2192 Diferentes especies tienen tama\u00f1os muy distintos.</p>"},{"location":"practicas/00-datos-y-visualizacion/#24-relaciones-bivariadas","title":"2.4 Relaciones Bivariadas","text":"<p>\u00bfC\u00f3mo se relacionan dos variables?</p> <p>La correlaci\u00f3n mide relaci\u00f3n lineal: - r = 1: Perfecta positiva (X aumenta \u2192 Y aumenta) - r = 0: Sin relaci\u00f3n lineal - r = -1: Perfecta negativa (X aumenta \u2192 Y disminuye)</p> <pre><code># Matriz de correlaci\u00f3n\ncorr_matrix = df[features].corr()\nprint(corr_matrix)\n# Visualizaci\u00f3n\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, \nsquare=True, ax=ax, cbar_kws={'label': 'Correlaci\u00f3n'})\nax.set_title('Matriz de Correlaci\u00f3n - Iris Features')\nplt.tight_layout()\nplt.savefig('./images/correlacion_matrix.png', dpi=300, bbox_inches='tight')\nplt.close()\n</code></pre> <p>Output esperado: </p><pre><code>SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\nSepalLengthCm       1.000000     -0.109369       0.871754      0.817954\nSepalWidthCm       -0.109369      1.000000      -0.420516     -0.356544\nPetalLengthCm       0.871754     -0.420516       1.000000      0.962757\nPetalWidthCm        0.817954     -0.356544       0.962757      1.000000\n</code></pre> <p></p> <p>Interpretaci\u00f3n: - Sepal_length vs petal_length: 0.87 \u2192 Fuerte relaci\u00f3n positiva (flores grandes en ambas medidas) - Sepal_width vs petal_length: -0.42 \u2192 Relaci\u00f3n negativa moderada (flores anchas tienden a ser peque\u00f1as en p\u00e9talos) - Petal_length vs petal_width: 0.96 \u2192 Casi perfecta (p\u00e9talos grandes implican anchos grandes)</p>"},{"location":"practicas/00-datos-y-visualizacion/#25-scatter-plots","title":"2.5 Scatter Plots","text":"<p>Para visualizar relaciones bivariadas directamente:</p> <p></p><pre><code>fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n# Scatter simple: sepal_length vs petal_length\naxes[0].scatter(df['sepal_length'], df['petal_length'], alpha=0.6)\naxes[0].set_xlabel('Sepal Length')\naxes[0].set_ylabel('Petal Length')\naxes[0].set_title('Correlaci\u00f3n: 0.87 (fuerte positiva)')\naxes[0].grid(True, alpha=0.3)\n# Scatter coloreado por species\nfor species in df['species'].unique():\nmask = df['species'] == species\naxes[1].scatter(df[mask]['sepal_length'], df[mask]['petal_length'], \nlabel=species, alpha=0.7)\naxes[1].set_xlabel('Sepal Length')\naxes[1].set_ylabel('Petal Length')\naxes[1].set_title('Por especie')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('./images/scatter_plots.png', dpi=300, bbox_inches='tight')\nplt.close()\n</code></pre> <p></p> <p>\u00bfPor qu\u00e9 colorear por species? Ves que Iris setosa (peque\u00f1a) se agrupa en esquina inferior-izquierda, mientras que Iris virginica (grande) en esquina superior-derecha. Las clases son linealmente separables \u2192 Un clasificador lineal funcionar\u00e1 bien.</p>"},{"location":"practicas/00-datos-y-visualizacion/#26-multicolinealidad","title":"2.6 Multicolinealidad","text":"<p>\u00bfPor qu\u00e9 importa?</p> <p>Multicolinealidad = Dos features muy correlacionadas. En regresi\u00f3n/clasificaci\u00f3n lineal, causa: - Coeficientes inestables (peque\u00f1os cambios en datos \u2192 grandes cambios en coeficientes) - Dificultad interpretando importancia de features - Overfitting potencial</p> <p>Detecci\u00f3n:</p> <pre><code># Mira correlaciones &gt; 0.9\nprint(corr_matrix[corr_matrix &gt; 0.9].sum())\n# En Iris: petal_length y petal_width est\u00e1n altamente correlacionadas (0.96)\n# Pero con 4 features, no es cr\u00edtico\n</code></pre> <p>Decisi\u00f3n: En Iris, no es problema (dataset peque\u00f1o, modelos simples). En datasets grandes, considera: - Eliminar una de las features correlacionadas - Usar regularizaci\u00f3n (L1/L2) - PCA (reducci\u00f3n de dimensionalidad)</p>"},{"location":"practicas/00-datos-y-visualizacion/#27-imbalance-y-representatividad","title":"2.7 Imbalance y Representatividad","text":"<p>\u00bfEst\u00e1n todas las clases bien representadas?</p> <pre><code># Conteo de clases\nprint(df['species'].value_counts())\n# Visualizaci\u00f3n\nfig, ax = plt.subplots(figsize=(8, 5))\ndf['species'].value_counts().plot(kind='bar', ax=ax, color=['blue', 'orange', 'green'])\nax.set_title('Distribuci\u00f3n de clases en Iris')\nax.set_xlabel('Especie')\nax.set_ylabel('Cantidad')\nax.set_xticklabels(ax.get_xticklabels(), rotation=0)\nplt.tight_layout()\nplt.savefig('./images/class_distribution.png', dpi=300, bbox_inches='tight')\nplt.close()\n</code></pre> <p>Output: </p><pre><code>species\nsetosa        50\nversicolor    50\nvirginica     50\n</code></pre><p></p> <p>Figura 2: Distribuci\u00f3n de Clases</p> <p>Interpretaci\u00f3n para Iris: Perfectamente balanceado (50 cada una). En la vida real, raro. Titanic, por ejemplo, tiene ~62% supervivientes, 38% no. En dataset muy desbalanceado (ej: detecci\u00f3n de fraude con 0.1% fraude), m\u00e9tricas como Accuracy enga\u00f1an.</p>"},{"location":"practicas/00-datos-y-visualizacion/#28-patrones-anomalias-e-insights","title":"2.8 Patrones, Anomal\u00edas e Insights","text":"<p>\u00daltimo paso del EDA: Storytelling.</p> <p>Preg\u00fantate: - \u00bfCu\u00e1l es la caracter\u00edstica m\u00e1s importante para predecir la clase? - \u00bfHay grupos naturales en los datos? - \u00bfQu\u00e9 anomal\u00edas notaste?</p> <p>Para Iris: - Petal_length es el mejor discriminante entre especies - Las clases son casi linealmente separables - No hay outliers obvios - Dataset es peque\u00f1o pero limpio</p> <p>Visualizaci\u00f3n final: PCA para contexto (opcional, solo visualizaci\u00f3n)</p> <p></p><pre><code>from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(df[features].values)\nfig, ax = plt.subplots(figsize=(10, 7))\nfor species in df['Species'].unique():\nmask = df['Species'] == species\nax.scatter(X_pca[mask, 0], X_pca[mask, 1], label=species, s=100, alpha=0.7)\nax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} varianza)')\nax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} varianza)')\nax.set_title('Proyecci\u00f3n PCA 2D - Iris Species')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('./images/pca_visualization.png', dpi=300, bbox_inches='tight')\nplt.close()\n</code></pre> <p></p>"},{"location":"practicas/00-datos-y-visualizacion/#3-train-test-split-y-estandarizacion","title":"3. Train-Test Split y Estandarizaci\u00f3n","text":""},{"location":"practicas/00-datos-y-visualizacion/#31-separacion-train-test","title":"3.1 Separaci\u00f3n Train-Test","text":"<p>\u00bfPor qu\u00e9 es cr\u00edtico?</p> <p>Entrenar y evaluar en los mismos datos es trampa. El modelo memoriza. La m\u00e9trica resultante no estima performance en datos nuevos (que es lo que importa).</p> <p>Analog\u00eda: Es como estudiante que memoriza las preguntas exactas de un examen pasado. Cuando llega el examen real con preguntas similares pero no id\u00e9nticas, se hunde.</p> <p></p><pre><code>from sklearn.model_selection import train_test_split\n# Split 80-20\nX = df[features].values\ny = df['species'].values\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.2, random_state=42, stratify=y\n)\nprint(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n# Train: (120, 4), Test: (30, 4)\n# stratify=y asegura que cada split tenga proporci\u00f3n similar de clases\nprint(f\"Train - setosa: {sum(y_train == 'setosa')}/120\")\nprint(f\"Test - setosa: {sum(y_test == 'setosa')}/30\")\n</code></pre> Output esperado: <pre><code>Train: (120, 4), Test: (30, 4)\nTrain - setosa: 0/120\nTest - setosa: 0/30\n</code></pre><p></p> <p>NUNCA hagas normalizaci\u00f3n/imputaci\u00f3n antes de splitear</p> <p>INCORRECTO: </p><pre><code>X_normalized = scaler.fit_transform(X)  # FIT EN TODO\nX_train, X_test, ... = train_test_split(X_normalized, y, ...)\n</code></pre><p></p> <p>CORRECTO: </p><pre><code>X_train, X_test, ... = train_test_split(X, y, ...)\nscaler.fit(X_train)  # FIT solo en train\nX_train_norm = scaler.transform(X_train)\nX_test_norm = scaler.transform(X_test)\n</code></pre><p></p>"},{"location":"practicas/00-datos-y-visualizacion/#32-estandarizacion-del-conjunto-de-entrenamiento","title":"3.2 Estandarizaci\u00f3n del Conjunto de Entrenamiento","text":"<p>Recuerda: La estandarizaci\u00f3n es cr\u00edtica para modelos lineales (logistic regression, SVM, etc.).</p> <pre><code>from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)  # FIT SOLO en train\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n# Verificar\nprint(f\"Train mean: {X_train_scaled.mean(axis=0)}\")  # ~0\nprint(f\"Train std: {X_train_scaled.std(axis=0)}\")    # ~1\nprint(f\"Test mean: {X_test_scaled.mean(axis=0)}\")    # No es ~0 (datos nuevos)\n</code></pre> <p>Output esperado: </p><pre><code>Train mean: [-1.20829273e-15 -2.53315887e-15  1.48029737e-16  1.55246186e-15]\nTrain std: [1. 1. 1. 1.]\nTest mean: [ 0.00995126  0.11078352 -0.03456365 -0.03615399]\n</code></pre><p></p> <p>\u00bfPor qu\u00e9 test no tiene mean=0?</p> <p>El scaler fue ajustado con estad\u00edsticas del train. Al aplicarlo al test (datos nuevos), el test conserva su propia distribuci\u00f3n. Esto es correcto: validamos con datos en su distribuci\u00f3n real.</p>"},{"location":"practicas/00-datos-y-visualizacion/#33-modelo-regresion-logistica","title":"3.3 Modelo: Regresi\u00f3n Log\u00edstica","text":"<p>\u00bfQu\u00e9 es?</p> <p>A pesar del nombre, es un clasificador, no regresi\u00f3n. Predice probabilidades de pertenencia a clases.</p> <p>Conceptualmente: - Para 2 clases: P(Y=1) = sigmoid(w\u2080 + w\u2081X\u2081 + w\u2082X\u2082 + ...) - Para 3+ clases (como Iris): Softmax (extensi\u00f3n de sigmoid)</p> <p>Interpretaci\u00f3n: - Coeficientes positivos: Aumentan probabilidad de la clase - Coeficientes negativos: Disminuyen probabilidad</p> <pre><code>from sklearn.linear_model import LogisticRegression\n# Crear modelo\nmodel = LogisticRegression(\nmax_iter=1000,\nrandom_state=42,\nmulti_class='multinomial'  # Para 3+ clases\n)\n# Entrenar\nmodel.fit(X_train_scaled, y_train)\n# Predicciones\ny_pred_train = model.predict(X_train_scaled)\ny_pred_test = model.predict(X_test_scaled)\n# Probabilidades\ny_proba_test = model.predict_proba(X_test_scaled)\nprint(y_proba_test[:5])  # [prob_setosa, prob_versicolor, prob_virginica]\n</code></pre> <p>Output esperado (probabilidades): </p><pre><code>Probabilidades de las primeras 5 muestras del test:\n[\n    [9.79355498e-01 2.06441434e-02 3.58223033e-07]\n    [3.77309225e-03 3.69498929e-01 6.26727979e-01]\n    [1.49647017e-01 8.41581277e-01 8.77170654e-03]\n    [9.58890418e-02 8.94120385e-01 9.99057342e-03]\n    [9.88807815e-01 1.11920151e-02 1.69552410e-07]\n]\n</code></pre><p></p>"},{"location":"practicas/00-datos-y-visualizacion/#34-metricas-de-clasificacion","title":"3.4 M\u00e9tricas de Clasificaci\u00f3n","text":"<p>\u26a0\ufe0f ATENCI\u00d3N</p> <p>Las m\u00e9tricas que ves a continuaci\u00f3n son para CLASIFICACI\u00d3N.</p> <p>Para REGRESI\u00d3N, usar\u00edas: - MSE (Mean Squared Error): Promedio de (y_true - y_pred)\u00b2 - RMSE (Root MSE): \u221aMSE (misma escala que y) - MAE (Mean Absolute Error): Promedio de |y_true - y_pred|</p> <p>Clasificaci\u00f3n - M\u00e9tricas principales:</p> <pre><code>from sklearn.metrics import (\naccuracy_score, precision_score, recall_score, f1_score, \nconfusion_matrix, classification_report\n)\n# Accuracy: % correcto\nacc = accuracy_score(y_test, y_pred_test)\nprint(f\"Accuracy: {acc:.4f}\")  # 1.0 (perfecto en este dataset)\n# Precision (para clase i): TP / (TP + FP) - \"de lo que predije positivo, cu\u00e1nto fue correcto\"\n# Recall (para clase i): TP / (TP + FN) - \"de los verdaderos positivos, cu\u00e1ntos encontr\u00e9\"\n#\n#  F1: Media arm\u00f3nica de Precision y Recall - \"balance entre ambos\"\nprint(classification_report(y_test, y_pred_test))\n</code></pre> <p>Output t\u00edpico: </p><pre><code>              Accuracy: 0.9333\n                 precision    recall  f1-score   support\n\n    Iris-setosa       1.00      1.00      1.00        10\nIris-versicolor       0.90      0.90      0.90        10\n Iris-virginica       0.90      0.90      0.90        10\n\n       accuracy                           0.93        30\n      macro avg       0.93      0.93      0.93        30\n   weighted avg       0.93      0.93      0.93        30\n</code></pre><p></p> <p>Matriz de confusi\u00f3n:</p> <p></p><pre><code>cm = confusion_matrix(y_test, y_pred_test)\nprint(cm)\n# Visualizaci\u00f3n\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\nxticklabels=model.classes_, yticklabels=model.classes_)\nax.set_xlabel('Predicho')\nax.set_ylabel('Real')\nax.set_title('Matriz de Confusi\u00f3n')\nplt.tight_layout()\nplt.savefig('./images/confusion_matrix.png', dpi=300, bbox_inches='tight')\nplt.close()\n</code></pre> <p></p> <p>Interpretaci\u00f3n: - Diagonal principal: Aciertos - Fuera de diagonal: Errores</p>"},{"location":"practicas/00-datos-y-visualizacion/#35-validacion-cruzada","title":"3.5 Validaci\u00f3n Cruzada","text":"<p>Problema: Un \u00fanico split train-test puede ser afortunado (test f\u00e1cil) o desafortunado (test dif\u00edcil).</p> <p>Soluci\u00f3n: Validaci\u00f3n cruzada (k-fold)</p> <p>Divide datos en k folds. Entrena k modelos, cada uno dejando un fold para validar:</p> <pre><code>from sklearn.model_selection import cross_validate\n# 5-fold cross-validation\ncv_results = cross_validate(\nmodel,\nX_train_scaled, y_train,\ncv=5,\nscoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n)\nprint(f\"CV Accuracy: {cv_results['test_accuracy'].mean():.4f} \u00b1 {cv_results['test_accuracy'].std():.4f}\")\nprint(f\"CV F1: {cv_results['test_f1_weighted'].mean():.4f} \u00b1 {cv_results['test_f1_weighted'].std():.4f}\")\n</code></pre> <p>Output: </p><pre><code>CV Accuracy: 0.9583 \u00b1 0.0264\nCV F1: 0.9580 \u00b1 0.0268\n</code></pre><p></p> <p>\u00bfQu\u00e9 significa?</p> <p>Media 95.83% \u00b1 2.64% \u2192 En promedio, 95.83%, pero var\u00eda \u00b12.68% entre folds. Desviaci\u00f3n baja = modelo estable.</p>"},{"location":"practicas/00-datos-y-visualizacion/#4-pipelines-en-machine-learning","title":"4. Pipelines en Machine Learning","text":"<p>Los pipelines encadenan m\u00faltiples transformaciones y modelos en un \u00fanico objeto reutilizable y reproducible. Es la forma profesional de hacer ML.</p>"},{"location":"practicas/00-datos-y-visualizacion/#41-por-que-pipelines","title":"4.1 \u00bfPor qu\u00e9 Pipelines?","text":"<p>Sin pipeline (Fr\u00e1gil):</p> <pre><code>scaler = StandardScaler()\nX_train_s = scaler.fit_transform(X_train)\nX_test_s = scaler.transform(X_test)\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train_s, y_train)\ny_pred = model.predict(X_test_s)\n</code></pre> <p>Problemas: - F\u00e1cil olvidar pasos - Data leakage accidental - C\u00f3digo repetitivo - Dif\u00edcil de reproducir</p> <p>Con pipeline (Profesional):</p> <pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\npipe = Pipeline([\n('scaler', StandardScaler()),\n('model', LogisticRegression(max_iter=1000, random_state=42))\n])\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\n</code></pre> <p>Ventajas: - C\u00f3digo limpio y legible - Imposible data leakage - F\u00e1cil cambiar componentes - Funciona autom\u00e1ticamente con validaci\u00f3n cruzada y grid search - Production-ready</p>"},{"location":"practicas/00-datos-y-visualizacion/#42-pipeline-basico","title":"4.2 Pipeline B\u00e1sico","text":"<pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score\n# Crear pipeline\npipe = Pipeline([\n('scaler', StandardScaler()),\n('logistic', LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial'))\n])\n# Entrenar\npipe.fit(X_train, y_train)\n# Predecir\ny_pred = pipe.predict(X_test)\ny_proba = pipe.predict_proba(X_test)\n# Evaluar\nacc = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred, average='weighted')\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1: {f1:.4f}\")\n</code></pre>"},{"location":"practicas/00-datos-y-visualizacion/#43-columntransformer-para-multiples-tipos-de-datos","title":"4.3 ColumnTransformer (para m\u00faltiples tipos de datos)","text":"<p>Aunque Iris solo tiene num\u00e9ricas, ense\u00f1amos el patr\u00f3n completo:</p> <pre><code>from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n# Definir tipos de features\nnumeric_features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n# Transformer para num\u00e9ricas\nnumeric_transformer = Pipeline([\n('scaler', StandardScaler())\n])\n# Combinar transformers\npreprocessor = ColumnTransformer([\n('num', numeric_transformer, numeric_features)\n])\n# Pipeline completo\nfull_pipeline = Pipeline([\n('preprocessor', preprocessor),\n('model', LogisticRegression(max_iter=1000, random_state=42))\n])\n# Usar igual que antes\nfull_pipeline.fit(X_train, y_train)\ny_pred = full_pipeline.predict(X_test)\n</code></pre>"},{"location":"practicas/00-datos-y-visualizacion/#44-pipeline-con-gridsearchcv","title":"4.4 Pipeline con GridSearchCV","text":"<p>Tunear autom\u00e1ticamente TODO:</p> <pre><code>from sklearn.model_selection import GridSearchCV\npipe = Pipeline([\n('scaler', StandardScaler()),\n('model', LogisticRegression(random_state=42))\n])\n# Par\u00e1metros a tunear\nparam_grid = {\n'model__C': [0.01, 0.1, 1, 10],\n'model__max_iter': [1000, 2000]\n}\n# Grid search\ngrid = GridSearchCV(pipe, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1)\ngrid.fit(X_train, y_train)\nprint(f\"Mejor C: {grid.best_params_['model__C']}\")\nprint(f\"Mejor max_iter: {grid.best_params_['model__max_iter']}\")\nprint(f\"Mejor score CV: {grid.best_score_:.4f}\")\n# Predecir con mejor modelo\ny_pred = grid.predict(X_test)\n</code></pre>"},{"location":"practicas/00-datos-y-visualizacion/#45-ventajas-de-pipelines-tabla-resumen","title":"4.5 Ventajas de Pipelines - Tabla Resumen","text":"Aspecto Sin Pipeline Con Pipeline Data leakage F\u00e1cil olvidarse Imposible Reproducibilidad Fr\u00e1gil Robusta C\u00f3digo Repetitivo Limpio Cambiar componentes Tedioso Una l\u00ednea CV/GridSearch Manual Autom\u00e1tico Producci\u00f3n Fr\u00e1gil Ready"},{"location":"practicas/00-datos-y-visualizacion/#5-tareas-para-casa","title":"5. Tareas para Casa","text":""},{"location":"practicas/00-datos-y-visualizacion/#objetivo","title":"Objetivo","text":"<p>Replicar el flujo completo de ML en un dataset de tu elecci\u00f3n.</p>"},{"location":"practicas/00-datos-y-visualizacion/#opciones-de-dataset","title":"Opciones de Dataset","text":"<p>Opci\u00f3n 1: Titanic (Recomendado para empezar) - 890 registros, 11 features - Objetivo: Supervivencia (binaria) - Desaf\u00edo: Valores faltantes, mezcla de tipos - Descarga: https://www.kaggle.com/c/titanic/data</p> <p>Opci\u00f3n 2: Wine Quality - 1,600 registros, 12 features - Objetivo: Calidad (0-10) - Descarga: https://www.kaggle.com/datasets/yasserh/wine-quality-dataset</p> <p>Opci\u00f3n 3: Adult Income - 32,500 registros, 14 features - Objetivo: Ingresos &gt;50K (binaria) - Descarga: https://archive.ics.uci.edu/ml/datasets/Adult</p> <p>Opci\u00f3n 4: Tu propio dataset - M\u00ednimo 2,000-4,000 registros - M\u00ednimo 10+ columnas - Mezcla de num\u00e9ricas y categ\u00f3ricas - Variable objetivo clara</p>"},{"location":"practicas/00-datos-y-visualizacion/#estructura-de-entrega","title":"Estructura de Entrega","text":"<p>MEMOR\u00cdA EN LATEX (Overleaf)</p>"},{"location":"practicas/01-aprendizaje-supervisado/","title":"1. Aprendizaje supervisado","text":""},{"location":"practicas/01-aprendizaje-supervisado/#practica-1-aprendizaje-supervisado","title":"Pr\u00e1ctica 1: Aprendizaje Supervisado","text":""},{"location":"practicas/01-aprendizaje-supervisado/#objetivo","title":"Objetivo","text":"<p>Aplicar los principales algoritmos de aprendizaje supervisado vistos en teor\u00eda sobre un mismo problema de clasificaci\u00f3n, comparando su rendimiento y analizando el efecto de sus hiperpar\u00e1metros.</p> <p>Se asume que ya dominas el preprocesamiento b\u00e1sico (tratamiento de valores faltantes, escalado, encoding y divisi\u00f3n train/test) visto en la Pr\u00e1ctica 0. Todo el preprocesamiento se da por supuesto y no es necesario detallarlo de nuevo en el informe.</p>"},{"location":"practicas/01-aprendizaje-supervisado/#dataset","title":"Dataset","text":"<p>Puedes elegir:</p> <ul> <li>Opci\u00f3n A: Reutilizar el dataset de la Pr\u00e1ctica 0 (recomendado para comparar directamente con los resultados anteriores).</li> <li>Opci\u00f3n B: Seleccionar un nuevo dataset de clasificaci\u00f3n con los mismos requerimientos que la Pr\u00e1ctica 0.</li> </ul> <p>El mismo dataset debe usarse en todas las partes de la pr\u00e1ctica.</p>"},{"location":"practicas/01-aprendizaje-supervisado/#parte-1-support-vector-machines","title":"Parte 1: Support Vector Machines","text":""},{"location":"practicas/01-aprendizaje-supervisado/#11-svm-con-diferentes-kernels","title":"1.1 SVM con Diferentes Kernels","text":"<p>Entrena modelos SVM con los cuatro kernels principales y compara su rendimiento con los hiperpar\u00e1metros por defecto.</p> <pre><code>from sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n# IMPORTANTE: SVM requiere escalado obligatorio\nkernels = {\n'Linear':     SVC(kernel='linear',  random_state=42),\n'Poly (d=3)': SVC(kernel='poly',    degree=3, random_state=42),\n'RBF':        SVC(kernel='rbf',     random_state=42),\n'Sigmoid':    SVC(kernel='sigmoid', random_state=42)\n}\nfor name, svc in kernels.items():\npipe = Pipeline([('scaler', StandardScaler()), ('svm', svc)])\npipe.fit(X_train, y_train)\nscores = cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy')\nn_sv = pipe.named_steps['svm'].n_support_\nprint(f\"{name}: CV={scores.mean():.4f} (+/-{scores.std():.4f}), \"\nf\"SV={n_sv.sum()} ({n_sv.sum()/len(X_train)*100:.1f}%)\")\n</code></pre> <p>Reporta en una tabla:</p> Kernel CV Accuracy Test Accuracy Precision Recall F1-Score N\u00b0 SV % Training Linear ... ... ... ... ... ... ... Poly (d=3) ... ... ... ... ... ... ... RBF ... ... ... ... ... ... ... Sigmoid ... ... ... ... ... ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#12-optimizacion-del-kernel-rbf","title":"1.2 Optimizaci\u00f3n del Kernel RBF","text":"<p>El kernel RBF suele ser el m\u00e1s efectivo. Explora el efecto de los hiperpar\u00e1metros <code>C</code> y <code>gamma</code>.</p> <ul> <li>C: controla el trade-off entre maximizar el margen y permitir errores. C alto \u2192 margen peque\u00f1o, menos errores (riesgo de overfitting). C bajo \u2192 margen grande, m\u00e1s errores permitidos (riesgo de underfitting).</li> <li>gamma: controla la influencia de cada muestra en la frontera. gamma alto \u2192 influencia local, frontera compleja. gamma bajo \u2192 influencia amplia, frontera suave.</li> </ul> <pre><code>from sklearn.model_selection import GridSearchCV\npipe_rbf = Pipeline([\n('scaler', StandardScaler()),\n('svm', SVC(kernel='rbf', random_state=42))\n])\nparam_grid = {\n'svm__C':     [0.1, 1, 10, 100],\n'svm__gamma': [0.001, 0.01, 0.1, 1, 'scale', 'auto']\n}\ngrid_rbf = GridSearchCV(pipe_rbf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\ngrid_rbf.fit(X_train, y_train)\nprint(f\"Mejores par\u00e1metros: {grid_rbf.best_params_}\")\nprint(f\"Mejor CV score:     {grid_rbf.best_score_:.4f}\")\nprint(f\"Test score:         {grid_rbf.best_estimator_.score(X_test, y_test):.4f}\")\n</code></pre> <p>Reporta en una tabla (selecciona al menos 10 combinaciones representativas):</p> C gamma CV Accuracy Test Accuracy N\u00b0 SV % Training Tiempo (s) 0.1 0.001 ... ... ... ... ... 1 scale ... ... ... ... ... 10 0.1 ... ... ... ... ... 100 1 ... ... ... ... ... ... ... ... ... ... ... ... Mejor ... ... ... ... ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#13-optimizacion-del-kernel-polinomial","title":"1.3 Optimizaci\u00f3n del Kernel Polinomial","text":"<pre><code>pipe_poly = Pipeline([\n('scaler', StandardScaler()),\n('svm', SVC(kernel='poly', random_state=42))\n])\nparam_grid = {\n'svm__degree': [2, 3, 4],\n'svm__C':      [0.1, 1, 10],\n'svm__gamma':  ['scale', 'auto', 0.1]\n}\ngrid_poly = GridSearchCV(pipe_poly, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\ngrid_poly.fit(X_train, y_train)\n</code></pre> <p>Reporta en una tabla:</p> degree C gamma CV Accuracy Test Accuracy N\u00b0 SV 2 1 scale ... ... ... 3 1 scale ... ... ... 4 1 scale ... ... ... ... ... ... ... ... ... Mejor ... ... ... ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#14-comparacion-de-implementaciones-kernel-lineal","title":"1.4 Comparaci\u00f3n de Implementaciones (Kernel Lineal)","text":"<p>Scikit-learn ofrece tres implementaciones de SVM lineal con distinta formulaci\u00f3n y coste computacional:</p> Implementaci\u00f3n Formulaci\u00f3n Cu\u00e1ndo usar <code>SVC(kernel='linear')</code> Dual (libsvm) n_samples &lt; 10,000 <code>LinearSVC</code> Primal (liblinear) Kernel lineal, datasets medianos <code>SGDClassifier(loss='hinge')</code> SGD con hinge loss Datasets grandes (&gt; 100,000) <pre><code>from sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom time import time\nmodels = {\n'SVC(linear)':   Pipeline([('scaler', StandardScaler()),\n('svm', SVC(kernel='linear', random_state=42))]),\n'LinearSVC':     Pipeline([('scaler', StandardScaler()),\n('svm', LinearSVC(random_state=42, max_iter=10000))]),\n'SGDClassifier': Pipeline([('scaler', StandardScaler()),\n('svm', SGDClassifier(loss='hinge', random_state=42))])\n}\nfor name, model in models.items():\nt0 = time()\nmodel.fit(X_train, y_train)\nt1 = time()\ncv = cross_val_score(model, X_train, y_train, cv=5)\nprint(f\"{name}: CV={cv.mean():.4f}, Test={model.score(X_test, y_test):.4f}, \"\nf\"Tiempo={t1-t0:.3f}s\")\n</code></pre> <p>Reporta en una tabla:</p> Implementaci\u00f3n CV Accuracy Test Accuracy N\u00b0 SV Tiempo (s) SVC(linear) ... ... ... ... LinearSVC ... ... N/A ... SGDClassifier ... ... N/A ..."},{"location":"practicas/01-aprendizaje-supervisado/#15-analisis-de-vectores-de-soporte","title":"1.5 An\u00e1lisis de Vectores de Soporte","text":"<pre><code>model_svc = SVC(kernel='rbf', C=..., gamma=..., random_state=42)\nmodel_svc.fit(X_train_scaled, y_train)\nprint(f\"Total de vectores de soporte: {model_svc.n_support_.sum()}\")\nprint(f\"Por clase:                    {model_svc.n_support_}\")\nprint(f\"Porcentaje del training set:  {model_svc.n_support_.sum()/len(X_train)*100:.2f}%\")\n</code></pre> <p>Reporta en una tabla comparando distintas configuraciones:</p> Configuraci\u00f3n N\u00b0 SV % Training Set Linear (C=1) ... ... RBF (C=1, \u03b3=scale) ... ... RBF (C=10, \u03b3=0.1) ... ... RBF (C=100, \u03b3=1) ... ... Poly (d=3, C=1) ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#16-evaluacion-final-parte-1","title":"1.6 Evaluaci\u00f3n Final (Parte 1)","text":"<pre><code>from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nbest_svm = grid_rbf.best_estimator_\nbest_svm.fit(X_train, y_train)\ny_pred = best_svm.predict(X_test)\nprint(classification_report(y_test, y_pred))\ncm = confusion_matrix(y_test, y_pred)\nConfusionMatrixDisplay(confusion_matrix=cm).plot()\nplt.savefig('confusion_matrix_svm.png', dpi=300, bbox_inches='tight')\n</code></pre> <p>Reporta en una tabla resumen de la Parte 1:</p> Modelo CV Accuracy Test Accuracy F1-Score N\u00b0 SV % Training SVM Linear (mejor C) ... ... ... ... ... SVM Poly (mejor config) ... ... ... ... ... SVM RBF (mejor config) ... ... ... ... ... SVM Sigmoid (mejor config) ... ... ... ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#parte-2-arboles-de-decision","title":"Parte 2: \u00c1rboles de Decisi\u00f3n","text":""},{"location":"practicas/01-aprendizaje-supervisado/#21-arbol-sin-restricciones-baseline","title":"2.1 \u00c1rbol sin Restricciones (Baseline)","text":"<p>Entrena un \u00e1rbol sin limitar su crecimiento para observar el comportamiento de overfitting.</p> <pre><code>from sklearn.tree import DecisionTreeClassifier\ntree_full = DecisionTreeClassifier(random_state=42)\ntree_full.fit(X_train, y_train)\nprint(f\"Train accuracy: {tree_full.score(X_train, y_train):.4f}\")\nprint(f\"Test  accuracy: {tree_full.score(X_test,  y_test):.4f}\")\nprint(f\"Profundidad:    {tree_full.get_depth()}\")\nprint(f\"N\u00b0 hojas:       {tree_full.get_n_leaves()}\")\n</code></pre> <p>Reporta en una tabla:</p> Configuraci\u00f3n Train Accuracy Test Accuracy Profundidad N\u00b0 Hojas Sin restricciones ... ... ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#22-efecto-de-la-profundidad-maxima","title":"2.2 Efecto de la Profundidad M\u00e1xima","text":"<pre><code>depths = [1, 2, 3, 5, 7, 10, 15, None]\nfor depth in depths:\ntree = DecisionTreeClassifier(max_depth=depth, random_state=42)\ncv   = cross_val_score(tree, X_train, y_train, cv=5)\ntree.fit(X_train, y_train)\nprint(f\"max_depth={str(depth):&gt;4}: train={tree.score(X_train, y_train):.4f}, \"\nf\"cv={cv.mean():.4f}, test={tree.score(X_test, y_test):.4f}, \"\nf\"hojas={tree.get_n_leaves()}\")\n</code></pre> <p>Reporta en una tabla:</p> max_depth Train Acc CV Acc Test Acc Profundidad real N\u00b0 Hojas 1 ... ... ... ... ... 2 ... ... ... ... ... 3 ... ... ... ... ... 5 ... ... ... ... ... 10 ... ... ... ... ... None ... ... ... ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#23-criterios-de-division","title":"2.3 Criterios de Divisi\u00f3n","text":"<p>Compara los criterios de impureza disponibles. Para clasificaci\u00f3n: Gini (usado en CART), entrop\u00eda (usado en ID3/C4.5) y log_loss.</p> <pre><code>criterios = ['gini', 'entropy', 'log_loss']\nfor criterio in criterios:\ntree = DecisionTreeClassifier(criterion=criterio, random_state=42)\ncv   = cross_val_score(tree, X_train, y_train, cv=5)\ntree.fit(X_train, y_train)\nprint(f\"{criterio}: CV={cv.mean():.4f} (+/-{cv.std():.4f}), \"\nf\"Test={tree.score(X_test, y_test):.4f}, hojas={tree.get_n_leaves()}\")\n</code></pre> <p>Reporta en una tabla:</p> Criterio CV Accuracy Test Accuracy N\u00b0 Hojas Gini ... ... ... Entropy ... ... ... Log Loss ... ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#24-poda-previa-grid-search-de-hiperparametros","title":"2.4 Poda Previa: Grid Search de Hiperpar\u00e1metros","text":"<p>Adem\u00e1s de <code>max_depth</code>, explora el efecto del resto de par\u00e1metros de poda previa.</p> <pre><code>param_grid = {\n'max_depth':         [3, 5, 10, None],\n'min_samples_split': [2, 5, 10, 20],\n'min_samples_leaf':  [1, 2, 5, 10],\n'max_leaf_nodes':    [None, 10, 20, 50]\n}\ngrid_tree = GridSearchCV(\nDecisionTreeClassifier(random_state=42),\nparam_grid,\ncv=5,\nscoring='accuracy',\nn_jobs=-1\n)\ngrid_tree.fit(X_train, y_train)\nprint(f\"Mejores par\u00e1metros: {grid_tree.best_params_}\")\nprint(f\"Mejor CV score:     {grid_tree.best_score_:.4f}\")\nprint(f\"Test score:         {grid_tree.best_estimator_.score(X_test, y_test):.4f}\")\n</code></pre> <p>Reporta en una tabla con las mejores configuraciones encontradas:</p> max_depth min_samples_split min_samples_leaf max_leaf_nodes CV Acc Test Acc ... ... ... ... ... ... Mejor ... ... ... ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#25-poda-posterior-cost-complexity-pruning","title":"2.5 Poda Posterior: Cost-Complexity Pruning","text":"<p>CART implementa poda posterior mediante <code>ccp_alpha</code>. A mayor valor de alpha, mayor poda.</p> <pre><code># Obtener la secuencia de alphas generada por CART\npath = DecisionTreeClassifier(random_state=42).cost_complexity_pruning_path(X_train, y_train)\nccp_alphas = path.ccp_alphas[:-1]  # El \u00faltimo genera solo el nodo ra\u00edz\nfor alpha in ccp_alphas:\ntree = DecisionTreeClassifier(ccp_alpha=alpha, random_state=42)\ncv   = cross_val_score(tree, X_train, y_train, cv=5)\ntree.fit(X_train, y_train)\nprint(f\"alpha={alpha:.5f}: CV={cv.mean():.4f}, \"\nf\"Test={tree.score(X_test, y_test):.4f}, hojas={tree.get_n_leaves()}\")\n</code></pre> <p>Reporta en una tabla (selecciona valores representativos del rango):</p> ccp_alpha CV Accuracy Test Accuracy Profundidad N\u00b0 Hojas 0.0 (sin poda) ... ... ... ... ... ... ... ... ... Mejor ... ... ... ... alpha grande ... ... ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#26-importancia-de-caracteristicas","title":"2.6 Importancia de Caracter\u00edsticas","text":"<pre><code>best_tree = grid_tree.best_estimator_\nimportances  = best_tree.feature_importances_\nfeature_names = X.columns.tolist()\nindices = importances.argsort()[::-1]\nfor i, idx in enumerate(indices):\nprint(f\"  {i+1}. {feature_names[idx]}: {importances[idx]:.4f}\")\n</code></pre> <p>Reporta en una tabla:</p> Ranking Feature Importancia (Gini) 1 ... ... 2 ... ... ... ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#27-visualizacion-del-arbol","title":"2.7 Visualizaci\u00f3n del \u00c1rbol","text":"<pre><code>from sklearn.tree import plot_tree, export_text\nplt.figure(figsize=(20, 10))\nplot_tree(best_tree, feature_names=feature_names, class_names=class_names,\nfilled=True, rounded=True, fontsize=10, max_depth=4)\nplt.savefig('decision_tree.png', dpi=300, bbox_inches='tight')\n# Representaci\u00f3n en texto\nprint(export_text(best_tree, feature_names=feature_names, max_depth=4))\n</code></pre>"},{"location":"practicas/01-aprendizaje-supervisado/#28-evaluacion-final-parte-2","title":"2.8 Evaluaci\u00f3n Final (Parte 2)","text":"<p>Reporta en una tabla comparativa:</p> Modelo CV Accuracy Test Accuracy F1-Score Profundidad N\u00b0 Hojas Sin restricciones ... ... ... ... ... Mejor poda previa ... ... ... ... ... Mejor poda posterior ... ... ... ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#parte-3-random-forest","title":"Parte 3: Random Forest","text":""},{"location":"practicas/01-aprendizaje-supervisado/#31-random-forest-con-configuracion-por-defecto","title":"3.1 Random Forest con Configuraci\u00f3n por Defecto","text":"<pre><code>from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, oob_score=True)\nrf.fit(X_train, y_train)\nprint(f\"OOB Score:  {rf.oob_score_:.4f}\")\nprint(f\"Test Score: {rf.score(X_test, y_test):.4f}\")\n</code></pre> <p>Reporta en una tabla:</p> Configuraci\u00f3n OOB Score Test Accuracy F1-Score RF por defecto (n=100) ... ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#32-efecto-del-numero-de-estimadores","title":"3.2 Efecto del N\u00famero de Estimadores","text":"<pre><code>for n in [10, 25, 50, 100, 200, 500]:\nrf = RandomForestClassifier(n_estimators=n, random_state=42, n_jobs=-1, oob_score=True)\nrf.fit(X_train, y_train)\nprint(f\"n_estimators={n:&gt;3}: OOB={rf.oob_score_:.4f}, \"\nf\"Test={rf.score(X_test, y_test):.4f}\")\n</code></pre> <p>Reporta en una tabla:</p> n_estimators OOB Score Test Accuracy Tiempo (s) 10 ... ... ... 25 ... ... ... 50 ... ... ... 100 ... ... ... 200 ... ... ... 500 ... ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#33-efecto-de-max_features","title":"3.3 Efecto de max_features","text":"<p><code>max_features</code> controla cu\u00e1ntas features se consideran en cada split. Reduce la correlaci\u00f3n entre \u00e1rboles, aumentando la diversidad del ensemble.</p> <pre><code>for mf in ['sqrt', 'log2', 0.5, None]:\nrf = RandomForestClassifier(n_estimators=100, max_features=mf,\nrandom_state=42, n_jobs=-1, oob_score=True)\nrf.fit(X_train, y_train)\nprint(f\"max_features={str(mf):&gt;6}: OOB={rf.oob_score_:.4f}, \"\nf\"Test={rf.score(X_test, y_test):.4f}\")\n</code></pre> <p>Reporta en una tabla:</p> max_features OOB Score Test Accuracy sqrt ... ... log2 ... ... 0.5 ... ... None (todas) ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#34-grid-search","title":"3.4 Grid Search","text":"<pre><code>param_grid = {\n'n_estimators':     [100, 200],\n'max_features':     ['sqrt', 'log2', None],\n'max_depth':        [None, 10, 20],\n'min_samples_leaf': [1, 2, 5]\n}\ngrid_rf = GridSearchCV(\nRandomForestClassifier(random_state=42, n_jobs=-1),\nparam_grid,\ncv=5,\nscoring='accuracy',\nn_jobs=-1\n)\ngrid_rf.fit(X_train, y_train)\nprint(f\"Mejores par\u00e1metros: {grid_rf.best_params_}\")\nprint(f\"Mejor CV score:     {grid_rf.best_score_:.4f}\")\nprint(f\"Test score:         {grid_rf.best_estimator_.score(X_test, y_test):.4f}\")\n</code></pre>"},{"location":"practicas/01-aprendizaje-supervisado/#35-importancia-de-caracteristicas","title":"3.5 Importancia de Caracter\u00edsticas","text":"<pre><code>import numpy as np\nfrom sklearn.inspection import permutation_importance\nbest_rf = grid_rf.best_estimator_\n# Gini Importance (MDI)\nimportances_gini = best_rf.feature_importances_\n# Permutation Importance (m\u00e1s fiable con features correlacionadas)\nperm = permutation_importance(best_rf, X_test, y_test, n_repeats=10, random_state=42)\nindices = importances_gini.argsort()[::-1]\nfor i, idx in enumerate(indices):\nprint(f\"  {i+1}. {feature_names[idx]}: \"\nf\"Gini={importances_gini[idx]:.4f}, \"\nf\"Perm={perm.importances_mean[idx]:.4f} (+/-{perm.importances_std[idx]:.4f})\")\n</code></pre> <p>Reporta en una tabla:</p> Ranking Feature Gini Importance Permutation Importance 1 ... ... ... 2 ... ... ... ... ... ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#36-extra-trees","title":"3.6 Extra Trees","text":"<p>Extra Trees introduce a\u00fan m\u00e1s aleatoriedad: en lugar de buscar el mejor split, lo elige de forma aleatoria entre las features seleccionadas.</p> <pre><code>from sklearn.ensemble import ExtraTreesClassifier\net = ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1)\net.fit(X_train, y_train)\ncv_et = cross_val_score(et, X_train, y_train, cv=5)\nprint(f\"Extra Trees: CV={cv_et.mean():.4f} (+/-{cv_et.std():.4f}), \"\nf\"Test={et.score(X_test, y_test):.4f}\")\n</code></pre>"},{"location":"practicas/01-aprendizaje-supervisado/#37-evaluacion-final-parte-3","title":"3.7 Evaluaci\u00f3n Final (Parte 3)","text":"<p>Reporta en una tabla comparativa:</p> Modelo CV Accuracy Test Accuracy F1-Score OOB Score Tiempo (s) Decision Tree (baseline) ... ... ... N/A ... Random Forest por defecto ... ... ... ... ... Random Forest mejor config ... ... ... ... ... Extra Trees ... ... ... N/A ..."},{"location":"practicas/01-aprendizaje-supervisado/#parte-4-gradient-boosting","title":"Parte 4: Gradient Boosting","text":""},{"location":"practicas/01-aprendizaje-supervisado/#41-gradient-boosting-con-configuracion-por-defecto","title":"4.1 Gradient Boosting con Configuraci\u00f3n por Defecto","text":"<pre><code>from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(random_state=42)\ngb.fit(X_train, y_train)\ncv_gb = cross_val_score(gb, X_train, y_train, cv=5)\nprint(f\"CV={cv_gb.mean():.4f} (+/-{cv_gb.std():.4f}), \"\nf\"Test={gb.score(X_test, y_test):.4f}\")\n</code></pre>"},{"location":"practicas/01-aprendizaje-supervisado/#42-efecto-del-numero-de-estimadores-y-learning-rate","title":"4.2 Efecto del N\u00famero de Estimadores y Learning Rate","text":"<p>En Gradient Boosting el n\u00famero de estimadores y el learning rate est\u00e1n fuertemente relacionados: cuantos m\u00e1s estimadores, menor debe ser el learning rate para no sobreajustar.</p> <pre><code>configs = [\n(50,  0.1), (100, 0.1), (200, 0.1),\n(50,  0.5), (100, 0.05), (200, 0.01)\n]\nfor n_est, lr in configs:\ngb = GradientBoostingClassifier(n_estimators=n_est, learning_rate=lr, random_state=42)\ncv = cross_val_score(gb, X_train, y_train, cv=5)\ngb.fit(X_train, y_train)\nprint(f\"n={n_est}, lr={lr}: CV={cv.mean():.4f}, Test={gb.score(X_test, y_test):.4f}\")\n</code></pre> <p>Reporta en una tabla:</p> n_estimators learning_rate CV Accuracy Test Accuracy 50 0.1 ... ... 100 0.1 ... ... 200 0.1 ... ... 100 0.05 ... ... 200 0.01 ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#43-efecto-de-max_depth","title":"4.3 Efecto de max_depth","text":"<p>En Gradient Boosting los \u00e1rboles base son deliberadamente simples (stumps o \u00e1rboles poco profundos).</p> <pre><code>for depth in [1, 2, 3, 5]:\ngb = GradientBoostingClassifier(max_depth=depth, n_estimators=100,\nlearning_rate=0.1, random_state=42)\ncv = cross_val_score(gb, X_train, y_train, cv=5)\ngb.fit(X_train, y_train)\nprint(f\"max_depth={depth}: CV={cv.mean():.4f}, \"\nf\"train={gb.score(X_train, y_train):.4f}, test={gb.score(X_test, y_test):.4f}\")\n</code></pre> <p>Reporta en una tabla:</p> max_depth Train Acc CV Accuracy Test Accuracy 1 (stump) ... ... ... 2 ... ... ... 3 ... ... ... 5 ... ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#44-grid-search","title":"4.4 Grid Search","text":"<pre><code>param_grid = {\n'n_estimators':  [100, 200],\n'learning_rate': [0.01, 0.05, 0.1, 0.5],\n'max_depth':     [1, 2, 3],\n'subsample':     [0.8, 1.0]\n}\ngrid_gb = GridSearchCV(\nGradientBoostingClassifier(random_state=42),\nparam_grid,\ncv=5,\nscoring='accuracy',\nn_jobs=-1\n)\ngrid_gb.fit(X_train, y_train)\nprint(f\"Mejores par\u00e1metros: {grid_gb.best_params_}\")\nprint(f\"Mejor CV score:     {grid_gb.best_score_:.4f}\")\nprint(f\"Test score:         {grid_gb.best_estimator_.score(X_test, y_test):.4f}\")\n</code></pre>"},{"location":"practicas/01-aprendizaje-supervisado/#45-adaboost","title":"4.5 AdaBoost","text":"<p>AdaBoost es otro m\u00e9todo de boosting. En lugar de ajustar residuos, pondera las muestras mal clasificadas para que los siguientes estimadores les presten m\u00e1s atenci\u00f3n.</p> <pre><code>from sklearn.ensemble import AdaBoostClassifier\n# AdaBoost con \u00e1rbol de decisi\u00f3n de profundidad 1 (stump) como estimador base\nada = AdaBoostClassifier(\nestimator=DecisionTreeClassifier(max_depth=1),\nn_estimators=100,\nlearning_rate=1.0,\nrandom_state=42\n)\nada.fit(X_train, y_train)\ncv_ada = cross_val_score(ada, X_train, y_train, cv=5)\nprint(f\"AdaBoost: CV={cv_ada.mean():.4f} (+/-{cv_ada.std():.4f}), \"\nf\"Test={ada.score(X_test, y_test):.4f}\")\n</code></pre> <p>Reporta en una tabla explorando n_estimators y learning_rate:</p> n_estimators learning_rate CV Accuracy Test Accuracy 50 1.0 ... ... 100 1.0 ... ... 100 0.5 ... ... 200 0.1 ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#46-evaluacion-final-parte-4","title":"4.6 Evaluaci\u00f3n Final (Parte 4)","text":"<p>Reporta en una tabla comparativa:</p> Modelo CV Accuracy Test Accuracy F1-Score Tiempo (s) Random Forest (mejor config) ... ... ... ... Gradient Boosting por defecto ... ... ... ... Gradient Boosting mejor config ... ... ... ... AdaBoost ... ... ... ..."},{"location":"practicas/01-aprendizaje-supervisado/#parte-final-comparacion-y-evaluacion-global","title":"Parte Final: Comparaci\u00f3n y Evaluaci\u00f3n Global","text":"<p>Re\u00fane los mejores modelos de cada parte en una tabla comparativa final.</p> <pre><code>from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nbest_models = {\n'SVM (mejor config)':   ...,\n'Decision Tree':        ...,\n'Random Forest':        ...,\n'Gradient Boosting':    ...,\n'AdaBoost':             ...\n}\nfor name, model in best_models.items():\nmodel.fit(X_train, y_train)\ncv  = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\nacc = model.score(X_test, y_test)\nprint(f\"{name}: CV={cv.mean():.4f} (+/-{cv.std():.4f}), Test={acc:.4f}\")\n</code></pre> <p>Tabla comparativa final (mejor configuraci\u00f3n de cada m\u00e9todo):</p> Modelo CV Accuracy Test Accuracy Precision Recall F1-Score Tiempo (s) SVM ... ... ... ... ... ... Decision Tree ... ... ... ... ... ... Random Forest ... ... ... ... ... ... Gradient Boosting ... ... ... ... ... ... AdaBoost ... ... ... ... ... ... <p>Incluye la matriz de confusi\u00f3n del mejor modelo global.</p> <pre><code>best_model = ...\ny_pred = best_model.predict(X_test)\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot()\nplt.savefig('confusion_matrix_final.png', dpi=300, bbox_inches='tight')\n</code></pre>"},{"location":"practicas/01-aprendizaje-supervisado/#entregas","title":"Entregas","text":""},{"location":"practicas/01-aprendizaje-supervisado/#1-memoria-en-latex-pdf","title":"1. Memoria en LaTeX (PDF)","text":"<ul> <li>Usa la plantilla proporcionada.</li> <li>Incluye todas las tablas con resultados completos de cada parte.</li> <li>Matriz de confusi\u00f3n del mejor modelo de cada parte y de la comparativa final.</li> <li>M\u00e1ximo 20 p\u00e1ginas.</li> </ul>"},{"location":"practicas/01-aprendizaje-supervisado/#2-notebook-de-python-ipynb","title":"2. Notebook de Python (.ipynb)","text":"<ul> <li>C\u00f3digo completo y ejecutable.</li> <li>Comentado apropiadamente.</li> <li>Organizado en secciones que se correspondan con las partes del enunciado.</li> </ul>"},{"location":"practicas/01-aprendizaje-supervisado/#recursos","title":"Recursos","text":"<ul> <li>SVC: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html</li> <li>LinearSVC: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html</li> <li>SGDClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html</li> <li>DecisionTreeClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</li> <li>RandomForestClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</li> <li>ExtraTreesClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html</li> <li>GradientBoostingClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html</li> <li>AdaBoostClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html</li> </ul>"}]}