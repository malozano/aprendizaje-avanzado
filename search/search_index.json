{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introducci\u00f3n","text":""},{"location":"#aprendizaje-avanzado","title":"Aprendizaje Avanzado","text":"<p>Este material se proporciona como apuntes para la asignatura Aprendizaje Avanzado del Grado en Ingenier\u00eda en Inteligencia Artificial de la Universidad de Alicante. </p>"},{"location":"#contexto-de-la-asignatura","title":"Contexto de la asignatura","text":"<p>La asignatura Aprendizaje Avanzado forma parte de la materia Aprendizaje Autom\u00e1tico  junto con Fundamentos del Aprendizaje Autom\u00e1tico y Redes Neuronales y Aprendizaje Profundo. Se imparte en el segundo cuatrimestre del tercer curso, como continuaci\u00f3n natural de Fundamentos del Aprendizaje Autom\u00e1tico y de forma paralela con Redes Neuronales y Aprendizaje Profundo.</p> <p>Una vez establecidas las bases del aprendizaje autom\u00e1tico en la asignatura de fundamentos, Aprendizaje Avanzado se centra en proporcionar una visi\u00f3n amplia de los diferentes tipos de modelos existentes, buscando adquirir la capacidad de seleccionar, integrar, optimizar y evaluar m\u00faltiples t\u00e9cnicas para resolver problemas complejos del mundo real.</p> <p>Se recomienda haber cursado previamente Fundamentos del Aprendizaje Autom\u00e1tico.</p>"},{"location":"#licencia","title":"Licencia","text":"<p>\u00a9 2025 Miguel \u00c1ngel Lozano Ortega</p> <p> </p> <p>Este material est\u00e1 disponible bajo licencia Creative Commons Attribution 4.0 International.  Esto significa que puedes usar, compartir y adaptar estos apuntes libremente, siempre que des el cr\u00e9dito apropiado.</p>"},{"location":"01-modelos-no-param/","title":"1. Modelos param\u00e9tricos y no param\u00e9tricos","text":""},{"location":"01-modelos-no-param/#tema-1-modelos-parametricos-y-no-parametricos","title":"Tema 1: Modelos param\u00e9tricos y no param\u00e9tricos","text":"<p>Los modelos no param\u00e9tricos, a diferencia de los modelos param\u00e9tricos, son una familia de modelos que no asumen una forma funcional fija. Una implicaci\u00f3n importante de este hecho es que la complejidad del modelo puede crecer en funci\u00f3n del conjunto de datos. </p> <p>En un modelo param\u00e9trico se define de antemano la forma funcional, que podr\u00eda ser por ejemplo lineal o cuadr\u00e1tica, y tendremos que estimar un n\u00famero fijo de par\u00e1metros, pero si la estructura de los datos es m\u00e1s compleja y no conocemos su forma, puede que no puedan ajustarse de forma adecuada al modelo.</p> <p>Los modelos no param\u00e9tricos no imponen una estructura fija, sino que permiten que sean los propios datos los que determinen la complejidad del modelo. </p> <p>Desde el punto de vista del compromiso (trade-off) entre sesgo (bias) y varianza, podemos intuir que los modelos param\u00e9tricos tender\u00e1n a tener un mayor sesgo, si el modelo no es lo suficientemente complejo como para poder representar los datos (riesgo de underfitting), mientras que los modelos no param\u00e9tricos tender\u00e1n a tener menor sesgo pero mayor varianza (riesgo de overfitting), siendo sensibles a los cambios en los datos de entrenamiento.</p> <p>Vamos a ilustrarlo a continuaci\u00f3n mediante un clasificador param\u00e9trico sencillo.</p>"},{"location":"01-modelos-no-param/#modelos-lineales","title":"Modelos lineales","text":"<p>Una familia caracter\u00edstica de modelos param\u00e9tricos son los modelos lineales, que son aquellos que asumen una relaci\u00f3n lineal entre los datos de entrada y la variable objetivo.</p> <p>El modelo lineal predice la salida como una combinaci\u00f3n lineal ponderada de las variables de entrada, con la siguiente forma:</p> \\[ \\hat{y} = w_1 x_1 + w_2 x_2 + \\ldots + w_N x_N = \\mathbf{x}^T \\mathbf{w} + b \\] <p>Donde \\(\\mathbf{x}\\) es el vector de caracter\u00edsticas de entrada, \\(\\mathbf{w}\\) es el vector de coeficientes que el modelo aprende, \\(b\\) es el sesgo o desplazamiento y \\(\\hat{y}\\) la predicci\u00f3n que nos da el modelo. </p> <p>Desde el punto de vista geom\u00e9trico, esta ecuaci\u00f3n define un hiperplano. Con esto, podemos dar la siguiente interpretaci\u00f3n al funcionamiento de estos modelos:</p> <ul> <li>Tareas de regresi\u00f3n: Se busca el hiperplano que mejor se ajusta a los datos de entrada.</li> <li>Tareas de clasificaci\u00f3n: Se busca el hiperplano que mejor separa los datos de dos clases. </li> </ul> <p>Vamos a centrarnos a continuaci\u00f3n en la tarea de clasificaci\u00f3n, y estudiaremos dos de los principales m\u00e9todos lineales para clasificaci\u00f3n: Regresi\u00f3n Log\u00edstica y SVM (Hastie et al., 2009)1. </p>"},{"location":"01-modelos-no-param/#hiperplanos-y-clasificacion","title":"Hiperplanos y clasificaci\u00f3n","text":"<p>En un problema de clasificaci\u00f3n, si contamos con datos linealmente separables podremos encontrar un hiperplano que nos permita clasificarlos sin errores. El hiperplano tendr\u00e1 la siguiente forma:</p> \\[ \\mathbf{x^T} \\mathbf{w} + b = 0 \\] <p>Donde sus par\u00e1metros son \\(\\mathbf{w}\\), que representa un vector perpendicular al hiperplano, y \\(b\\), como t\u00e9rmino de desplazamiento. </p> <p>Para facilitar la visualizaci\u00f3n, vamos a considerar el caso concreto de dos dimensiones, donde el hiperplano anterior ser\u00eda una l\u00ednea, con la siguiente ecuaci\u00f3n:</p> \\[ w_1 x_1 + w_2 x_2 + b = 0 \\ \\] <p>Por ejemplo, si consideramos como par\u00e1metros el vector \\(\\mathbf{w} = [0.45, 0.89]\\) y \\(b = -2\\), tendremos la siguiente recta:</p> <p>Figura 1: Ejemplo de hiperplano para separar dos conjuntos de datos </p> <p>En la  podemos observar que si el vector \\(\\mathbf{w}\\) es unitario, como en el ejemplo anterior, entonces el t\u00e9rmino \\(b\\) coincide con la distancia del plano al origen. En el caso general en el que \\(\\mathbf{w}\\) no es unitario, la distancia al origen ser\u00e1 \\(|b| / \\lVert \\mathbf{w} \\rVert\\).  </p> <p>La ecuaci\u00f3n del hiperplano nos proporciona una forma sencilla de clasificar los puntos seg\u00fan se encuentren a uno u otro lado, tomando dicha ecuaci\u00f3n como funci\u00f3n:</p> \\[ f(\\mathbf{\\mathbf{x}}) = \\mathbf{x^T} \\mathbf{w} + b \\] <p>En la funci\u00f3n \\(f(\\mathbf{x})\\), todos los puntos que pertenezcan al hiperplano nos dar\u00e1n \\(f(\\mathbf{x})=0\\) (cumplir\u00edan la ecuaci\u00f3n del hiperplano), pero lo que realmente nos interesa es que todos los puntos que est\u00e9n al lado al que apunta el vector \\(\\mathbf{w}\\) har\u00e1n que \\(f(\\mathbf{x})\\) tenga signo positivo, mientras que los que est\u00e9n al lado contrario har\u00e1n que tenga signo negativo.  Es decir, podemos utilizar el signo de dicha funci\u00f3n como clasificador:</p> \\[ G(\\mathbf{x}) = signo[f(\\mathbf{x})] \\] <p>De esta forma, diferentes puntos de datos quedar\u00edan clasificados tal como se muestra en la </p> <p>Figura 2: Ejemplo de clasificaci\u00f3n con un hiperplano </p> <p>Adem\u00e1s, en caso de que \\(\\mathbf{w}\\) sea unitario, la funci\u00f3n \\(f(\\mathbf{\\mathbf{x}})\\) nos dar\u00e1 la distancia (con signo) desde cada punto al plano. Podemos ver esto de forma intuitiva, considerando que el producto escalar \\(\\mathbf{x^T} \\mathbf{w}\\) nos da la proyecci\u00f3n de \\(\\mathbf{x}\\) sobre el vector \\(\\mathbf{w}\\) perpendicular al hiperplano. Esto nos dar\u00e1 la distancia al hiperplano si pasara por el origen, y el t\u00e9rmino \\(b\\) introduce un desplazamiento.</p> <p>Existen diferentes m\u00e9todos que nos permiten aprender, a partir de un conjunto de datos, un hiperplano que separe los datos de dos clases. </p>"},{"location":"01-modelos-no-param/#regresion-logistica","title":"Regresi\u00f3n Log\u00edstica","text":"<p>Aunque el nombre pueda resultar confuso, se trata de un m\u00e9todo de clasificaci\u00f3n, y no de regresi\u00f3n. Si bien con el m\u00e9todo de regresi\u00f3n lineal se busca ajustar un hiperplano a un conjunto de puntos, de forma que se minimice la distancia entre los puntos del conjunto de entrada y el hiperplano, en el caso de la regresi\u00f3n log\u00edstica buscamos el hiperplano que mejor separe dos clases de datos. Hablamos en este caso de regresi\u00f3n log\u00edstica binomial, en la que contamos \u00fanicamente con dos clases, aunque tambi\u00e9n podr\u00edamos extender este m\u00e9todo a un mayor n\u00famero de clases, hablando en este caso de regresi\u00f3n log\u00edstica multinomial. Vamos a centrarnos de momento por simplicidad en el caso binomial.  </p> <p>Como hemos visto anteriormente, a partir de la ecuaci\u00f3n del hiperplano podemos determinar si un punto est\u00e1 a uno u otro lado a partir del signo de la funci\u00f3n \\(f(x)\\) anterior.</p> <p>Este modelo destaca por su interpretabilidad, y es ampliamente utilizado como modelo base en numerosos problemas de clasificaci\u00f3n. </p> <p>La clave principal de la regresi\u00f3n logistica consiste en aplicar sobre la funci\u00f3n anterior la funci\u00f3n sigmoide, modelando mediante esta funci\u00f3n la probabilidad de pertenencia a cada clase.</p>"},{"location":"01-modelos-no-param/#funcion-sigmoide","title":"Funci\u00f3n sigmoide","text":"<p>La funci\u00f3n sigmoide \\(\\sigma(z)\\) tiene la siguiente forma:</p> \\[ \\sigma(z) = \\frac{1}{1+ e^{-z}} \\] <p>Podemos verla representada en la .</p> <p>Figura 3: Forma de la funci\u00f3n sigmoide </p> <p>Como podemos observar, esta funci\u00f3n presenta una transici\u00f3n suave desde \\(0\\) (cuando \\(z \\rightarrow -\\infty\\)) hasta \\(1\\) (cuando \\(z \\rightarrow \\infty\\)), teniendo su punto medio en \\(\\sigma(0) = 0.5\\). </p> <p>La funci\u00f3n es siempre creciente y derivable, lo cual es una propiedad importante para la optimizaci\u00f3n.</p> <p>La funci\u00f3n sigmoide tiene la siguiente derivada:</p> \\[ \\sigma'(z) = \\sigma(z) (1 - \\sigma(z)) \\] <p>Esta forma simplifica mucho el c\u00e1lculo del gradiente.</p> <p>Podemos sustituir \\(z\\) por nuestra funci\u00f3n \\(f(x)\\) que nos permite clasificar los puntos en funci\u00f3n del signo, teniendo:</p> \\[ h_w(\\mathbf{x}) = \\sigma(f(x)) =  \\frac{1}{1 + e^{-f(x)}} = \\frac{1}{1 + e^{-(\\mathbf{x}^T \\mathbf{w} + b)}} \\] <p>Podemos interpretar \\(h_w(x)\\) como la probabilidad estimada de que \\(y = 1\\) (es decir, de que pertenezca a la clase positiva). Podemos expresarlo como:</p> \\[ h_w(\\mathbf{x}) = P(y=1 | \\mathbf{x}, \\mathbf{w}, b) \\] <p>Estimaremos los par\u00e1metros mediante m\u00e1xima verosimilitud, lo cual equivale a minimizar la p\u00e9rdida logar\u00edtmica (log-loss).</p>"},{"location":"01-modelos-no-param/#funcion-de-coste","title":"Funci\u00f3n de coste","text":"<p>La funci\u00f3n de p\u00e9rdida logar\u00edtmica (log-loss) para una sola muestra tiene la siguiente forma:</p> \\[ L(h_w(\\mathbf{x}), y) = -y \\log (h_w(x)) - (1-y) log(1-h_w(x)) \\] <p>Esta funci\u00f3n tiene la propiedad de que penaliza fuertemente las predicciones confiadas pero incorrectas. Es decir, si la salida esperada es \\(y=1\\) pero la predicci\u00f3n \\(h(\\mathbf{x}) \\rightarrow 0\\), entonces la penalizaci\u00f3n ser\u00e1 alta.</p> <p>Consideremos ahora que tenemos un conjunto de entrenamiento con \\(N\\) pares \\((\\mathbf{x_i}, y_i)\\) con \\(\\mathbf{x_i} \\in \\mathbb{R}^d\\) y \\(y_i \\in \\{0, 1\\}\\) (problema binomial), siendo \\(d\\) el n\u00famero de features.</p> <p>Con todo ello, para el conjunto de muestras podemos construir la siguiente funci\u00f3n de coste:</p> \\[ J(\\mathbf{w}) = -\\frac{1}{N} \\sum_{i=1}^N [y_i \\log (h_w(\\mathbf{x}_i)) + (1-y_i) \\log (1 - h_w(\\mathbf{x}_i))] \\] <p>Esta funci\u00f3n tiene la propiedad de que es convexa (tiene un \u00fanico m\u00ednimo global) y diferenciable, y como hemos comentado, penaliza las predicciones claramente incorrectas.</p>"},{"location":"01-modelos-no-param/#optimizacion","title":"Optimizaci\u00f3n","text":"<p>Buscamos encontrar los pesos \\(\\mathbf{w}\\) que minimicen la funci\u00f3n de coste anterior. </p> \\[ \\mathbf{\\hat{w}} = \\arg \\min_{\\mathbf{w}} J(\\mathbf{w})  \\] <p>Obtenemos el gradiente de la funci\u00f3n, mediante la derivada parcial respecto a cada peso \\(w_j\\):</p> \\[ \\frac{\\delta J(\\mathbf{w})}{\\delta w_j} = \\frac{1}{N} \\sum_{i=1}^N (h_w(\\mathbf{x}_i) - y_i) x_{ij} \\] <p>Podemos expresar el gradiente en forma vectorial, para todos los pesos, de la siguiente forma:</p> \\[ \\nabla J(\\mathbf{w}) = \\frac{1}{N} \\mathbf{x}^T (h_w(\\mathbf{x}) - y)  \\] <p>Donde \\(\\mathbf{x}\\) es una matriz de dimensi\u00f3n \\(N \\times d\\), \\(h_w(\\mathbf{x})\\) es un vector de predicciones de dimensi\u00f3n \\(N \\times 1\\) y \\(y\\) es un vector de etiquetas de dimensi\u00f3n \\(N \\times 1\\) (una fila para cada ejemplo de entrada). </p> <p>De la misma forma, podemos obtener la derivada parcial respecto al sesgo (\\(b\\)):</p> \\[ \\frac{\\delta J(\\mathbf{w})}{\\delta b} = \\frac{1}{N} \\sum_{i=1}^N (h_w(\\mathbf{x}_i) - y_i) \\] <p>Con esto, podremos aplicar Descenso por Gradiente o Descenso por Gradiente estoc\u00e1stico (SGD) para optimizar los pesos. Tambi\u00e9n tenemos otros algoritmos de optimizaci\u00f3n como Coordinate Descent (Hsieh et al., 2008)2, en el que en lugar de aplicar descenso por gradiente a la vez sobre todas las coordenadas, se selecciona de forma iterativa una coordenada, se congela el resto, y se optimiza para la coordenada seleccionada. El algoritmo itera por las diferentes coordenadas hasta la convergencia. Encontramos tambi\u00e9n otros m\u00e9todos de optimizaci\u00f3n, como el m\u00e9todo de Newton (Nocedal &amp; Wright, 2006)3 que utiliza segundas derivadas y presenta la ventaja de una convergencia m\u00e1s r\u00e1pida, aunque resulta algo costoso. Tenemos tambi\u00e9n L-BFGS (Liu &amp; Nocedal, 1989)4 que es una aproximaci\u00f3n eficiente del m\u00e9todo de Newton y es el utilizado por defecto en la implementaci\u00f3n de <code>LogisticRegression</code> en sklearn. Esta implementaci\u00f3n  incluye diferentes solvers alternativos que podemos utilizar para la optimizaci\u00f3n. </p>"},{"location":"01-modelos-no-param/#regularizacion","title":"Regularizaci\u00f3n","text":"<p>Podemos a\u00f1adir a la funci\u00f3n de coste un t\u00e9rmino de penalizaci\u00f3n para prevenir el overfitting y controlar la complejidad del modelo. Encontramos diferentes tipos de regularizaci\u00f3n:</p>"},{"location":"01-modelos-no-param/#regularizacion-l2-ridge","title":"Regularizaci\u00f3n L2 (Ridge)","text":"<p>Busca penalizar pesos grandes, para favorecer soluciones m\u00e1s simples:</p> \\[ \\begin{align*} J(\\mathbf{w}) = &amp;-\\frac{1}{N} \\sum_{i=1}^N [y_i \\log (h_w(\\mathbf{x}_i)) + (1-y_i) \\log (1 - h_w(\\mathbf{x}_i))] + \\\\ &amp; + \\frac{\\lambda}{2N} \\sum_{k=1}^d w_k^2 \\end{align*} \\]"},{"location":"01-modelos-no-param/#regularizacion-l1-lasso","title":"Regularizaci\u00f3n L1 (Lasso)","text":"<p>Favorece que algunos pesos puedan ser exactamente \\(0\\), actuando de esta forma como una selecci\u00f3n de caracter\u00edsticas:</p> \\[ \\begin{align*} J(\\mathbf{w}) = &amp;-\\frac{1}{N} \\sum_{i=1}^N [y_i \\log (h_w(\\mathbf{x}_i)) + (1-y_i) \\log (1 - h_w(\\mathbf{x}_i))] + \\\\ &amp; + \\frac{\\lambda}{N} \\sum_{k=1}^d | w_k | \\end{align*} \\]"},{"location":"01-modelos-no-param/#regularizacion-elastic-net","title":"Regularizaci\u00f3n Elastic Net","text":"<p>Combina L1 y L2:</p> \\[ \\begin{align*} J(\\mathbf{w}) = &amp;-\\frac{1}{N} \\sum_{i=1}^N [y_i \\log (h_w(\\mathbf{x}_i)) + (1-y_i) \\log (1 - h_w(\\mathbf{x}_i))] + \\\\ &amp; + \\frac{\\lambda_1}{N} \\sum_{k=1}^d | w_k | + \\frac{\\lambda_2}{2N} \\sum_{k=1}^d w_k^2  \\end{align*} \\] <p>En todos estos casos tenemos un hiper-par\u00e1metro \\(\\lambda\\) con el que podemos ajustar la regularizaci\u00f3n. Con \\(\\lambda = 0\\) no aplicamos regularizaci\u00f3n, con lo que tendremos mayor riesgo de overfitting, mientras que con valores muy altos podr\u00edamos tener mayor riesgo de underfitting. </p>"},{"location":"01-modelos-no-param/#limitaciones-de-los-modelos-lineales","title":"Limitaciones de los modelos lineales","text":"<p>Como hemos visto, los modelos lineales como regresi\u00f3n log\u00edstica buscan el hiperplano que mejor separe los datos de las diferentes clases. Sin embargo, esto no siempre ser\u00e1 posible.</p>"},{"location":"01-modelos-no-param/#regresion-logistica-con-datos-linealmente-separables","title":"Regresi\u00f3n log\u00edstica con datos linealmente separables","text":"<p>En caso de tener datos linealmente separables, como los que se muestran en la  , existir\u00e1 un hiperplano que los separe. En este caso, un clasificador lineal con solo 3 par\u00e1metros (\\(w_1, w_2, b\\)) ser\u00e1 suficiente para representar los datos.</p> <p>Figura 4: Conjunto de datos linealmente separables </p> <p>Con esta distribuci\u00f3n de los datos, incluso si apareciese alg\u00fan solape entre las dos clases o alg\u00fan outlier, seguir\u00eda siendo posible encontrar un hiperplano que nos permita clasificarlos con una alta precisi\u00f3n, y solo un peque\u00f1o porcentaje de errores (ver ).</p> <p>Figura 5: Conjunto de datos separables con solape </p> <p>Podemos adem\u00e1s ver en la  el mapa de probabilidades que nos proporciona el modelo de regresi\u00f3n log\u00edstica.</p> <p>Figura 6: Mapa de probabilidades </p>"},{"location":"01-modelos-no-param/#regresion-logistica-con-datos-linealmente-no-separables","title":"Regresi\u00f3n log\u00edstica con datos linealmente no separables","text":"<p>Sin embargo, si la distribuci\u00f3n de los datos cambiase, y pasaran a ser no separables, como los que se muestran en la , el modelo anterior no ser\u00eda suficiente para representarlos.</p> <p>Figura 7: Conjunto de datos linealmente no separables </p> <p>En este caso los datos siguen un patr\u00f3n conocido como XOR, distribuido en \\(4\\) cuadrantes, y no es posible encontrar ning\u00fan hiperplano que los separe. Cualquier hiperplano nos dar\u00e1 siempre un error aproximado del \\(50\\%\\), que equivale a lanzar una moneda al aire para predecir la clase, tal como se ve en la figura anterior. </p> <p>Vemos en este caso como el no poder adaptar la complejidad del modelo a los datos (estar\u00edamos limitados a \\(3\\) par\u00e1metros \\(w_1, w_2, b\\)) hace que el modelo no pueda ajustarse de forma adecuada.</p>"},{"location":"01-modelos-no-param/#posibles-soluciones","title":"Posibles soluciones","text":"<p>Para clasificar los puntos del \u00faltimo ejemplo minimizando el error de clasificaci\u00f3n, podr\u00edamos optar por:</p> <ul> <li> <p>Clasificador lineal con ingenier\u00eda de caracter\u00edsticas: Crear manualmente caracter\u00edsticas no lineales (por ejemplo \\(x_1^2,x_2^2,x_1 \\cdot x_2\\)) y usar un modelo lineal sobre ellas. Esto requiere tener conocimiento del dominio, para determinar qu\u00e9 caracter\u00edsticas necesitar\u00edamos para que la funci\u00f3n pueda ajustarse a nuestros datos.</p> </li> <li> <p>Clasificador param\u00e9trico no lineal: Por ejemplo, una red neuronal con n\u00famero fijo de par\u00e1metros. Necesitaremos determinar la arquitectura m\u00e1s adecuada.</p> </li> <li> <p>Clasificador no param\u00e9trico: En este caso no ser\u00e1 necesario conocer previamente la estructura de los datos (su forma funcional), sino que el modelo se adaptar\u00e1 autom\u00e1ticamente a su complejidad.</p> </li> </ul> <p>Veremos a continuaci\u00f3n ejemplos de cada una de estas soluciones.</p>"},{"location":"01-modelos-no-param/#ingenieria-de-caracteristicas","title":"Ingenier\u00eda de caracter\u00edsticas","text":"<p>Vamos en primer lugar a ver c\u00f3mo podr\u00edamos utilizar ingenier\u00eda de caracter\u00edsticas para separar el conjunto de datos anterior (ejemplo XOR). </p> <p>Al estar en dos dimensiones contamos con las features \\(x_1\\) y \\(x_2\\). Lo que vamos a hacer es a\u00f1adir adem\u00e1s las features \\(x_1^2\\), \\(x_1 x_2\\) y \\(x_2^2\\). N\u00f3tese que se trata de features derivadas de las originales. Esto nos va a permitir definir una frontera curva entre los datos, donde el papel de la caracter\u00edstica \\(x_1 x_2\\) ser\u00e1 fundamental, ya que seg\u00fan su signo podremos inferir la clase en una operaci\u00f3n XOR. Podemos ver esto ilustrado en la .</p> <p>Figura 8: Ingenier\u00eda de caracter\u00edsticas </p> <p>Si aplicamos a este conjunto de datos el modelo de regresi\u00f3n log\u00edstica introduciendo las features indicadas anteriormente, obtenemos la frontera de clasificaci\u00f3n que se muestra en la .</p> <p>Figura 9: Frontera de decisi\u00f3n con ingenier\u00eda de caracter\u00edsticas </p> <p>Lo que hemos hecho ha sido crear \\(M\\)  caracter\u00edsticas de entrada \\(h(\\mathbf{x}) = (h_1(\\mathbf{x}), h_2(\\mathbf{x}), \\ldots, h_M(\\mathbf{x}))\\) a partir de las caracter\u00edsticas originales \\(\\mathbf{x}\\). Es decir, con \\(h(\\mathbf{x})\\) estamos proyectando las caracter\u00edsticas originales en un nuevo espacio de caracter\u00edsticas. Con esto, el hiperplano de separaci\u00f3n quedar\u00eda de la siguiente forma:</p> \\[ f(\\mathbf{x}) = h(\\mathbf{x})^T \\mathbf{w} + b \\] <p>Podemos observar que aunque el modelo sigue siendo lineal respecto a las caracter\u00edsticas proyectadas \\(h(\\mathbf{x})\\), puede que ya no lo sea en el espacio original de caracter\u00edsticas de \\(\\mathbf{x}\\). Esto es lo que ocurre en el caso del ejemplo anterior, en el que las caracter\u00edsticas proyectadas son \\(h(\\mathbf{x}) = (x_1, x_2, x_1 x_2, x_1^2, x_2^2)\\), y por lo tanto tenemos un polinomio de grado 2 en el espacio original.</p>"},{"location":"01-modelos-no-param/#modelos-parametricos-no-lineales","title":"Modelos param\u00e9tricos no lineales","text":"<p>Un tipo destacado modelo param\u00e9trico no lineal son las Redes Neuronales. Vamos a establecer la relaci\u00f3n de la Regresi\u00f3n Log\u00edstica con las Redes Neuronales y a estudiar como estos modelos pueden resolver problemas no lineales como el planteado.</p> <p>Es f\u00e1cil determinar que el modelo de regresi\u00f3n log\u00edstica es equivalente a un perceptr\u00f3n con \\(N\\) entradas que aplique una funci\u00f3n sigmoide como funci\u00f3n de activaci\u00f3n (ver ). </p> <p>Figura 10: Perceptr\u00f3n con \\(N\\) entradas y funci\u00f3n de activaci\u00f3n Sigmoide </p> <p>En este caso, la salida de la neurona ser\u00eda:</p> \\[ y = \\sigma(\\sum_{i=1}^N w_i x_i + b) \\] <p>Donde \\(x_i\\) son las entradas, \\(w_b\\) los pesos para cada entrada y \\(b\\) el sesgo o bias. </p> <p>Cuando a la red le a\u00f1adimos varias capas ocultas (ver ), lo que estaremos haciendo es aprender a transformar el espacio original de caracter\u00edsticas \\(\\mathbf{X}\\) en un espacio latente \\(\\mathbf{H}\\) que facilite su clasificaci\u00f3n. Podremos encontrar estas caracter\u00edsticas en la \u00faltima capa de la red, y sobre ellas se aplicar\u00e1 una clasificaci\u00f3n equivalente a la regresi\u00f3n log\u00edstica.</p> <p>Figura 11: Red neuronal profunda con neurona de salida Sigmoidea </p> <p>Esta transformaci\u00f3n del espacio de caracter\u00edsticas nos podr\u00eda permitir mapear nuestros datos de entrada no linealmente separables sobre un espacio en el que si que lo sean. En este caso tenemos un modelo param\u00e9trico pero no lineal. </p> <p>Cabe destacar que aplicando ingenier\u00eda de caracter\u00edsticas debemos dise\u00f1ar manualmente las nuevas caracter\u00edsticas que derivamos del conjunto original, por lo que es necesario tener conocimiento sobre la forma funcional de los datos. Sin embargo, en este caso es la propia red quien aprende de forma autom\u00e1tica las caracter\u00edsticas m\u00e1s adecuadas para resolver el problema.</p>"},{"location":"01-modelos-no-param/#modelos-no-parametricos","title":"Modelos no param\u00e9tricos","text":"<p>Nuestra tercera opci\u00f3n es el uso de modelos no param\u00e9tricos. Estos modelos cuentan con la ventaja de que su complejidad se adapta a los datos. </p> <p>Dentro de este grupo encontramos por ejemplo modelos basados en vecindad como K-NN, que son capaces de adaptarse sin problema al problema XOR anterior y a formas m\u00e1s complejas. En la  vemos la frontera de decisi\u00f3n obtenida cuando aplicamos K-NN al conjunto de datos que sigue el patr\u00f3n XOR.</p> <p>Figura 12: Aplicaci\u00f3n de K-NN al problema XOR </p> <p>En la pr\u00f3xima sesi\u00f3n veremos el modelo SVM, que originalmente se plantea como un modelo param\u00e9trico lineal, pero que puede transformarse en un modelo no param\u00e9trico mediante el conocido como kernel trick. </p> <p>Encontramos muchos otros modelos no param\u00e9tricos, parte de los cuales enumeramos a continuaci\u00f3n:</p> <ul> <li> <p>M\u00e9todos basados en vecindad. Encontramos K-NN tanto para clasificaci\u00f3n como para regresi\u00f3n, as\u00ed como variantes como K-NN ponderado por distancia, as\u00ed como m\u00e9todos de estimaci\u00f3n de densidad como Kernel Density Estimation (KDE)</p> </li> <li> <p>M\u00e9todos de kernel, como SVM y sus variantes. </p> </li> <li> <p>M\u00e9todos basados en \u00e1rboles. Encontramos los \u00e1rboles de decisi\u00f3n, que nos permiten abordar problemas de clasificaci\u00f3n y regresi\u00f3n, y m\u00e9todos de ensemble que combinan diferentes clasificadores \"d\u00e9biles\" para construir un clasificador \"fuerte\". Dentro de este \u00faltimo subgrupo, encontramos m\u00e9todos como Random Forest, AdaBoost, Gradient Boosting y XGBoost.</p> </li> <li> <p>M\u00e9todos de clustering, como DBSCAN,  Gaussian Mixture Models (GMM) y Spectral Clustering.  </p> </li> <li> <p>M\u00e9todos de aprendizaje por refuerzo, como Q-learning y SARSA.</p> </li> <li> <p>M\u00e9todos de reducci\u00f3n de la dimensionalidad, como t-SNE y UMAP.</p> </li> </ul> <p>En las pr\u00f3ximas sesiones estudiaremos varios de estos modelos.</p> <ol> <li> <p>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). The elements of statistical learning: Data mining, inference, and prediction (2nd ed., pp. 119--128). Springer.\u00a0\u21a9</p> </li> <li> <p>Hsieh, C.-J., Chang, K.-W., Lin, C.-J., Keerthi, S. S., &amp; Sundararajan, S. (2008). A dual coordinate descent method for large-scale linear SVM. Proceedings of the 25th International Conference on Machine Learning, 408--415.\u00a0\u21a9</p> </li> <li> <p>Nocedal, J., &amp; Wright, S. J. (2006). Numerical optimization (2nd ed.). Springer.\u00a0\u21a9</p> </li> <li> <p>Liu, D. C., &amp; Nocedal, J. (1989). On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(1-3), 503--528.\u00a0\u21a9</p> </li> </ol>"},{"location":"02-svm/","title":"2. SVM","text":""},{"location":"02-svm/#support-vector-machines-svm","title":"Support Vector Machines (SVM)","text":"<p>Las Support Vector Machines (SVM) (Cortes &amp; Vapnik, 1995)1 son algoritmos de aprendizaje supervisado utilizados principalmente para clasificaci\u00f3n, aunque tambi\u00e9n pueden aplicarse a regresi\u00f3n. </p>"},{"location":"02-svm/#aplicaciones-de-svm","title":"Aplicaciones de SVM","text":"<p>Aunque en la actualidad los modelos basados en aprendizaje profundo dominan en campos como la Visi\u00f3n por Computador (CV) o el Procesamiento del Lenguaje Natural (NLP), y en general cuando contamos con extensos conjuntos de datos y disponibilidad de gran capacidad de computaci\u00f3n, modelos como SVM pueden ser competitivos cuando contemos con datos tabulares de tama\u00f1o peque\u00f1o o mediano.</p> <p>SVM ofrece un buen rendimiento con  conjuntos de datos peque\u00f1os. A modo orientativo, con conjuntos de menos de 1.000 ejemplos puede resultar la opci\u00f3n m\u00e1s adecuada, y podr\u00eda mantenerse competitivo incluso con datasets del orden de 10.000 ejemplos.  Encontramos otros modelos que siguen ofreciendo resultados competitivos en estos casos, como XGBoost o Random Forest. </p> <p>Por ejemplo, un \u00e1rea en la que estos modelos pueden resultar de inter\u00e9s es en el an\u00e1lisis de datos m\u00e9dicos, en los que contamos con datasets peque\u00f1os (datos de pacientes) pero con alta dimensionalidad (por ejemplo teniendo en cuenta la expresi\u00f3n de diferentes genes). Adem\u00e1s, tenemos la ventaja de que este tipo de modelos facilita la interpretabilidad, lo cual los hace especialmente interesantes en estos \u00e1mbitos. </p>"},{"location":"02-svm/#maximizacion-del-margen","title":"Maximizaci\u00f3n del margen","text":"<p>Como hemos visto anteriormente, en un problema de clasificaci\u00f3n binaria buscamos encontrar un hiperplano que separe los datos de las dos clases, pero, \u00bfcu\u00e1l es el hiperplano de separaci\u00f3n \u00f3ptimo? Lo que plantea SVM es buscar el hiperplano que maximiza el margen entre las dos clases, a diferencia del modelo de regresi\u00f3n log\u00edstica en el que lo que se buscaba era maximizar la verosimilitud. Es decir, regresi\u00f3n log\u00edstica proporciona probabilidades bien calibradas, mientras que SVM prioriza la robustez del margen, sin producir probabilidades de forma directa.</p> <p>El margen ser\u00e1 la distancia desde el hiperplano hasta los puntos m\u00e1s cercanos de cada clase. Estos puntos m\u00e1s cercanos son conocidos como vectores de soporte (ver ). </p> <p>Figura 1: Maximizaci\u00f3n del margen </p> <p>Adem\u00e1s, como veremos m\u00e1s adelante, SVM se puede generalizar para casos en los que los datos no sean separables de forma l\u00edneal. </p>"},{"location":"02-svm/#margen-duro","title":"Margen duro","text":"<p>Vamos en primer lugar a suponer que los datos son linealmente separables. Hablamos entonces de margen duro, ya que estableceremos la restricci\u00f3n de que los puntos pertenecientes a cada clase deben quedar siempre al lado correcto del margen.</p> <p>Consideremos que tenemos un conjunto de entrenamiento con \\(N\\) pares \\((\\mathbf{x_i}, y_i)\\) con \\(\\mathbf{x_i} \\in \\mathbb{R}^d\\) y \\(y_i \\in \\{-1, 1\\}\\) (problema de clasificaci\u00f3n binaria), siendo \\(d\\) el n\u00famero de features.</p> <p>En caso de que el vector \\(\\mathbf{w}\\) sea unitario, la funci\u00f3n \\(f(\\mathbf{x})\\) nos dar\u00e1 la distancia desde el hiperplano a cada punto \\(\\mathbf{x}\\). </p> <p>Con esto, para buscar el hiperplano que maximice el margen \\(M\\), deberemos resolver el siguiente problema de optimizaci\u00f3n:</p> \\[ \\begin{align*} \\max_{\\mathbf{w}, b, \\lVert \\mathbf{w}  \\rVert=1} \\quad  &amp; M \\\\ \\text{s.a.} \\quad &amp; y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq M, i = 1, \\ldots, N \\end{align*} \\] <p>Podemos eliminar la restricci\u00f3n de que \\(\\mathbf{w}\\) sea unitario dividiendo la ecuaci\u00f3n del hiperplano entre \\(\\lVert \\mathbf{w} \\rVert\\). Si dividimos toda la ecuaci\u00f3n seguir\u00e1 representando al mismo hiperplano y nos permitir\u00e1  reemplazar la condici\u00f3n con:</p> \\[ \\frac{1}{\\lVert \\mathbf{w} \\rVert} y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq M \\] <p>O lo que es lo mismo:</p> \\[ y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq M \\lVert \\mathbf{w} \\rVert \\] <p>En este caso, el hiperplano seguir\u00e1 siendo el mismo independientemente del valor de \\(\\lVert \\mathbf{w} \\rVert\\). Por lo tanto, podemos considerar de forma arbitraria que \\(\\lVert \\mathbf{w} \\rVert = 1 / M\\), lo cual nos permite reescribir la restricci\u00f3n anterior de la siguiente forma:</p> \\[ y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq 1  \\]"},{"location":"02-svm/#forma-primal","title":"Forma primal","text":"<p>Con lo anterior, el problema de optimizaci\u00f3n a resolver tendr\u00eda la siguiente forma:</p> \\[ \\begin{align*} \\min_{\\mathbf{w}, b} \\quad &amp; \\mathbf{\\lVert w \\rVert} \\\\ \\text{s.a.} \\quad &amp; y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq 1, i = 1, \\ldots, N \\end{align*} \\] <p>Esta es la conocida como forma primal, en la que tenemos nuestra funci\u00f3n objetivo y una serie de restricciones. Podr\u00edamos resolver este problema aplicando alg\u00fan m\u00e9todo de optimizaci\u00f3n como descenso por gradiente, o descenso por gradiente estoc\u00e1stico (SGD), para buscar los par\u00e1metros \\(\\mathbf{w}\\) y \\(b\\) \u00f3ptimos. </p> <p>Sin embargo, vamos a utilizar el m\u00e9todo de los multiplicadores de Lagrange (Boyd &amp; Vandenberghe, 2004)2 para transformar este problema con restricciones a un problema en el que las restricciones se transforman en penalizaciones a la funci\u00f3n objetivo.</p> <p>El problema de optimizaci\u00f3n anterior ser\u00eda equivalente al siguiente, ya que el m\u00ednimo de \\(\\mathbf{\\lVert w \\rVert}\\) ser\u00e1 el mismo que el de \\(\\frac{1}{2} \\mathbf{\\lVert w \\rVert}^2\\):</p> \\[ \\begin{align*} \\min_{ \\mathbf{w}, b} \\quad  &amp; \\frac{1}{2} \\mathbf{\\lVert w \\rVert}^2 \\\\ \\text{s.a.} \\quad &amp; y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq 1, i = 1, \\ldots, N \\end{align*} \\] <p>Sin embargo, esta segunda forma nos da ventajas importantes, especialmente la diferenciabilidad de \\(\\frac{1}{2} \\mathbf{\\lVert w \\rVert}^2\\), que es derivable en todos sus puntos, mientras \\(\\mathbf{\\lVert w \\rVert}\\) no es derivable cuando \\(\\mathbf{\\lVert w \\rVert}\\) = 0. </p> <p>Debemos recordar que estamos asumiendo de momento que los datos son separables (margen duro), y por lo tanto consideramos \u00fanicamente dos casos posibles:</p> <ul> <li>\\(y_i(\\mathbf{x}_i^T\\mathbf{w} + b) &gt; 1\\) : correctamente clasificados fuera del margen.</li> <li>\\(y_i(\\mathbf{x}_i^T\\mathbf{w} + b) = 1\\) : Vectores de soporte, pertenecientes al margen.</li> </ul> <p>Para resolver el problema de optimizaci\u00f3n mediante multiplicadores de Lagrange, la funci\u00f3n Lagrangiana primal que deberemos minimizar respecto a \\(\\mathbf{w}\\) y \\(b\\) es la siguiente:</p> \\[ L_P(\\mathbf{w}, b, \\alpha) = \\frac{1}{2} \\lVert \\mathbf{w} \\rVert^2 - \\sum_{i=1}^N \\alpha_i [ y_i ( \\mathbf{x}_i^T \\mathbf{w} + b ) - 1 ] \\] <p>Hemos transformado cada restricci\u00f3n en un t\u00e9rmino de la funci\u00f3n a minimizar, y aplicado a cada uno de estos t\u00e9rminos un multiplicador \\(\\alpha_i\\) (multiplicador de Lagrange). </p> <p>Derivamos la funci\u00f3n anterior respecto a \\(\\mathbf{w}\\) y \\(b\\), y establecemos las derivadas a \\(0\\) para buscar el punto en el que la funci\u00f3n presenta un m\u00ednimo (condici\u00f3n de estacionariedad). Tenemos entonces:</p> \\[ \\begin{align*} \\frac {\\delta L(\\mathbf{w}, b, \\alpha)}{\\delta \\mathbf{w}} &amp;= \\mathbf{w} - \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i = 0 &amp; \\Rightarrow \\mathbf{w} = \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i  \\\\ \\frac {\\delta L(\\mathbf{w}, b, \\alpha)}{\\delta b} &amp;= \\sum_{i=1}^N \\alpha_i y_i  = 0  &amp; \\Rightarrow 0 = \\sum_{i=1}^N \\alpha_i y_i \\end{align*} \\] <p>Sustituyendo \\(\\mathbf{w}\\) en la Lagrangiana (teniendo en cuenta que \\(\\lVert \\mathbf{w} \\rVert^2 = \\mathbf{w}^T \\mathbf{w}\\)) tenemos:</p> \\[ \\begin{align*} L_D(\\mathbf{w}, b, \\alpha) &amp;= \\frac{1}{2}  \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i^T \\sum_{j=1}^N \\alpha_j y_j \\mathbf{x}_j   - \\sum_{i=1}^N \\alpha_i [ y_i ( \\mathbf{x}_i^T \\sum_{j=1}^N \\alpha_j y_j \\mathbf{x}_j + b ) - 1 ] = \\\\ &amp;= \\frac{1}{2}  \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i^T \\sum_{j=1}^N \\alpha_j y_j \\mathbf{x}_j     - \\sum_{i=1}^N \\alpha_i y_i  \\mathbf{x}_i^T \\sum_{j=1}^N \\alpha_j y_j \\mathbf{x}_j  - b \\sum_{i=1}^N \\alpha_i y_i   + \\sum_{i=1}^N \\alpha_i = \\\\  &amp;= \\sum_{i=1}^N \\alpha_i - \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j  \\end{align*} \\]"},{"location":"02-svm/#forma-dual","title":"Forma dual","text":"<p>Tenemos entonces el problema dual. A diferencia del problema primal donde minimiz\u00e1bamos la norma de \\(\\mathbf{w}\\), ahora buscamos maximizar la funci\u00f3n \\(L_D(\\alpha)\\):</p> \\[ \\begin{align*} \\max_\\alpha \\quad &amp; L_D(\\alpha) =  \\sum_{i=1}^N \\alpha_i - \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j  \\\\ s.a.\\quad  &amp; \\sum_{i=1}^N \\alpha_i y_i  = 0  \\\\ &amp;\\alpha_i \\geq 0 \\  \\forall i=1, \\ldots, N \\end{align*} \\] <p>Esto ocurre porque al transformar el problema mediante multiplicadores de Lagrange, la funci\u00f3n dual nos proporciona una cota inferior del valor \u00f3ptimo del problema primal, por lo que para encontrar la mejor soluci\u00f3n debemos maximizar esta cota. </p> <p>Debemos destacar en este punto que en el caso de la forma dual deberemos optimizar los multiplicadores \\(\\alpha_i\\), en lugar de los par\u00e1metro \\(\\mathbf{w}\\) y \\(b\\) como ocurr\u00eda en el caso de la forma primal. </p> <p>Nos encontramos con un problema de programaci\u00f3n cuadr\u00e1tica (QP) convexo con restricciones lineales. Este tipo de problemas tienen la siguiente forma general:</p> \\[ f(\\mathbf{\\alpha}) = \\frac{1}{2} \\mathbf{\\alpha}^T Q \\mathbf{\\alpha} + c^T \\mathbf{\\alpha} \\] <p>Para que el problema sea convexo, la matriz \\(Q\\) debe ser semidefinida positiva, y esto se cumple en el caso de SVM, ya que tenemos:</p> \\[  Q_{ij} = y_i y^j \\mathbf{x}^T_i \\mathbf{x}_j \\] <p>Podremos por lo tanto aplicar alg\u00fan algoritmo de optimizaci\u00f3n para este tipo de problemas. Encontramos diferentes solvers, como por ejemplo CVXOPT o OSQP en Python. </p> <p>En la pr\u00e1ctica, el algoritmo m\u00e1s utilizado es SMO (Sequential Minimal Optimization) (Platt, 1998)3. Este es el algoritmo utilizado por ejemplo por LIBSVM, que es la librer\u00eda que encontramos integrada en scikit-learn. En este caso, en lugar, de resolver un problema QP completo con \\(N\\) variables, selecciona solo dos variables \\(\\alpha_i\\) y \\(\\alpha_j\\) iterativamente, fijando el resto, las optimiza, e itera hasta la convergencia. </p>"},{"location":"02-svm/#condiciones-kkt","title":"Condiciones KKT","text":"<p>En un problema de optimizaci\u00f3n convexo con restricciones para que un punto sea \u00f3ptimo debe satisfacer un conjunto de condiciones conocidas como condiciones KKT (Karush-Kuhn-Tucker) (Boyd &amp; Vandenberghe, 2004; Kuhn &amp; Tucker, 1951)4 2:</p> <ol> <li>Estacionariedad. Buscamos que la funci\u00f3n Lagrangiana tenga gradiente \\(0\\). Se cumple al haber igualado las derivadas a \\(0\\). </li> <li>Factibilidad. El punto debe ser factible y cumplir las restricciones.</li> <li> <p>Signo. Todos los multiplicadores asociados a restricciones de desigualdad deben tener signo positivo: $$ \\alpha_i \\geq 0 \\quad \\forall i=1, \\ldots, N $$  </p> </li> <li> <p>Complementariedad. Adem\u00e1s de las condiciones anteriores, es importante cumplir tambi\u00e9n la siguiente condici\u00f3n:</p> </li> </ol> \\[ \\alpha_i [ y_i ( \\mathbf{x}_i^T \\mathbf{w} + b) - 1 ] = 0 \\quad \\forall i=1, \\ldots, N \\] <p>Esta \u00faltima condici\u00f3n nos dice que:</p> <ul> <li> <p>Si \\(\\alpha_i &gt; 0\\), entonces la restricci\u00f3n es activa y debe cumplirse \\([ y_i ( \\mathbf{x}_i^T \\mathbf{w} + b) - 1 ] = 0\\). Estos ser\u00e1n los puntos conocidos como vectores de soporte, que se encuentran justo en el margen de separaci\u00f3n.</p> </li> <li> <p>En caso de que \\(y_i (\\mathbf{x}_i^T \\mathbf{w} + b) &gt; 1\\), entonces el punto estar\u00e1 fuera del margen y la restricci\u00f3n no ser\u00e1 activa, siendo \\(\\alpha_i = 0\\). En este caso no se tratar\u00e1 de un vector de soporte.</p> </li> </ul> <p>Es importante destacar que solo los puntos con \\(\\alpha_i &gt; 0\\) (vectores de soporte) contribuyen a la soluci\u00f3n. El resto de puntos no afectar\u00e1n al hiperplano. </p> <p>Podemos observar que \\(\\mathbf{w}\\) se obtendr\u00e1 como combinaci\u00f3n lineal de los vectores de soporte \\(\\mathbf{x}_i\\) (aquellos con \\(\\alpha_i &gt; 0\\)). El par\u00e1metro \\(b\\) se puede obtener resolviendo la condici\u00f3n de complementariedad para cualquiera de los vectores de soporte.</p>"},{"location":"02-svm/#margen-blando","title":"Margen blando","text":"<p>Todo lo anterior es v\u00e1lido bajo la suposici\u00f3n de que los datos son linealmente separables, pero si esto no se cumple entonces el problema primal no tendr\u00e1 soluci\u00f3n. </p> <p>Supongamos ahora que existe un solape entre los datos. Una forma de tratar con este solape es maximizar \\(M\\) permitiendo que algunos datos est\u00e9n en el lado incorrecto del margen, para lo cual se definen las variables \\(\\xi = (\\xi_1, \\xi_2, \\ldots, \\xi_N)\\), relajando la restricci\u00f3n del primal de la siguiente forma:</p> \\[ y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq 1 - \\xi_i \\quad \\forall i, \\xi_i \\geq 0 \\] <p>Podemos interpretar \\(\\xi_i\\) como la cantidad proporcional que permitimos que una predicci\u00f3n est\u00e9 en el lado incorrecto del margen (ver ). Si tenemos \\(\\xi_i &gt; 1\\) entonces la correspondiente predicci\u00f3n estar\u00eda mal clasificada, mientras que con valores \\(0 &lt; \\xi_i &lt; 1\\) estar\u00eda correctamente clasificada pero en el lado incorrecto del margen.</p> <p>Figura 2: Margen blando y variables \\(\\xi_i\\) </p> <p>Si acotamos el sumatorio \\(\\sum_{i=1}^N \\xi_i\\) a un valor constante, entonces estaremos acotando el n\u00famero m\u00e1ximo de errores de clasificaci\u00f3n de los datos de entrenamiento a dicha constante. Esto lo trasladaremos como una penalizaci\u00f3n a nuestra funci\u00f3n objetivo.</p>"},{"location":"02-svm/#forma-primal_1","title":"Forma primal","text":"<p>Al igual que hicimos en el caso con margen duro, describimos el problema como una soluci\u00f3n de programaci\u00f3n cuadr\u00e1tica utilizando multiplicadores de Lagrange, en este caso introduciendo las variables \\(\\xi_i\\), con la siguiente funci\u00f3n objetivo:</p> \\[ \\begin{align*} \\min_{ \\mathbf{w}, b} \\quad  &amp; \\frac{1}{2} \\mathbf{\\lVert w \\rVert}^2 + C \\sum^N_{i=1} \\xi_i \\\\ \\text{s.a.} \\quad &amp; \\xi_i \\geq 0,\\ y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq 1 - \\xi_i, \\quad \\forall i = 1, \\ldots, N \\end{align*} \\] <p>Podemos ver que el par\u00e1metro \\(C\\) grad\u00faa la penalizaci\u00f3n de las variables \\(\\xi_i\\). Cuanto m\u00e1s alto sea \\(C\\), m\u00e1s penalizar\u00e1 cada punto fuera del margen. En el caso extremo, con \\(C=\\infty\\) equivaldr\u00eda al caso con margen duro y no se permitir\u00eda ning\u00fan punto en el lado incorrecto del margen.</p> <p>La funci\u00f3n de Langrange primal en este caso es:</p> \\[ L_P(\\mathbf{w}, b, \\xi, \\alpha, \\mu) = \\frac{1}{2} \\lVert \\mathbf{w} \\rVert^2 + C \\sum^N_{i=1} \\xi_i - \\sum_{i=1}^N \\alpha_i [ y_i ( \\mathbf{x}_i^T \\mathbf{w} + b ) - (1-\\xi_i) ] - \\sum_{i=1}^N \\mu_i \\xi_i \\] <p>Tendremos que minimizar esta funci\u00f3n respecto a \\(\\mathbf{w}\\), \\(b\\) y \\(\\xi_i\\), por lo que igualaremos las correspondientes derivadas a \\(0\\):</p> \\[ \\begin{align*} \\frac {\\delta L_P(\\mathbf{w}, b, \\xi, \\alpha, \\mu)}{\\delta \\mathbf{w}} &amp;= \\mathbf{w} - \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i = 0 &amp; \\Rightarrow \\mathbf{w} = \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i  \\\\ \\frac {\\delta L_P(\\mathbf{w}, b, \\xi, \\alpha, \\mu)}{\\delta b} &amp;= \\sum_{i=1}^N \\alpha_i y_i  = 0  &amp; \\Rightarrow 0 = \\sum_{i=1}^N \\alpha_i y_i   \\\\ \\frac {\\delta L_P(\\mathbf{w}, b, \\xi, \\alpha, \\mu)}{\\delta \\xi_i} &amp;= C - \\alpha_i - \\mu_i = 0 &amp; \\Rightarrow \\alpha_i = C - \\mu_i \\end{align*} \\]"},{"location":"02-svm/#forma-dual_1","title":"Forma dual","text":"<p>Sustituyendo las derivadas anteriores en la funci\u00f3n primal, obtenemos la forma dual:</p> \\[ \\begin{align*} \\max_\\alpha \\quad &amp; L_D(\\alpha) =  \\sum_{i=1}^N \\alpha_i - \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j  \\\\ s.a.\\quad  &amp; \\sum_{i=1}^N \\alpha_i y_i  = 0  \\\\ &amp;0 \\leq \\alpha_i \\leq C \\quad  \\forall i=1, \\ldots, N \\end{align*} \\] <p>La funci\u00f3n \\(L_D\\) nos da una cota inferior de la funci\u00f3n objetivo para cualquier punto viable, por lo que buscaremos maximizarla. </p> <p>Adem\u00e1s, se deben cumplir las diferentes condiciones KKT:</p> <ol> <li> <p>Estacionariedad. Se cumple habiendo igualado las derivadas a \\(0\\).</p> </li> <li> <p>Factibilidad. Deben cumplirse las restricciones originales del problema primal: $$  y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq 1 - \\xi_i \\quad \\forall i \\ \\xi_i \\geq 0 \\quad \\forall i $$</p> </li> <li> <p>Signo. Los multiplicadores asociados a restricciones de desigualdad no deben ser negativos: $$  \\alpha_i \\geq 0 \\quad \\forall i \\ \\mu_i \\geq 0 \\quad \\forall i $$</p> </li> <li> <p>Complementariedad. Esta es la m\u00e1s importante a tener en cuenta, ya que define qu\u00e9 restricciones son activas (aquellas con par\u00e1metros \\(\\alpha_i &gt; 0\\) y \\(\\mu_i &gt; 0\\)), indicando de esta forma cu\u00e1les son los vectores de soporte.  $$ \\alpha_i[y_i(\\mathbf{x}_i^T \\mathbf{w} + b) - 1 + \\xi_i] = 0 \\quad \\forall i \\ \\mu_i \\xi_i = 0 \\Rightarrow (C-\\alpha_i) \\xi_i = 0 \\quad \\forall i  $$</p> </li> </ol> <p>Podemos distinguir varios casos:</p> <ul> <li> <p>\\(\\alpha_i = 0\\). Son puntos correctamente clasificados, que no son vectores de soporte. En este caso siempre tendremos \\(\\xi_i = 0\\) debido a las condiciones de complementariedad.</p> </li> <li> <p>\\(0 &lt;  \\alpha_i &lt; C\\). Estos son los vectores de soporte que se sit\u00faan exactamente en el margen. En estos casos \\(\\mu_i &gt; 0\\), y por lo tanto \\(\\xi_i = 0\\), por lo que no hay violaci\u00f3n del margen.</p> </li> <li> <p>\\(\\alpha_i = C\\). En este caso tenemos vectores de soporte que violan el margen. En estos casos \\(\\mu_i = 0\\), por lo que podemos tener \\(\\xi_i &gt; 0\\). Teniendo en cuenta que se debe cumplir \\(y_i (\\mathbf{x}^T_i \\mathbf{w} + b) = 1 - \\xi_i\\), si \\(0 &lt; \\xi_i &lt; 1\\) entonces el vector viola el margen pero estar\u00e1 bien clasificado, mientras que en caso de que \\(\\xi &gt; 1\\) entonces estar\u00e1 mal clasificado.</p> </li> </ul> <p>Una vez resuelto el problema de optimizaci\u00f3n y obtenidos los \\(\\alpha_i\\) \u00f3ptimos, podemos observar que los coeficientes \\(\\mathbf{w}\\) se obtendr\u00edan como combinaci\u00f3n lineal \u00fanicamente de los vectores de soporte (entradas \\(\\mathbf{x_i}\\) para las que \\(\\alpha_i &gt; 0\\)):</p> \\[ \\mathbf{w} = \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i  \\] <p>Una vez obtenidos los coeficientes, para despejar \\(b\\), podemos utilizar cualquiera de los puntos del margen (\\(\\alpha_i &gt; 0\\), \\(\\xi_i = 0\\)) en la primera ecuaci\u00f3n de la restricci\u00f3n de complementariedad, aunque habitualmente se suele hacer una media de la estimaci\u00f3n de todos ellos para tener una mayor estabilidad num\u00e9rica. </p>"},{"location":"02-svm/#efecto-del-parametro-c","title":"Efecto del par\u00e1metro C","text":"<p>Es importante entender el rol del par\u00e1metro \\(C\\):</p> <ul> <li>Con valores altos de \\(C\\), se penalizar\u00e1n \\(\\xi_i\\) positivos, y podremos tender al overfitting. </li> <li>Por el contrario, con valores bajos de \\(C\\) se tender\u00e1 a valores peque\u00f1os de \\(\\lVert \\mathbf{w} \\rVert\\), lo que causar\u00e1 que la frontera sea m\u00e1s suave (ampliando el margen).</li> </ul>"},{"location":"02-svm/#primal-vs-dual","title":"Primal VS Dual","text":"<p>Como hemos visto, SVM lineal puede resolverse de ambas formas. Son dos formas complementarias que resuelven el mismo problema. La clave est\u00e1 en que en cada caso cambian las variables a optimizar. En el caso de la forma primal optimizamos directamente los par\u00e1metros del modelo \\((\\mathbf{w}, b)\\), donde \\(\\mathbf{w} \\in \\mathbb{R}^d\\) (siendo \\(d\\) el n\u00famero de features), mientras que en el caso de la forma dual estaremos optimizando los \\(N\\) multiplicadores \\(\\alpha_i\\) (uno para cada ejemplo de entrenamiento).</p> <p>Por lo tanto, la conclusi\u00f3n m\u00e1s inmediata es que si \\(N \\gg d\\) convendr\u00eda utilizar la forma primal, mientras que en el caso de tener \\(N &lt; d\\) ser\u00eda preferible la forma dual. </p> <p>La implementaci\u00f3n SVCLinear de sklearn nos permite elegir entre utilizar la forma dual o la forma primal, e incluso nos permite dejar que la implementaci\u00f3n seleccione el problema autom\u00e1ticamente en funci\u00f3n del n\u00famero de samples, el n\u00famero de features y otros par\u00e1metros. Esta implementaci\u00f3n utiliza internamente como optimizador el m\u00e9todo Coordinate Descent (Hsieh et al., 2008)5, implementado en la librer\u00eda LIBLINEAR. Recordemos que es un m\u00e9todo similar a descenso por gradiente, pero en el que se seleccionan caracter\u00edsticas una a una. Se fijan todas las variables excepto una, se optimiza para esa variable, y repite iterativamente para cada variable, iterando hasta la convergencia. </p> <p>Por otro lado, en caso de contar con un extenso conjunto de datos, puede ser conveniente utilizar Descenso por Gradiente Estoc\u00e1stico. En este caso, contamos con SGDClassifier que nos permite especificar diferentes funciones de p\u00e9rdida para diferentes modelos lineales. Por defecto, utiliza la funci\u00f3n de p\u00e9rdida <code>hinge</code> que equivale a SVM Lineal (maximizar el margen es equivamente a minimizar el hinge loss). En este caso estaremos resolviendo siempre el problema primal (hemos de tener en cuenta que utilizaremos esta implementaci\u00f3n cuando el n\u00famero de samples sea muy grande). Si como funci\u00f3n de p\u00e9rdida utilizamos <code>log_loss</code> entonces tendremos un clasificador de regresi\u00f3n log\u00edstica.   </p> <p>Adem\u00e1s del criterio de n\u00famero de samples frente a n\u00famero de features, otra ventaja de la forma dual es la esparsidad de la soluci\u00f3n, ya que solo unos pocos puntos tienen \\(\\alpha_i &gt; 0\\) y contribuyen.</p> <p>Pero la ventaja m\u00e1s destacada de utilizar la forma lineal es que nos permite aplicar el conocido como Kernel trick, con el que podremos transformar el modelo en no lineal, e incluso en no param\u00e9trico.</p>"},{"location":"02-svm/#kernel-trick","title":"Kernel trick","text":"<p>Hemos visto como mediante ingenier\u00eda de caracter\u00edsticas podemos proyectar las caracter\u00edsticas originales \\(\\mathbf{x}\\) en un nuevo espacio de caracter\u00edsticas \\(h(\\mathbf{x})\\). El clasificador SVM presenta una extensi\u00f3n de esta idea, que nos permite que la dimensi\u00f3n del nuevo espacio de caracter\u00edsticas sea muy alta, e incluso infinita en algunos casos.</p> <p>Para que esto sea abordable, la idea es representar el problema de optimizaci\u00f3n de forma que las caracter\u00edsticas de entrada se presenten solo como producto escalar. </p> <p>Podemos representar la funci\u00f3n dual de la siguiente forma:</p> \\[ L_D(\\alpha) =  \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j \\langle h(\\mathbf{x}_i), h(\\mathbf{x}_j) \\rangle \\] <p>Donde hemos sustituido el producto escalar de \\(\\mathbf{x_i}^T \\mathbf{x_j}\\) por el producto escalar entre las caracter\u00edsticas transformadas. </p> <p>De esta forma, la funci\u00f3n del hiperplano de separaci\u00f3n quedar\u00eda expresada de la siguiente forma:</p> \\[ f(x) = h(\\mathbf{x})^T \\mathbf{w} + b  \\] <p>Recordando que los pesos se calculan como una combinaci\u00f3n lineal de los vectores de soporte, tendr\u00edamos:</p> \\[ \\begin{align*} f(x) &amp;= h(\\mathbf{x})^T  \\sum_{i=1}^N \\alpha_i y_i h(\\mathbf{x}_i)  + b = \\\\ &amp;= \\sum_{i=1}^N \\alpha_i y_i \\langle h(\\mathbf{x}), h(\\mathbf{x}_i) \\rangle  + b \\end{align*} \\] <p>Podemos ver entonces que tanto en la formulaci\u00f3n del problema dual como en la funci\u00f3n de separaci\u00f3n soluci\u00f3n del problema las variables de entrada \\(h(\\mathbf{x})\\) est\u00e1n involucradas \u00fanicamente en forma de producto escalar, por lo que lo \u00fanico que necesitamos conocer de ellas es lo que conocemos como funci\u00f3n de Kernel:</p> \\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\langle h(\\mathbf{x}_i), h(\\mathbf{x}_j) \\rangle \\] <p>La funci\u00f3n \\(K\\) produce el producto escalar de las caracter\u00edsticas en el espacio transformado, y solo necesitamos conocer esta funci\u00f3n, no hace falta que trabajemos en el espacio transformado. Esto es lo que se conoce como Kernel trick (Boser et al., 1992)6. Hay que destacar que la aplicaci\u00f3n de este Kernel trick solo es posible con la formulaci\u00f3n del problema dual. Por lo tanto, en la implementaci\u00f3n SVC de sklearn siempre se resolver\u00e1 el problema dual, permitiendo de esta forma el uso de diferentes Kernels.</p> <p>Utilizando diferentes funciones de Kernel podremos aplicar de forma sencilla y eficiente diferentes transformaciones del espacio de caracter\u00edsticas (ver ). Vamos a ver los Kernels m\u00e1s comunes.</p> <p>Figura 3: Comparativa de SVM con diferentes Kernels aplicados a datos no separables linealmente: lineal (izquierda), polinomial (centro) y RBF (derecha) </p>"},{"location":"02-svm/#kernel-lineal","title":"Kernel lineal","text":"<p>Generalizando el uso de los Kernels, podemos ver que el modelo SVM lineal que hemos estudiado hasta el momento se podr\u00eda considerar un caso particular en el que se utiliza el siguiente Kernel:</p> \\[ K(\\mathbf{x}_i, \\mathbf{x}_j) =\\mathbf{x}_i^T \\mathbf{x}_j \\] <p>Este Kernel podr\u00e1 resultar adecuado cuando sepamos que los datos son linealmente separables o en casos en los que tengamos alta dimensionalidad. Este tipo de Kernel facilita la interpretabilidad.</p>"},{"location":"02-svm/#kernel-polinomial","title":"Kernel polinomial","text":"<p>Est\u00e1 relacionado con el uso de caracter\u00edsticas polinomiales, pero tiene la ventaja de que no es necesario definir las caracter\u00edsticas expl\u00edcitamente, sino que se apoya en el Kernel trick para obtener una mayor eficiencia. Tiene la siguiente forma:</p> \\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma  \\mathbf{x}_i^T \\mathbf{x}_j + r) ^d \\] <p>Donde \\(d\\) es el grado del polinomio, \\(\\gamma\\) controla la influencia de cada sample en la frontera de decisi\u00f3n y \\(r\\) es un t\u00e9rmino de sesgo en el polinomio. </p> <p>En este caso, el Kernel est\u00e1 mapeando las caracter\u00edsticas a un espacio de mayor dimensionalidad, pero la dimensi\u00f3n sigue siendo finita y fija, por lo que seguimos teniendo un modelo param\u00e9trico. En este caso la frontera de decisi\u00f3n puede que ya no sea lineal en el espacio original de caracter\u00edsticas (aunque lo seguir\u00e1 siendo en el transformado), tal como hemos visto anteriormente en el caso de ingenier\u00eda de caracter\u00edsticas.</p> <p>En la  podemos ver el efecto del par\u00e1metro de grado \\(d\\) en el Kernel polinomial. Con un mayor grado podemos tener fronteras m\u00e1s complejas, pero tambi\u00e9n mayor riesgo de overfitting.</p> <p>Figura 4: Efecto del grado en el Kernel polinomial </p>"},{"location":"02-svm/#kernel-radial-basis-function-rbf","title":"Kernel Radial Basis Function (RBF)","text":"<p>Se trata de uno de los Kernels m\u00e1s utilizados, y permite representar relaciones no lineales complejas. Se basa en crear campanas de similaridad alrededor de los puntos, generando algo parecido a un mapa de calor. Tiene la siguiente forma:</p> \\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = e^{- \\gamma \\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert^2} \\] <p>El par\u00e1metro \\(\\gamma\\) controla la influencia de cada sample en la frontera de decisi\u00f3n, permitiendo as\u00ed ajustar la suavidad de la frontera. </p> <p>En este caso, el Kernel Gaussiano mapea a un espacio de dimensi\u00f3n infinita.  Intuitivamente, esto significa que crea una funci\u00f3n base (una 'campana gaussiana') centrada en cada punto del conjunto de datos, permitiendo representar fronteras de decisi\u00f3n arbitrariamente complejas. El modelo seleccionar\u00e1 como vectores de soporte \u00fanicamente aquellos puntos necesarios para definir la frontera, lo que permite que la complejidad del modelo se adapte autom\u00e1ticamente a los datos de entrada. Por ello, en este caso el modelo pasa a ser no param\u00e9trico.</p> <p>En la figura  podemos ver el efecto del par\u00e1metro \\(\\gamma\\) en el Kernel RBF. Con valores altos de este par\u00e1metro (\\(\\gamma = 10\\)) podemos observar fronteras muy irregulares y un mayor n\u00famero de vectores de soporte, indicando tendencia al overfitting. Por el contrario, con valores bajos de \\(\\gamma\\) (por ejemplo \\(\\gamma=0.1\\)) se obtienen fronteras m\u00e1s suaves que generalizan mejor, aunque si es demasiado bajo podr\u00eda causar underfitting.  </p> <p>Figura 5: Efecto del par\u00e1metro \\(\\gamma\\) en el Kernel RBF </p>"},{"location":"02-svm/#kernel-sigmoide","title":"Kernel sigmoide","text":"<p>Tambi\u00e9n conocido como Kernel neural network, tiene un comportamiento similar a redes neuronales de una capa, y tiene la siguiente forma:</p> \\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh (\\gamma  \\mathbf{x}_i^T \\mathbf{x}_j + r) \\] <p>Donde, al igual que en los casos anteriores, \\(\\gamma\\) controla la influencia de cada sample en la frontera de decisi\u00f3n y \\(r\\) es un t\u00e9rmino de sesgo que desplaza los datos en una direcci\u00f3n y otra.</p> <p>Es menos utilizado actualmente, y su uso se limita a casos muy concretos en los que los datos tienen una forma sigmoidal. </p> <ol> <li> <p>Cortes, C., &amp; Vapnik, V. (1995). Support-vector networks. Machine Learning, 20(3), 273--297. https://doi.org/10.1007/BF00994018 \u21a9</p> </li> <li> <p>Boyd, S., &amp; Vandenberghe, L. (2004). Convex optimization. Cambridge University Press.\u00a0\u21a9\u21a9</p> </li> <li> <p>Platt, J. C. (1998). Sequential minimal optimization: A fast algorithm for training support vector machines (Nos. MSR-TR-98-14). Microsoft Research.\u00a0\u21a9</p> </li> <li> <p>Kuhn, H. W., &amp; Tucker, A. W. (1951). Nonlinear programming. Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability, 481--492.\u00a0\u21a9</p> </li> <li> <p>Hsieh, C.-J., Chang, K.-W., Lin, C.-J., Keerthi, S. S., &amp; Sundararajan, S. (2008). A dual coordinate descent method for large-scale linear SVM. Proceedings of the 25th International Conference on Machine Learning, 408--415.\u00a0\u21a9</p> </li> <li> <p>Boser, B. E., Guyon, I. M., &amp; Vapnik, V. N. (1992). A training algorithm for optimal margin classifiers. Proceedings of the Fifth Annual Workshop on Computational Learning Theory, 144--152. https://doi.org/10.1145/130385.130401 \u21a9</p> </li> </ol>"},{"location":"03-arboles-decision/","title":"3. \u00c1rboles de decisi\u00f3n","text":""},{"location":"03-arboles-decision/#arboles-de-decision","title":"\u00c1rboles de decisi\u00f3n","text":"<p>Pr\u00f3ximamente</p>"}]}