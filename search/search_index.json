{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introducci\u00f3n","text":""},{"location":"#aprendizaje-avanzado","title":"Aprendizaje Avanzado","text":"<p>Este material se proporciona como apuntes para la asignatura Aprendizaje Avanzado del Grado en Ingenier\u00eda en Inteligencia Artificial de la Universidad de Alicante. </p>"},{"location":"#contexto-de-la-asignatura","title":"Contexto de la asignatura","text":"<p>La asignatura Aprendizaje Avanzado forma parte de la materia Aprendizaje Autom\u00e1tico  junto con Fundamentos del Aprendizaje Autom\u00e1tico y Redes Neuronales y Aprendizaje Profundo. Se imparte en el segundo cuatrimestre del tercer curso, como continuaci\u00f3n natural de Fundamentos del Aprendizaje Autom\u00e1tico y de forma paralela con Redes Neuronales y Aprendizaje Profundo.</p> <p>Una vez establecidas las bases del aprendizaje autom\u00e1tico en la asignatura de fundamentos, Aprendizaje Avanzado se centra en proporcionar una visi\u00f3n amplia de los diferentes tipos de modelos existentes, buscando adquirir la capacidad de seleccionar, integrar, optimizar y evaluar m\u00faltiples t\u00e9cnicas para resolver problemas complejos del mundo real.</p> <p>Se recomienda haber cursado previamente Fundamentos del Aprendizaje Autom\u00e1tico.</p>"},{"location":"#planificacion","title":"Planificaci\u00f3n","text":"<p>La asignatura se estructura en cuatro grandes bloques:</p> <ul> <li>Bloque I. Aprendizaje supervisado</li> <li>Bloque II. Aprendizaje no supervisado</li> <li>Bloque III. Aprendizaje por refuerzo</li> <li>Bloque IV. Aprendizaje aplicado a problemas del mundo real</li> </ul> <p>A continuaci\u00f3n se detalla la planificaci\u00f3n de sesiones de la asignatura:</p> Fecha Teor\u00eda Pr\u00e1cticas S1 (26 de enero) I.1 Modelos param\u00e9tricos y no param\u00e9tricos. Modelos lineales. Regresi\u00f3n Log\u00edstica. P0. Datos y visualizaci\u00f3n S2 (3 de febrero) I.2 SVM y kernel trick P0. Datos y visualizaci\u00f3n S3 (10 de febrero) * Charla mes cultural EPS P1. Aprendizaje supervisado S4 (17 de febrero) I.3 \u00c1rboles de decisi\u00f3n P1. Aprendizaje supervisado S5 (24 de febrero) I.4 M\u00e9todos de ensamble. Bagging. Random Forest P1. Aprendizaje supervisado S6 (3 de marzo) I.5 Boosting. AdaBoost. Control 1 P1. Aprendizaje supervisado S7 (10 de marzo) I.6 Gradient Boosting. XGBoost P1. Aprendizaje supervisado S8 (17 de marzo) II.7 Clustering. GMM y EM P2. Aprendizaje no supervisado S9 (24 de marzo) II.8 DBSCAN P2. Aprendizaje no supervisado S10 (31 de marzo) II.9 Clustering espectral. Control 2 P2. Aprendizaje no supervisado S11 (21 de abril) III.10 Aprendizaje por refuerzo P3. Aprendizaje por refuerzo S12 (28 de abril) III.11 Aprendizaje por refuerzo P3. Aprendizaje por refuerzo S13 (5 de mayo) IV.12 Selecci\u00f3n y optimizaci\u00f3n de modelos P3. Aprendizaje por refuerzo S14 (12 de mayo) IV.13 Aplicaci\u00f3n a problemas del mundo real. Control 3 P3. Aprendizaje por refuerzo S15 (19 de mayo) Presentaci\u00f3n de proyectos Presentaci\u00f3n de proyectos"},{"location":"#licencia","title":"Licencia","text":"<p>\u00a9 2026 Departamento de Ciencia de la Computaci\u00f3n e Inteligencia Artificial. Universidad de Alicante</p> <p> </p> <p>Este material est\u00e1 disponible bajo licencia Creative Commons Attribution 4.0 International.  Esto significa que puedes usar, compartir y adaptar estos apuntes libremente, siempre que des el cr\u00e9dito apropiado.</p>"},{"location":"01-modelos-no-param/","title":"1. Modelos param\u00e9tricos y no param\u00e9tricos","text":""},{"location":"01-modelos-no-param/#sesion-1-modelos-parametricos-y-no-parametricos","title":"Sesi\u00f3n 1: Modelos param\u00e9tricos y no param\u00e9tricos","text":"<p>Los modelos no param\u00e9tricos, a diferencia de los modelos param\u00e9tricos, son una familia de modelos que no asumen una forma funcional fija. Una implicaci\u00f3n importante de este hecho es que la complejidad del modelo puede crecer en funci\u00f3n del conjunto de datos. </p> <p>En un modelo param\u00e9trico se define de antemano la forma funcional, que podr\u00eda ser por ejemplo lineal o cuadr\u00e1tica, y tendremos que estimar un n\u00famero fijo de par\u00e1metros, pero si la estructura de los datos es m\u00e1s compleja y no conocemos su forma, puede que no puedan ajustarse de forma adecuada al modelo.</p> <p>Los modelos no param\u00e9tricos no imponen una estructura fija, sino que permiten que sean los propios datos los que determinen la complejidad del modelo. </p> <p>Desde el punto de vista del compromiso (trade-off) entre sesgo (bias) y varianza, podemos intuir que los modelos param\u00e9tricos tender\u00e1n a tener un mayor sesgo, si el modelo no es lo suficientemente complejo como para poder representar los datos (riesgo de underfitting), mientras que los modelos no param\u00e9tricos tender\u00e1n a tener menor sesgo pero mayor varianza (riesgo de overfitting), siendo sensibles a los cambios en los datos de entrenamiento.</p> <p>Vamos a ilustrarlo a continuaci\u00f3n mediante un clasificador param\u00e9trico sencillo.</p>"},{"location":"01-modelos-no-param/#modelos-lineales","title":"Modelos lineales","text":"<p>Una familia caracter\u00edstica de modelos param\u00e9tricos son los modelos lineales, que son aquellos que asumen una relaci\u00f3n lineal entre los datos de entrada y la variable objetivo.</p> <p>El modelo lineal predice la salida como una combinaci\u00f3n lineal ponderada de las variables de entrada, con la siguiente forma:</p> \\[ \\hat{y} = w_1 x_1 + w_2 x_2 + \\ldots + w_d x_d = \\mathbf{x}^T \\mathbf{w} + b \\] <p>Donde \\(\\mathbf{x}\\) es el vector de caracter\u00edsticas de entrada, \\(\\mathbf{w}\\) es el vector de coeficientes que el modelo aprende, \\(b\\) es el sesgo o desplazamiento y \\(\\hat{y}\\) la predicci\u00f3n que nos da el modelo. </p> <p>Desde el punto de vista geom\u00e9trico, esta ecuaci\u00f3n define un hiperplano. Con esto, podemos dar la siguiente interpretaci\u00f3n al funcionamiento de estos modelos:</p> <ul> <li>Tareas de regresi\u00f3n: Se busca el hiperplano que mejor se ajusta a los datos de entrada.</li> <li>Tareas de clasificaci\u00f3n: Se busca el hiperplano que mejor separa los datos de dos clases. </li> </ul> <p>Vamos a centrarnos a continuaci\u00f3n en la tarea de clasificaci\u00f3n, y estudiaremos dos de los principales m\u00e9todos lineales para clasificaci\u00f3n: Regresi\u00f3n Log\u00edstica y SVM (Hastie et al., 2009)1. </p>"},{"location":"01-modelos-no-param/#hiperplanos-y-clasificacion","title":"Hiperplanos y clasificaci\u00f3n","text":"<p>En un problema de clasificaci\u00f3n, si contamos con datos linealmente separables podremos encontrar un hiperplano que nos permita clasificarlos sin errores. El hiperplano tendr\u00e1 la siguiente forma:</p> \\[ \\mathbf{x^T} \\mathbf{w} + b = 0 \\] <p>Donde sus par\u00e1metros son \\(\\mathbf{w}\\), que representa un vector perpendicular al hiperplano, y \\(b\\), como t\u00e9rmino de desplazamiento. </p> <p>Para facilitar la visualizaci\u00f3n, vamos a considerar el caso concreto de dos dimensiones, donde el hiperplano anterior ser\u00eda una l\u00ednea, con la siguiente ecuaci\u00f3n:</p> \\[ w_1 x_1 + w_2 x_2 + b = 0 \\ \\] <p>Por ejemplo, si consideramos como par\u00e1metros el vector \\(\\mathbf{w} = [0.45, 0.89]\\) y \\(b = -2\\), tendremos la siguiente recta:</p> <p>Figura 1: Ejemplo de hiperplano para separar dos conjuntos de datos </p> <p>En la  podemos observar que si el vector \\(\\mathbf{w}\\) es unitario, como en el ejemplo anterior, entonces el t\u00e9rmino \\(b\\) coincide con la distancia del plano al origen. En el caso general en el que \\(\\mathbf{w}\\) no es unitario, la distancia al origen ser\u00e1 \\(|b| / \\lVert \\mathbf{w} \\rVert\\).  </p> <p>La ecuaci\u00f3n del hiperplano nos proporciona una forma sencilla de clasificar los puntos seg\u00fan se encuentren a uno u otro lado, tomando dicha ecuaci\u00f3n como funci\u00f3n:</p> \\[ f(\\mathbf{\\mathbf{x}}) = \\mathbf{x^T} \\mathbf{w} + b \\] <p>En la funci\u00f3n \\(f(\\mathbf{x})\\), todos los puntos que pertenezcan al hiperplano nos dar\u00e1n \\(f(\\mathbf{x})=0\\) (cumplir\u00edan la ecuaci\u00f3n del hiperplano), pero lo que realmente nos interesa es que todos los puntos que est\u00e9n al lado al que apunta el vector \\(\\mathbf{w}\\) har\u00e1n que \\(f(\\mathbf{x})\\) tenga signo positivo, mientras que los que est\u00e9n al lado contrario har\u00e1n que tenga signo negativo.  Es decir, podemos utilizar el signo de dicha funci\u00f3n como clasificador:</p> \\[ G(\\mathbf{x}) = signo[f(\\mathbf{x})] \\] <p>De esta forma, diferentes puntos de datos quedar\u00edan clasificados tal como se muestra en la </p> <p>Figura 2: Ejemplo de clasificaci\u00f3n con un hiperplano </p> <p>Adem\u00e1s, en caso de que \\(\\mathbf{w}\\) sea unitario, la funci\u00f3n \\(f(\\mathbf{\\mathbf{x}})\\) nos dar\u00e1 la distancia (con signo) desde cada punto al plano. Podemos ver esto de forma intuitiva, considerando que el producto escalar \\(\\mathbf{x^T} \\mathbf{w}\\) nos da la proyecci\u00f3n de \\(\\mathbf{x}\\) sobre el vector \\(\\mathbf{w}\\) perpendicular al hiperplano. Esto nos dar\u00e1 la distancia al hiperplano si pasara por el origen, y el t\u00e9rmino \\(b\\) introduce un desplazamiento.</p> <p>Existen diferentes m\u00e9todos que nos permiten aprender, a partir de un conjunto de datos, un hiperplano que separe los datos de dos clases. </p>"},{"location":"01-modelos-no-param/#regresion-logistica","title":"Regresi\u00f3n Log\u00edstica","text":"<p>Aunque el nombre pueda resultar confuso, se trata de un m\u00e9todo de clasificaci\u00f3n, y no de regresi\u00f3n. Si bien con el m\u00e9todo de regresi\u00f3n lineal se busca ajustar un hiperplano a un conjunto de puntos, de forma que se minimice la distancia entre los puntos del conjunto de entrada y el hiperplano, en el caso de la regresi\u00f3n log\u00edstica buscamos el hiperplano que mejor separe dos clases de datos. Hablamos en este caso de regresi\u00f3n log\u00edstica binomial, en la que contamos \u00fanicamente con dos clases, aunque tambi\u00e9n podr\u00edamos extender este m\u00e9todo a un mayor n\u00famero de clases, hablando en este caso de regresi\u00f3n log\u00edstica multinomial. </p> <p>Vamos a centrarnos de momento por simplicidad en el caso binomial, en el que la salida \\(y\\) podr\u00e1 tomar dos posibles valores \\(\\{0, 1\\}\\). Los ejemplos ser\u00e1n clasificados en una clase u otra seg\u00fan el lado del hiperplano en el que se sit\u00faen.   </p> <p>Como hemos visto anteriormente, a partir de la ecuaci\u00f3n del hiperplano podemos determinar si un punto est\u00e1 a uno u otro lado a partir del signo de la funci\u00f3n \\(f(x)\\), siendo positiva (clase \\(1\\)) en el lado hacia el que apunta el vector \\(\\mathbf{w}\\), y negativa (clase \\(0\\)) en el otro lado. </p> <p>Este modelo destaca por su interpretabilidad, y es ampliamente utilizado como modelo base en numerosos problemas de clasificaci\u00f3n. </p> <p>La clave principal de la regresi\u00f3n logistica consiste en aplicar sobre la funci\u00f3n anterior la funci\u00f3n sigmoide, modelando mediante esta funci\u00f3n la probabilidad de pertenencia a cada clase.</p>"},{"location":"01-modelos-no-param/#funcion-sigmoide","title":"Funci\u00f3n sigmoide","text":"<p>La funci\u00f3n sigmoide \\(\\sigma(z)\\) tiene la siguiente forma:</p> \\[ \\sigma(z) = \\frac{1}{1+ e^{-z}} \\] <p>Podemos verla representada en la .</p> <p>Figura 3: Forma de la funci\u00f3n sigmoide </p> <p>Como podemos observar, esta funci\u00f3n presenta una transici\u00f3n suave desde \\(0\\) (cuando \\(z \\rightarrow -\\infty\\)) hasta \\(1\\) (cuando \\(z \\rightarrow \\infty\\)), teniendo su punto medio en \\(\\sigma(0) = 0.5\\). </p> <p>La funci\u00f3n es siempre creciente y derivable, lo cual es una propiedad importante para la optimizaci\u00f3n.</p> <p>La funci\u00f3n sigmoide tiene la siguiente derivada:</p> \\[ \\sigma'(z) = \\sigma(z) (1 - \\sigma(z)) \\] <p>Esta forma simplifica mucho el c\u00e1lculo del gradiente.</p> <p>Podemos sustituir \\(z\\) por nuestra funci\u00f3n \\(f(\\mathbf{x})\\) que nos permite clasificar los puntos en funci\u00f3n del signo, teniendo:</p> \\[ \\sigma(f(\\mathbf{x})) =  \\frac{1}{1 + e^{-f(\\mathbf{x})}} = \\frac{1}{1 + e^{-(\\mathbf{x}^T \\mathbf{w} + b)}} \\] <p>Podemos interpretar la funci\u00f3n anterior como la probabilidad estimada de que \\(y = 1\\) (es decir, de que pertenezca a la clase positiva). Definimos de esta forma:</p> \\[ p_i = \\sigma(f(\\mathbf{x}_i)) = \\frac{1}{1 + e^{-(\\mathbf{x}_i^T \\mathbf{w} + b)}} \\] <p>Donde \\(p_i\\) define la probabilidad estimada de que el ejemplo \\(i\\) pertenezca a la clase \\(1\\). Tendremos por lo tanto:</p> \\[ \\begin{align*} P(y_i=1 | \\mathbf{x}_i) &amp;= p_i \\\\ P(y_i=0 | \\mathbf{x}_i) &amp;= 1 - p_i  \\end{align*} \\] <p>Considerando estos dos posibles valores para \\(y_i\\), podemos escribir ambos casos en una \u00fanica f\u00f3rmula, teniendo as\u00ed la verosimilitud de observar \\(y_i\\) cuando la entrada es \\(\\mathbf{x}_i\\):</p> \\[ P(y_i | \\mathbf{x}_i) = (p_i)^y (1 - p_i)^{1-y}  \\] <p>Estimaremos los par\u00e1metros mediante m\u00e1xima verosimilitud, lo cual equivale a minimizar la p\u00e9rdida logar\u00edtmica (log-loss).</p>"},{"location":"01-modelos-no-param/#funcion-de-coste","title":"Funci\u00f3n de coste","text":"<p>La funci\u00f3n de p\u00e9rdida logar\u00edtmica (log-loss) o binary cross-entropy para una sola muestra tiene la siguiente forma:</p> \\[ L(p_i, y_i) = -y_i \\log (p_i) - (1-y_i) log(1-p_i) \\] <p>Esta funci\u00f3n tiene la propiedad de que penaliza fuertemente las predicciones confiadas pero incorrectas. Es decir, si la salida esperada es \\(y_i=1\\) pero la predicci\u00f3n \\(p_i \\rightarrow 0\\), entonces la penalizaci\u00f3n ser\u00e1 alta.</p> <p>Consideremos ahora que tenemos un conjunto de entrenamiento con \\(N\\) pares \\((\\mathbf{x_i}, y_i)\\) con \\(\\mathbf{x_i} \\in \\mathbb{R}^d\\) y \\(y_i \\in \\{0, 1\\}\\) (problema binomial), siendo \\(d\\) el n\u00famero de features.</p> <p>Con todo ello, para el conjunto de muestras podemos construir la siguiente funci\u00f3n de coste:</p> \\[ J(\\mathbf{w}) = -\\frac{1}{N} \\sum_{i=1}^N [y_i \\log (p_i) + (1-y_i) \\log (1 - p_i)] \\] <p>Esta funci\u00f3n tiene la propiedad de que es convexa (tiene un \u00fanico m\u00ednimo global) y diferenciable, y como hemos comentado, penaliza las predicciones claramente incorrectas.</p>"},{"location":"01-modelos-no-param/#optimizacion","title":"Optimizaci\u00f3n","text":"<p>Buscamos encontrar los pesos \\(\\mathbf{w}\\) que minimicen la funci\u00f3n de coste anterior. </p> \\[ \\mathbf{\\hat{w}} = \\arg \\min_{\\mathbf{w}} J(\\mathbf{w})  \\] <p>Obtenemos el gradiente de la funci\u00f3n, mediante la derivada parcial respecto a cada peso \\(w_j\\):</p> \\[ \\frac{\\partial J(\\mathbf{w})}{\\partial w_j} = \\frac{1}{N} \\sum_{i=1}^N (p_i - y_i) x_{ij} \\] <p>Podemos expresar el gradiente en forma vectorial, para todos los pesos, de la siguiente forma:</p> \\[ \\nabla J(\\mathbf{w}) = \\frac{1}{N} \\mathbf{X}^T (\\mathbf{p} - \\mathbf{y})  \\] <p>Donde \\(\\mathbf{X}\\) es una matriz de dimensi\u00f3n \\(N \\times d\\) (una fila para cada ejemplo de entrada), \\(\\mathbf{p}\\) es un vector de predicciones (\\(p_i\\)) de dimensi\u00f3n \\(N \\times 1\\) y \\(\\mathbf{y}\\) es un vector de etiquetas de dimensi\u00f3n \\(N \\times 1\\). </p> <p>De la misma forma, podemos obtener la derivada parcial respecto al sesgo (\\(b\\)):</p> \\[ \\frac{\\partial J(\\mathbf{w})}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^N (p_i - y_i) \\] <p>Con esto, podremos aplicar Descenso por Gradiente o Descenso por Gradiente estoc\u00e1stico (SGD) para optimizar los pesos. Tambi\u00e9n tenemos otros algoritmos de optimizaci\u00f3n como Coordinate Descent (Hsieh et al., 2008)2, en el que en lugar de aplicar descenso por gradiente a la vez sobre todas las coordenadas, se selecciona de forma iterativa una coordenada, se congela el resto, y se optimiza para la coordenada seleccionada. El algoritmo itera por las diferentes coordenadas hasta la convergencia. Encontramos tambi\u00e9n otros m\u00e9todos de optimizaci\u00f3n, como el m\u00e9todo de Newton (Nocedal &amp; Wright, 2006)3 que utiliza segundas derivadas y presenta la ventaja de una convergencia m\u00e1s r\u00e1pida, aunque resulta algo costoso. Tenemos tambi\u00e9n L-BFGS (Liu &amp; Nocedal, 1989)4 que es una aproximaci\u00f3n eficiente del m\u00e9todo de Newton y es el utilizado por defecto en la implementaci\u00f3n de <code>LogisticRegression</code> en sklearn. Esta implementaci\u00f3n  incluye diferentes solvers alternativos que podemos utilizar para la optimizaci\u00f3n. </p>"},{"location":"01-modelos-no-param/#regularizacion","title":"Regularizaci\u00f3n","text":"<p>Podemos a\u00f1adir a la funci\u00f3n de coste un t\u00e9rmino de penalizaci\u00f3n para prevenir el overfitting y controlar la complejidad del modelo. Encontramos diferentes tipos de regularizaci\u00f3n:</p>"},{"location":"01-modelos-no-param/#regularizacion-l2-ridge","title":"Regularizaci\u00f3n L2 (Ridge)","text":"<p>Busca penalizar pesos grandes, para favorecer soluciones m\u00e1s simples:</p> \\[ \\begin{align*} J(\\mathbf{w}) = &amp;-\\frac{1}{N} \\sum_{i=1}^N [y_i \\log (p_i) + (1-y_i) \\log (1 - p_i)] + \\\\ &amp; + \\frac{\\lambda}{2N} \\sum_{j=1}^d w_j^2 \\end{align*} \\]"},{"location":"01-modelos-no-param/#regularizacion-l1-lasso","title":"Regularizaci\u00f3n L1 (Lasso)","text":"<p>Favorece que algunos pesos puedan ser exactamente \\(0\\), actuando de esta forma como una selecci\u00f3n de caracter\u00edsticas:</p> \\[ \\begin{align*} J(\\mathbf{w}) = &amp;-\\frac{1}{N} \\sum_{i=1}^N [y_i \\log (p_i) + (1-y_i) \\log (1 - p_i)] + \\\\ &amp; + \\frac{\\lambda}{N} \\sum_{j=1}^d | w_j | \\end{align*} \\]"},{"location":"01-modelos-no-param/#regularizacion-elastic-net","title":"Regularizaci\u00f3n Elastic Net","text":"<p>Combina L1 y L2:</p> \\[ \\begin{align*} J(\\mathbf{w}) = &amp;-\\frac{1}{N} \\sum_{i=1}^N [y_i \\log (p_i) + (1-y_i) \\log (1 - p_i)] + \\\\ &amp; + \\frac{\\lambda_1}{N} \\sum_{j=1}^d | w_j | + \\frac{\\lambda_2}{2N} \\sum_{k=1}^d w_k^2  \\end{align*} \\] <p>En todos estos casos tenemos un hiper-par\u00e1metro \\(\\lambda\\) con el que podemos ajustar la regularizaci\u00f3n. Con \\(\\lambda = 0\\) no aplicamos regularizaci\u00f3n, con lo que tendremos mayor riesgo de overfitting, mientras que con valores muy altos podr\u00edamos tener mayor riesgo de underfitting. </p>"},{"location":"01-modelos-no-param/#clasificacion-multi-clase","title":"Clasificaci\u00f3n multi-clase","text":"<p>Hemos visto hasta ahora el caso binomial (2 clases), pero como hemos comentado, podemos aplicar Regresi\u00f3n Log\u00edstica tambi\u00e9n a problemas de clasificaci\u00f3n multi-clase. </p> <p>Consideramos ahora que debemos clasificar los ejemplos de entrada en \\(K\\) clases. Tenemos dos formas para hacer esto:</p> <ul> <li>One-vs-Rest (OvR): Creamos \\(K\\) clasificadores binarios, uno para cada clase.</li> <li>Multinomial: Se entrana un \u00fanico modelo que modela la distribuci\u00f3n de probabilidad sobre todas las clases. </li> </ul> <p>Vamos a ver cada uno de estos casos.</p>"},{"location":"01-modelos-no-param/#one-vs-rest","title":"One-vs-Rest","text":"<p>Podemos aplicar cualquier clasificador binario a problemas multiclase utilizando la estrategia One-vs-Rest (OvR). </p> <p>Esta estrategia consiste en crear \\(K\\) clasificadores binarios, uno para cada clase. Cada clasificador binario \\(k\\), con \\(k = 1, 2, \\ldots, K\\), clasifica los ejemplos en dos categor\u00edas: los que pertenecen a la clase \\(k\\) y los que pertenecen a cualquier de las otras clases.</p> <p>Para la predicci\u00f3n, se calcula la probabilidad de pertenencia con cada uno de los clasificadores \\(k = 1, 2, \\ldots, K\\). Cada clasificador \\(k\\) nos dar\u00e1 la probabilidad de que el ejemplo pertenezca a la correspondiente clase \\(k\\):</p> \\[ P(y=k | \\mathbf{x}) = \\frac{1}{1 + e^{-(\\mathbf{x}^T \\mathbf{w}_k + b_k)}} \\] <p>Aquel que obtenga una mayor probabilidad ser\u00e1 la clase seleccionada como predicci\u00f3n:</p> \\[ \\hat{k} = \\arg \\max_k P(y=k | \\mathbf{x}) \\] <p>En sklearn podemos utilizar este enfoque utilizando la clase OneVsRestClassifier. Podemos aplicar este m\u00e9todo a cualquier clasificador binario. Tambi\u00e9n encontramos OneVsOneClassifier que entrena un clasificador para cada par de clases. Aunque One-vs-One (OvO) es m\u00e1s robusto frente a desbalanceo que OvR, requiere entrenar \\(K(K-1)/2\\) clasificadores, lo que puede ser prohibitivo cuando \\(K\\) es grande. En OvO, cada clasificador votar\u00e1 por una clase, y la clase que reciba m\u00e1s votos ser\u00e1 la seleccionada.</p> <p>Aunque OvR es un modelo sencillo, f\u00e1cil de implementar y r\u00e1pido de entrenar, tiene una serie de desventajas. Encontramos en primer lugar que los datos de entrenamiento de cada clasificador est\u00e1n fuertemente desbalanceados, ya que hay muchos menos ejemplos de la clase \\(k\\) correspondiente al clasificador que del resto de clases.</p> <p>Por otro lado, este enfoque no nos da probabilidades bien calibradas: los scores de los \\(K\\) clasificadores no suman necesariamente 1, ya que cada clasificador se entrena de forma independiente. Si necesitamos contar con probabilidades bien calibradas, podemos utilizar el enfoque de clasificaci\u00f3n multinomial.</p>"},{"location":"01-modelos-no-param/#multinomial","title":"Multinomial","text":"<p>En este caso, las probabilidades de pertenencia a cada clase en lugar de modelarse con una funci\u00f3n sigmoide se modelan mediante la funci\u00f3n softmax:</p> \\[ P(y=k | \\mathbf{x}) = \\frac{ e^{(\\mathbf{x}^T \\mathbf{w}_k + b_k)} } { \\sum_{j=1}^K e^{(\\mathbf{x}^T \\mathbf{w}_j + b_j)}} \\] <p>Con esto garantizamos que las probabilidades est\u00e9n entre \\(0\\) y \\(1\\) y que todas ellas sumen exactamente \\(1\\):</p> <ul> <li>\\(P(y = k | \\mathbf{x}) \\in  [0, 1]\\)</li> <li>\\(\\sum_{k=1}^{K} P(y = k | \\mathbf{x}) = 1\\) </li> </ul> <p>En este caso la funci\u00f3n a optimizar ser\u00e1 cross-entropy multiclase (o categorical cross-entropy). Este enfoque modela las relaciones entre clases y nos proporciona probabilidades bien calibradas, aunque es m\u00e1s costoso que el anterior.</p> <p>Si tenemos un gran n\u00famero de clases y necesitamos m\u00e1s velocidad, puede ser conveniente utilizar el enfoque OvR, mientras que si el n\u00famero de clases es menor, o necesitamos probabilidades calibradas ser\u00e1 m\u00e1s apropiado el m\u00e9todo multinomial. En la implementaci\u00f3n <code>LogisticRegression</code> de sklearn, todos los solvers utilizar\u00e1n el enfoque multinomial en problemas multi-clase excepto <code>liblinear</code>, que solo soporta clasificaci\u00f3n binaria. En este \u00faltimo caso, si queremos aplicarlo a un problema multiclase, deberemos utilizar OvR. </p>"},{"location":"01-modelos-no-param/#limitaciones-de-los-modelos-lineales","title":"Limitaciones de los modelos lineales","text":"<p>Como hemos visto, los modelos lineales como regresi\u00f3n log\u00edstica buscan el hiperplano que mejor separe los datos de las diferentes clases. Sin embargo, esto no siempre ser\u00e1 posible.</p>"},{"location":"01-modelos-no-param/#regresion-logistica-con-datos-linealmente-separables","title":"Regresi\u00f3n log\u00edstica con datos linealmente separables","text":"<p>En caso de tener datos linealmente separables, como los que se muestran en la  , existir\u00e1 un hiperplano que los separe. En este caso, un clasificador lineal con solo 3 par\u00e1metros (\\(w_1, w_2, b\\)) ser\u00e1 suficiente para representar los datos.</p> <p>Figura 4: Conjunto de datos linealmente separables </p> <p>Con esta distribuci\u00f3n de los datos, incluso si apareciese alg\u00fan solape entre las dos clases o alg\u00fan outlier, seguir\u00eda siendo posible encontrar un hiperplano que nos permita clasificarlos con una alta precisi\u00f3n, y solo un peque\u00f1o porcentaje de errores (ver ).</p> <p>Figura 5: Conjunto de datos separables con solape </p> <p>Podemos adem\u00e1s ver en la  el mapa de probabilidades que nos proporciona el modelo de regresi\u00f3n log\u00edstica.</p> <p>Figura 6: Mapa de probabilidades </p>"},{"location":"01-modelos-no-param/#regresion-logistica-con-datos-linealmente-no-separables","title":"Regresi\u00f3n log\u00edstica con datos linealmente no separables","text":"<p>Sin embargo, si la distribuci\u00f3n de los datos cambiase, y pasaran a ser no separables, como los que se muestran en la , el modelo anterior no ser\u00eda suficiente para representarlos.</p> <p>Figura 7: Conjunto de datos linealmente no separables </p> <p>En este caso los datos siguen un patr\u00f3n conocido como XOR, distribuido en \\(4\\) cuadrantes, y no es posible encontrar ning\u00fan hiperplano que los separe. Cualquier hiperplano nos dar\u00e1 siempre un error aproximado del \\(50\\%\\), que equivale a lanzar una moneda al aire para predecir la clase, tal como se ve en la figura anterior. </p> <p>Vemos en este caso como el no poder adaptar la complejidad del modelo a los datos (estar\u00edamos limitados a \\(3\\) par\u00e1metros \\(w_1, w_2, b\\)) hace que el modelo no pueda ajustarse de forma adecuada.</p>"},{"location":"01-modelos-no-param/#posibles-soluciones","title":"Posibles soluciones","text":"<p>Para clasificar los puntos del \u00faltimo ejemplo minimizando el error de clasificaci\u00f3n, podr\u00edamos optar por:</p> <ul> <li> <p>Clasificador lineal con ingenier\u00eda de caracter\u00edsticas: Crear manualmente caracter\u00edsticas no lineales (por ejemplo \\(x_1^2,x_2^2,x_1 \\cdot x_2\\)) y usar un modelo lineal sobre ellas. Esto requiere tener conocimiento del dominio, para determinar qu\u00e9 caracter\u00edsticas necesitar\u00edamos para que la funci\u00f3n pueda ajustarse a nuestros datos.</p> </li> <li> <p>Clasificador param\u00e9trico no lineal: Por ejemplo, una red neuronal con n\u00famero fijo de par\u00e1metros. Necesitaremos determinar la arquitectura m\u00e1s adecuada.</p> </li> <li> <p>Clasificador no param\u00e9trico: En este caso no ser\u00e1 necesario conocer previamente la estructura de los datos (su forma funcional), sino que el modelo se adaptar\u00e1 autom\u00e1ticamente a su complejidad.</p> </li> </ul> <p>Veremos a continuaci\u00f3n ejemplos de cada una de estas soluciones.</p>"},{"location":"01-modelos-no-param/#ingenieria-de-caracteristicas","title":"Ingenier\u00eda de caracter\u00edsticas","text":"<p>Vamos en primer lugar a ver c\u00f3mo podr\u00edamos utilizar ingenier\u00eda de caracter\u00edsticas para separar el conjunto de datos anterior (ejemplo XOR). </p> <p>Al estar en dos dimensiones contamos con las features \\(x_1\\) y \\(x_2\\). Lo que vamos a hacer es a\u00f1adir adem\u00e1s las features \\(x_1^2\\), \\(x_1 x_2\\) y \\(x_2^2\\). N\u00f3tese que se trata de features derivadas de las originales. Esto nos va a permitir definir una frontera curva entre los datos, donde el papel de la caracter\u00edstica \\(x_1 x_2\\) ser\u00e1 fundamental, ya que seg\u00fan su signo podremos inferir la clase en una operaci\u00f3n XOR. Podemos ver esto ilustrado en la .</p> <p>Figura 8: Ingenier\u00eda de caracter\u00edsticas </p> <p>Si aplicamos a este conjunto de datos el modelo de regresi\u00f3n log\u00edstica introduciendo las features indicadas anteriormente, obtenemos la frontera de clasificaci\u00f3n que se muestra en la .</p> <p>Figura 9: Frontera de decisi\u00f3n con ingenier\u00eda de caracter\u00edsticas </p> <p>Lo que hemos hecho ha sido crear \\(M\\)  caracter\u00edsticas de entrada \\(h(\\mathbf{x}) = (h_1(\\mathbf{x}), h_2(\\mathbf{x}), \\ldots, h_M(\\mathbf{x}))\\) a partir de las caracter\u00edsticas originales \\(\\mathbf{x}\\). Es decir, con \\(h(\\mathbf{x})\\) estamos proyectando las caracter\u00edsticas originales en un nuevo espacio de caracter\u00edsticas. Con esto, el hiperplano de separaci\u00f3n quedar\u00eda de la siguiente forma:</p> \\[ f(\\mathbf{x}) = h(\\mathbf{x})^T \\mathbf{w} + b \\] <p>Podemos observar que aunque el modelo sigue siendo lineal respecto a las caracter\u00edsticas proyectadas \\(h(\\mathbf{x})\\), puede que ya no lo sea en el espacio original de caracter\u00edsticas de \\(\\mathbf{x}\\). Esto es lo que ocurre en el caso del ejemplo anterior, en el que las caracter\u00edsticas proyectadas son \\(h(\\mathbf{x}) = (x_1, x_2, x_1 x_2, x_1^2, x_2^2)\\), y por lo tanto tenemos un polinomio de grado 2 en el espacio original.</p>"},{"location":"01-modelos-no-param/#modelos-parametricos-no-lineales","title":"Modelos param\u00e9tricos no lineales","text":"<p>Un tipo destacado modelo param\u00e9trico no lineal son las Redes Neuronales. Vamos a establecer la relaci\u00f3n de la Regresi\u00f3n Log\u00edstica con las Redes Neuronales y a estudiar como estos modelos pueden resolver problemas no lineales como el planteado.</p> <p>Es f\u00e1cil determinar que el modelo de regresi\u00f3n log\u00edstica binomial es equivalente a un perceptr\u00f3n con \\(d\\) entradas (tantas como features) que aplique una funci\u00f3n sigmoide como funci\u00f3n de activaci\u00f3n (ver ) y funci\u00f3n de p\u00e9rdida binary cross-entropy. </p> <p>Figura 10: Perceptr\u00f3n con \\(d\\) entradas y funci\u00f3n de activaci\u00f3n Sigmoide </p> <p>En este caso, la salida de la neurona ser\u00eda:</p> \\[ y = \\sigma(\\sum_{i=1}^d w_i x_i + b) \\] <p>Donde \\(x_i\\) son las entradas, \\(w_b\\) los pesos para cada entrada y \\(b\\) el sesgo o bias. </p> <p>De la misma forma, un modelo de regresi\u00f3n log\u00edstica multinomial ser\u00eda equivalente a una red con una \u00fanica capa softmax.</p> <p>Cuando a la red le a\u00f1adimos varias capas ocultas (ver ), lo que estaremos haciendo es aprender a transformar el espacio original de caracter\u00edsticas \\(\\mathbf{X}\\) en un espacio latente \\(\\mathbf{H}\\) que facilite su clasificaci\u00f3n. Podremos encontrar estas caracter\u00edsticas en la \u00faltima capa de la red, y sobre ellas se aplicar\u00e1 una clasificaci\u00f3n equivalente a la regresi\u00f3n log\u00edstica.</p> <p>Figura 11: Red neuronal profunda con neurona de salida Sigmoidea </p> <p>Esta transformaci\u00f3n del espacio de caracter\u00edsticas nos podr\u00eda permitir mapear nuestros datos de entrada no linealmente separables sobre un espacio en el que si que lo sean. En este caso tenemos un modelo param\u00e9trico pero no lineal. </p> <p>Cabe destacar que aplicando ingenier\u00eda de caracter\u00edsticas debemos dise\u00f1ar manualmente las nuevas caracter\u00edsticas que derivamos del conjunto original, por lo que es necesario tener conocimiento sobre la forma funcional de los datos. Sin embargo, en este caso es la propia red quien aprende de forma autom\u00e1tica las caracter\u00edsticas m\u00e1s adecuadas para resolver el problema.</p>"},{"location":"01-modelos-no-param/#modelos-no-parametricos","title":"Modelos no param\u00e9tricos","text":"<p>Nuestra tercera opci\u00f3n es el uso de modelos no param\u00e9tricos. Estos modelos cuentan con la ventaja de que su complejidad se adapta a los datos. </p> <p>Dentro de este grupo encontramos por ejemplo modelos basados en vecindad como K-NN, que son capaces de adaptarse sin problema al problema XOR anterior y a formas m\u00e1s complejas. En la  vemos la frontera de decisi\u00f3n obtenida cuando aplicamos K-NN al conjunto de datos que sigue el patr\u00f3n XOR.</p> <p>Figura 12: Aplicaci\u00f3n de K-NN al problema XOR </p> <p>En la pr\u00f3xima sesi\u00f3n veremos el modelo SVM, que originalmente se plantea como un modelo param\u00e9trico lineal, pero que puede transformarse en un modelo no param\u00e9trico mediante el conocido como kernel trick. </p> <p>Encontramos muchos otros modelos no param\u00e9tricos, parte de los cuales enumeramos a continuaci\u00f3n:</p> <ul> <li> <p>M\u00e9todos basados en vecindad. Encontramos K-NN tanto para clasificaci\u00f3n como para regresi\u00f3n, as\u00ed como variantes como K-NN ponderado por distancia, as\u00ed como m\u00e9todos de estimaci\u00f3n de densidad como Kernel Density Estimation (KDE)</p> </li> <li> <p>M\u00e9todos de kernel, como SVM y sus variantes. </p> </li> <li> <p>M\u00e9todos basados en \u00e1rboles. Encontramos los \u00e1rboles de decisi\u00f3n, que nos permiten abordar problemas de clasificaci\u00f3n y regresi\u00f3n, y m\u00e9todos de ensemble que combinan diferentes clasificadores \"d\u00e9biles\" para construir un clasificador \"fuerte\". Dentro de este \u00faltimo subgrupo, encontramos m\u00e9todos como Random Forest, AdaBoost, Gradient Boosting y XGBoost.</p> </li> <li> <p>M\u00e9todos de clustering, como DBSCAN,  Gaussian Mixture Models (GMM) y Spectral Clustering.  </p> </li> <li> <p>M\u00e9todos de aprendizaje por refuerzo, como Q-learning y SARSA.</p> </li> <li> <p>M\u00e9todos de reducci\u00f3n de la dimensionalidad, como t-SNE y UMAP.</p> </li> </ul> <p>En las pr\u00f3ximas sesiones estudiaremos varios de estos modelos.</p> <ol> <li> <p>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). The elements of statistical learning: Data mining, inference, and prediction (2nd ed., pp. 119--128). Springer.\u00a0\u21a9</p> </li> <li> <p>Hsieh, C.-J., Chang, K.-W., Lin, C.-J., Keerthi, S. S., &amp; Sundararajan, S. (2008). A dual coordinate descent method for large-scale linear SVM. Proceedings of the 25th International Conference on Machine Learning, 408--415.\u00a0\u21a9</p> </li> <li> <p>Nocedal, J., &amp; Wright, S. J. (2006). Numerical optimization (2nd ed.). Springer.\u00a0\u21a9</p> </li> <li> <p>Liu, D. C., &amp; Nocedal, J. (1989). On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(1-3), 503--528.\u00a0\u21a9</p> </li> </ol>"},{"location":"02-svm/","title":"2. SVM","text":""},{"location":"02-svm/#sesion-2-support-vector-machines-svm","title":"Sesi\u00f3n 2: Support Vector Machines (SVM)","text":"<p>Las Support Vector Machines (SVM) (Cortes &amp; Vapnik, 1995)1 son algoritmos de aprendizaje supervisado utilizados principalmente para clasificaci\u00f3n, aunque tambi\u00e9n pueden aplicarse a regresi\u00f3n. </p>"},{"location":"02-svm/#aplicaciones-de-svm","title":"Aplicaciones de SVM","text":"<p>Aunque en la actualidad los modelos basados en aprendizaje profundo dominan en campos como la Visi\u00f3n por Computador (CV) o el Procesamiento del Lenguaje Natural (NLP), y en general cuando contamos con extensos conjuntos de datos y disponibilidad de gran capacidad de computaci\u00f3n, modelos como SVM pueden ser competitivos cuando contemos con datos tabulares de tama\u00f1o peque\u00f1o o mediano.</p> <p>SVM ofrece un buen rendimiento con  conjuntos de datos peque\u00f1os. A modo orientativo, con conjuntos de menos de 1.000 ejemplos puede resultar la opci\u00f3n m\u00e1s adecuada, y podr\u00eda mantenerse competitivo incluso con datasets del orden de 10.000 ejemplos.  Encontramos otros modelos que siguen ofreciendo resultados competitivos en estos casos, como XGBoost o Random Forest. </p> <p>Por ejemplo, un \u00e1rea en la que estos modelos pueden resultar de inter\u00e9s es en el an\u00e1lisis de datos m\u00e9dicos, en los que contamos con datasets peque\u00f1os (datos de pacientes) pero con alta dimensionalidad (por ejemplo teniendo en cuenta la expresi\u00f3n de diferentes genes). Adem\u00e1s, tenemos la ventaja de que este tipo de modelos facilita la interpretabilidad, lo cual los hace especialmente interesantes en estos \u00e1mbitos. </p>"},{"location":"02-svm/#maximizacion-del-margen","title":"Maximizaci\u00f3n del margen","text":"<p>Como hemos visto anteriormente, en un problema de clasificaci\u00f3n binaria buscamos encontrar un hiperplano que separe los datos de las dos clases, pero, \u00bfcu\u00e1l es el hiperplano de separaci\u00f3n \u00f3ptimo? Lo que plantea SVM es buscar el hiperplano que maximiza el margen entre las dos clases, a diferencia del modelo de regresi\u00f3n log\u00edstica en el que lo que se buscaba era maximizar la verosimilitud. Es decir, regresi\u00f3n log\u00edstica proporciona probabilidades bien calibradas, mientras que SVM prioriza la robustez del margen, sin producir probabilidades de forma directa.</p> <p>El margen ser\u00e1 la distancia desde el hiperplano hasta los puntos m\u00e1s cercanos de cada clase. Estos puntos m\u00e1s cercanos son conocidos como vectores de soporte (ver ). </p> <p>Figura 1: Maximizaci\u00f3n del margen </p> <p>Adem\u00e1s, como veremos m\u00e1s adelante, SVM se puede generalizar para casos en los que los datos no sean separables de forma l\u00edneal. </p>"},{"location":"02-svm/#margen-duro","title":"Margen duro","text":"<p>Vamos en primer lugar a suponer que los datos son linealmente separables. Hablamos entonces de margen duro, ya que estableceremos la restricci\u00f3n de que los puntos pertenecientes a cada clase deben quedar siempre al lado correcto del margen.</p> <p>Consideremos que tenemos un conjunto de entrenamiento con \\(N\\) pares \\((\\mathbf{x_i}, y_i)\\) con \\(\\mathbf{x_i} \\in \\mathbb{R}^d\\) y \\(y_i \\in \\{-1, 1\\}\\) (problema de clasificaci\u00f3n binaria), siendo \\(d\\) el n\u00famero de features.</p> <p>En caso de que el vector \\(\\mathbf{w}\\) sea unitario, la funci\u00f3n \\(f(\\mathbf{x})\\) nos dar\u00e1 la distancia desde el hiperplano a cada punto \\(\\mathbf{x}\\). </p> <p>Con esto, para buscar el hiperplano que maximice el margen \\(M\\), deberemos resolver el siguiente problema de optimizaci\u00f3n:</p> \\[ \\begin{align*} \\max_{\\mathbf{w}, b, \\lVert \\mathbf{w}  \\rVert=1} \\quad  &amp; M \\\\ \\text{s.a.} \\quad &amp; y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq M, i = 1, \\ldots, N \\end{align*} \\] <p>Podemos eliminar la restricci\u00f3n de que \\(\\mathbf{w}\\) sea unitario dividiendo la ecuaci\u00f3n del hiperplano entre \\(\\lVert \\mathbf{w} \\rVert\\). Si dividimos toda la ecuaci\u00f3n seguir\u00e1 representando al mismo hiperplano y nos permitir\u00e1  reemplazar la condici\u00f3n con:</p> \\[ \\frac{1}{\\lVert \\mathbf{w} \\rVert} y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq M \\] <p>O lo que es lo mismo:</p> \\[ y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq M \\lVert \\mathbf{w} \\rVert \\] <p>En este caso, el hiperplano seguir\u00e1 siendo el mismo independientemente del valor de \\(\\lVert \\mathbf{w} \\rVert\\). Por lo tanto, podemos considerar de forma arbitraria que \\(\\lVert \\mathbf{w} \\rVert = 1 / M\\), lo cual nos permite reescribir la restricci\u00f3n anterior de la siguiente forma:</p> \\[ y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq 1  \\]"},{"location":"02-svm/#forma-primal","title":"Forma primal","text":"<p>Con lo anterior, el problema de optimizaci\u00f3n a resolver tendr\u00eda la siguiente forma:</p> \\[ \\begin{align*} \\min_{\\mathbf{w}, b} \\quad &amp; \\mathbf{\\lVert w \\rVert} \\\\ \\text{s.a.} \\quad &amp; y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq 1, i = 1, \\ldots, N \\end{align*} \\] <p>Esta es la conocida como forma primal, en la que tenemos nuestra funci\u00f3n objetivo y una serie de restricciones. Podr\u00edamos resolver este problema aplicando alg\u00fan m\u00e9todo de optimizaci\u00f3n como descenso por gradiente, o descenso por gradiente estoc\u00e1stico (SGD), para buscar los par\u00e1metros \\(\\mathbf{w}\\) y \\(b\\) \u00f3ptimos. </p> <p>Sin embargo, vamos a utilizar el m\u00e9todo de los multiplicadores de Lagrange (Boyd &amp; Vandenberghe, 2004)2 para transformar este problema con restricciones a un problema en el que las restricciones se transforman en penalizaciones a la funci\u00f3n objetivo.</p> <p>El problema de optimizaci\u00f3n anterior ser\u00eda equivalente al siguiente, ya que el m\u00ednimo de \\(\\mathbf{\\lVert w \\rVert}\\) ser\u00e1 el mismo que el de \\(\\frac{1}{2} \\mathbf{\\lVert w \\rVert}^2\\):</p> \\[ \\begin{align*} \\min_{ \\mathbf{w}, b} \\quad  &amp; \\frac{1}{2} \\mathbf{\\lVert w \\rVert}^2 \\\\ \\text{s.a.} \\quad &amp; y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq 1, i = 1, \\ldots, N \\end{align*} \\] <p>Sin embargo, esta segunda forma nos da ventajas importantes, especialmente la diferenciabilidad de \\(\\frac{1}{2} \\mathbf{\\lVert w \\rVert}^2\\), que es derivable en todos sus puntos, mientras \\(\\mathbf{\\lVert w \\rVert}\\) no es derivable cuando \\(\\mathbf{\\lVert w \\rVert}\\) = 0. </p> <p>Debemos recordar que estamos asumiendo de momento que los datos son separables (margen duro), y por lo tanto consideramos \u00fanicamente dos casos posibles:</p> <ul> <li>\\(y_i(\\mathbf{x}_i^T\\mathbf{w} + b) &gt; 1\\) : correctamente clasificados fuera del margen.</li> <li>\\(y_i(\\mathbf{x}_i^T\\mathbf{w} + b) = 1\\) : Vectores de soporte, pertenecientes al margen.</li> </ul> <p>Para resolver el problema de optimizaci\u00f3n mediante multiplicadores de Lagrange, la funci\u00f3n Lagrangiana primal que deberemos minimizar respecto a \\(\\mathbf{w}\\) y \\(b\\) es la siguiente:</p> \\[ L_P(\\mathbf{w}, b, \\alpha) = \\frac{1}{2} \\lVert \\mathbf{w} \\rVert^2 - \\sum_{i=1}^N \\alpha_i [ y_i ( \\mathbf{x}_i^T \\mathbf{w} + b ) - 1 ] \\] <p>Hemos transformado cada restricci\u00f3n en un t\u00e9rmino de la funci\u00f3n a minimizar, y aplicado a cada uno de estos t\u00e9rminos un multiplicador \\(\\alpha_i\\) (multiplicador de Lagrange). </p> <p>Derivamos la funci\u00f3n anterior respecto a \\(\\mathbf{w}\\) y \\(b\\), y establecemos las derivadas a \\(0\\) para buscar el punto en el que la funci\u00f3n presenta un m\u00ednimo (condici\u00f3n de estacionariedad). Tenemos entonces:</p> \\[ \\begin{align*} \\frac {\\partial L(\\mathbf{w}, b, \\alpha)}{\\partial \\mathbf{w}} &amp;= \\mathbf{w} - \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i = 0 &amp; \\Rightarrow \\mathbf{w} = \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i  \\\\ \\frac {\\partial L(\\mathbf{w}, b, \\alpha)}{\\partial b} &amp;= \\sum_{i=1}^N \\alpha_i y_i  = 0  &amp; \\Rightarrow 0 = \\sum_{i=1}^N \\alpha_i y_i \\end{align*} \\] <p>Sustituyendo \\(\\mathbf{w}\\) en la Lagrangiana (teniendo en cuenta que \\(\\lVert \\mathbf{w} \\rVert^2 = \\mathbf{w}^T \\mathbf{w}\\)) tenemos:</p> \\[ \\begin{align*} L_D(\\mathbf{w}, b, \\alpha) &amp;= \\frac{1}{2}  \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i^T \\sum_{j=1}^N \\alpha_j y_j \\mathbf{x}_j   - \\sum_{i=1}^N \\alpha_i [ y_i ( \\mathbf{x}_i^T \\sum_{j=1}^N \\alpha_j y_j \\mathbf{x}_j + b ) - 1 ] = \\\\ &amp;= \\frac{1}{2}  \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i^T \\sum_{j=1}^N \\alpha_j y_j \\mathbf{x}_j     - \\sum_{i=1}^N \\alpha_i y_i  \\mathbf{x}_i^T \\sum_{j=1}^N \\alpha_j y_j \\mathbf{x}_j  - b \\sum_{i=1}^N \\alpha_i y_i   + \\sum_{i=1}^N \\alpha_i = \\\\  &amp;= \\sum_{i=1}^N \\alpha_i - \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j  \\end{align*} \\]"},{"location":"02-svm/#forma-dual","title":"Forma dual","text":"<p>Tenemos entonces el problema dual. A diferencia del problema primal donde minimiz\u00e1bamos la norma de \\(\\mathbf{w}\\), ahora buscamos maximizar la funci\u00f3n \\(L_D(\\alpha)\\):</p> \\[ \\begin{align*} \\max_\\alpha \\quad &amp; L_D(\\alpha) =  \\sum_{i=1}^N \\alpha_i - \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j  \\\\ s.a.\\quad  &amp; \\sum_{i=1}^N \\alpha_i y_i  = 0  \\\\ &amp;\\alpha_i \\geq 0 \\  \\forall i=1, \\ldots, N \\end{align*} \\] <p>Esto ocurre porque al transformar el problema mediante multiplicadores de Lagrange, la funci\u00f3n dual nos proporciona una cota inferior del valor \u00f3ptimo del problema primal, por lo que para encontrar la mejor soluci\u00f3n debemos maximizar esta cota. </p> <p>Debemos destacar en este punto que en el caso de la forma dual deberemos optimizar los multiplicadores \\(\\alpha_i\\), en lugar de los par\u00e1metro \\(\\mathbf{w}\\) y \\(b\\) como ocurr\u00eda en el caso de la forma primal. </p> <p>Nos encontramos con un problema de programaci\u00f3n cuadr\u00e1tica (QP) convexo con restricciones lineales. Este tipo de problemas tienen la siguiente forma general:</p> \\[ f(\\mathbf{\\alpha}) = \\frac{1}{2} \\mathbf{\\alpha}^T Q \\mathbf{\\alpha} + c^T \\mathbf{\\alpha} \\] <p>Para que el problema sea convexo, la matriz \\(Q\\) debe ser semidefinida positiva, y esto se cumple en el caso de SVM, ya que tenemos:</p> \\[  Q_{ij} = y_i y^j \\mathbf{x}^T_i \\mathbf{x}_j \\] <p>Podremos por lo tanto aplicar alg\u00fan algoritmo de optimizaci\u00f3n para este tipo de problemas. Encontramos diferentes solvers, como por ejemplo CVXOPT o OSQP en Python. </p> <p>En la pr\u00e1ctica, el algoritmo m\u00e1s utilizado es SMO (Sequential Minimal Optimization) (Platt, 1998)3. Este es el algoritmo utilizado por ejemplo por LIBSVM, que es la librer\u00eda que encontramos integrada en scikit-learn. En este caso, en lugar, de resolver un problema QP completo con \\(N\\) variables, selecciona solo dos variables \\(\\alpha_i\\) y \\(\\alpha_j\\) iterativamente, fijando el resto, las optimiza, e itera hasta la convergencia. </p>"},{"location":"02-svm/#condiciones-kkt","title":"Condiciones KKT","text":"<p>En un problema de optimizaci\u00f3n convexo con restricciones para que un punto sea \u00f3ptimo debe satisfacer un conjunto de condiciones conocidas como condiciones KKT (Karush-Kuhn-Tucker) (Boyd &amp; Vandenberghe, 2004; Kuhn &amp; Tucker, 1951)4 2:</p> <ol> <li>Estacionariedad. Buscamos que la funci\u00f3n Lagrangiana tenga gradiente \\(0\\). Se cumple al haber igualado las derivadas a \\(0\\). </li> <li>Factibilidad. El punto debe ser factible y cumplir las restricciones.</li> <li> <p>Signo. Todos los multiplicadores asociados a restricciones de desigualdad deben tener signo positivo: $$ \\alpha_i \\geq 0 \\quad \\forall i=1, \\ldots, N $$  </p> </li> <li> <p>Complementariedad. Adem\u00e1s de las condiciones anteriores, es importante cumplir tambi\u00e9n la siguiente condici\u00f3n:</p> </li> </ol> \\[ \\alpha_i [ y_i ( \\mathbf{x}_i^T \\mathbf{w} + b) - 1 ] = 0 \\quad \\forall i=1, \\ldots, N \\] <p>Esta \u00faltima condici\u00f3n nos dice que:</p> <ul> <li> <p>Si \\(\\alpha_i &gt; 0\\), entonces la restricci\u00f3n es activa y debe cumplirse \\([ y_i ( \\mathbf{x}_i^T \\mathbf{w} + b) - 1 ] = 0\\). Estos ser\u00e1n los puntos conocidos como vectores de soporte, que se encuentran justo en el margen de separaci\u00f3n.</p> </li> <li> <p>En caso de que \\(y_i (\\mathbf{x}_i^T \\mathbf{w} + b) &gt; 1\\), entonces el punto estar\u00e1 fuera del margen y la restricci\u00f3n no ser\u00e1 activa, siendo \\(\\alpha_i = 0\\). En este caso no se tratar\u00e1 de un vector de soporte.</p> </li> </ul> <p>Es importante destacar que solo los puntos con \\(\\alpha_i &gt; 0\\) (vectores de soporte) contribuyen a la soluci\u00f3n. El resto de puntos no afectar\u00e1n al hiperplano. </p> <p>Podemos observar que \\(\\mathbf{w}\\) se obtendr\u00e1 como combinaci\u00f3n lineal de los vectores de soporte \\(\\mathbf{x}_i\\) (aquellos con \\(\\alpha_i &gt; 0\\)). El par\u00e1metro \\(b\\) se puede obtener resolviendo la condici\u00f3n de complementariedad para cualquiera de los vectores de soporte.</p>"},{"location":"02-svm/#margen-blando","title":"Margen blando","text":"<p>Todo lo anterior es v\u00e1lido bajo la suposici\u00f3n de que los datos son linealmente separables, pero si esto no se cumple entonces el problema primal no tendr\u00e1 soluci\u00f3n. </p> <p>Supongamos ahora que existe un solape entre los datos. Una forma de tratar con este solape es maximizar \\(M\\) permitiendo que algunos datos est\u00e9n en el lado incorrecto del margen, para lo cual se definen las variables \\(\\xi = (\\xi_1, \\xi_2, \\ldots, \\xi_N)\\), relajando la restricci\u00f3n del primal de la siguiente forma:</p> \\[ y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq 1 - \\xi_i \\quad \\forall i, \\xi_i \\geq 0 \\] <p>Podemos interpretar \\(\\xi_i\\) como la cantidad proporcional que permitimos que una predicci\u00f3n est\u00e9 en el lado incorrecto del margen (ver ). Si tenemos \\(\\xi_i &gt; 1\\) entonces la correspondiente predicci\u00f3n estar\u00eda mal clasificada, mientras que con valores \\(0 &lt; \\xi_i &lt; 1\\) estar\u00eda correctamente clasificada pero en el lado incorrecto del margen.</p> <p>Figura 2: Margen blando y variables \\(\\xi_i\\) </p> <p>Si acotamos el sumatorio \\(\\sum_{i=1}^N \\xi_i\\) a un valor constante, entonces estaremos acotando el n\u00famero m\u00e1ximo de errores de clasificaci\u00f3n de los datos de entrenamiento a dicha constante. Esto lo trasladaremos como una penalizaci\u00f3n a nuestra funci\u00f3n objetivo.</p>"},{"location":"02-svm/#forma-primal_1","title":"Forma primal","text":"<p>Al igual que hicimos en el caso con margen duro, describimos el problema como una soluci\u00f3n de programaci\u00f3n cuadr\u00e1tica utilizando multiplicadores de Lagrange, en este caso introduciendo las variables \\(\\xi_i\\), con la siguiente funci\u00f3n objetivo:</p> \\[ \\begin{align*} \\min_{ \\mathbf{w}, b} \\quad  &amp; \\frac{1}{2} \\mathbf{\\lVert w \\rVert}^2 + C \\sum^N_{i=1} \\xi_i \\\\ \\text{s.a.} \\quad &amp; \\xi_i \\geq 0,\\ y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq 1 - \\xi_i, \\quad \\forall i = 1, \\ldots, N \\end{align*} \\] <p>Podemos ver que el par\u00e1metro \\(C\\) grad\u00faa la penalizaci\u00f3n de las variables \\(\\xi_i\\). Cuanto m\u00e1s alto sea \\(C\\), m\u00e1s penalizar\u00e1 cada punto fuera del margen. En el caso extremo, con \\(C=\\infty\\) equivaldr\u00eda al caso con margen duro y no se permitir\u00eda ning\u00fan punto en el lado incorrecto del margen.</p> <p>La funci\u00f3n de Langrange primal en este caso es:</p> \\[ L_P(\\mathbf{w}, b, \\xi, \\alpha, \\mu) = \\frac{1}{2} \\lVert \\mathbf{w} \\rVert^2 + C \\sum^N_{i=1} \\xi_i - \\sum_{i=1}^N \\alpha_i [ y_i ( \\mathbf{x}_i^T \\mathbf{w} + b ) - (1-\\xi_i) ] - \\sum_{i=1}^N \\mu_i \\xi_i \\] <p>Tendremos que minimizar esta funci\u00f3n respecto a \\(\\mathbf{w}\\), \\(b\\) y \\(\\xi_i\\), por lo que igualaremos las correspondientes derivadas a \\(0\\):</p> \\[ \\begin{align*} \\frac {\\partial L_P(\\mathbf{w}, b, \\xi, \\alpha, \\mu)}{\\partial \\mathbf{w}} &amp;= \\mathbf{w} - \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i = 0 &amp; \\Rightarrow \\mathbf{w} = \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i  \\\\ \\frac {\\partial L_P(\\mathbf{w}, b, \\xi, \\alpha, \\mu)}{\\partial b} &amp;= \\sum_{i=1}^N \\alpha_i y_i  = 0  &amp; \\Rightarrow 0 = \\sum_{i=1}^N \\alpha_i y_i   \\\\ \\frac {\\partial L_P(\\mathbf{w}, b, \\xi, \\alpha, \\mu)}{\\partial \\xi_i} &amp;= C - \\alpha_i - \\mu_i = 0 &amp; \\Rightarrow \\alpha_i = C - \\mu_i \\end{align*} \\]"},{"location":"02-svm/#forma-dual_1","title":"Forma dual","text":"<p>Sustituyendo las derivadas anteriores en la funci\u00f3n primal, obtenemos la forma dual:</p> \\[ \\begin{align*} \\max_\\alpha \\quad &amp; L_D(\\alpha) =  \\sum_{i=1}^N \\alpha_i - \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j  \\\\ s.a.\\quad  &amp; \\sum_{i=1}^N \\alpha_i y_i  = 0  \\\\ &amp;0 \\leq \\alpha_i \\leq C \\quad  \\forall i=1, \\ldots, N \\end{align*} \\] <p>La funci\u00f3n \\(L_D\\) nos da una cota inferior de la funci\u00f3n objetivo para cualquier punto viable, por lo que buscaremos maximizarla. </p> <p>Adem\u00e1s, se deben cumplir las diferentes condiciones KKT:</p> <ol> <li> <p>Estacionariedad. Se cumple habiendo igualado las derivadas a \\(0\\).</p> </li> <li> <p>Factibilidad. Deben cumplirse las restricciones originales del problema primal: $$  y_i(\\mathbf{x}_i^T\\mathbf{w} + b) \\geq 1 - \\xi_i \\quad \\forall i \\ \\xi_i \\geq 0 \\quad \\forall i $$</p> </li> <li> <p>Signo. Los multiplicadores asociados a restricciones de desigualdad no deben ser negativos: $$  \\alpha_i \\geq 0 \\quad \\forall i \\ \\mu_i \\geq 0 \\quad \\forall i $$</p> </li> <li> <p>Complementariedad. Esta es la m\u00e1s importante a tener en cuenta, ya que define qu\u00e9 restricciones son activas (aquellas con par\u00e1metros \\(\\alpha_i &gt; 0\\) y \\(\\mu_i &gt; 0\\)), indicando de esta forma cu\u00e1les son los vectores de soporte.  $$ \\alpha_i[y_i(\\mathbf{x}_i^T \\mathbf{w} + b) - 1 + \\xi_i] = 0 \\quad \\forall i \\ \\mu_i \\xi_i = 0 \\Rightarrow (C-\\alpha_i) \\xi_i = 0 \\quad \\forall i  $$</p> </li> </ol> <p>Podemos distinguir varios casos:</p> <ul> <li> <p>\\(\\alpha_i = 0\\). Son puntos correctamente clasificados, que no son vectores de soporte. En este caso siempre tendremos \\(\\xi_i = 0\\) debido a las condiciones de complementariedad.</p> </li> <li> <p>\\(0 &lt;  \\alpha_i &lt; C\\). Estos son los vectores de soporte que se sit\u00faan exactamente en el margen. En estos casos \\(\\mu_i &gt; 0\\), y por lo tanto \\(\\xi_i = 0\\), por lo que no hay violaci\u00f3n del margen.</p> </li> <li> <p>\\(\\alpha_i = C\\). En este caso tenemos vectores de soporte que violan el margen. En estos casos \\(\\mu_i = 0\\), por lo que podemos tener \\(\\xi_i &gt; 0\\). Teniendo en cuenta que se debe cumplir \\(y_i (\\mathbf{x}^T_i \\mathbf{w} + b) = 1 - \\xi_i\\), si \\(0 &lt; \\xi_i &lt; 1\\) entonces el vector viola el margen pero estar\u00e1 bien clasificado, mientras que en caso de que \\(\\xi &gt; 1\\) entonces estar\u00e1 mal clasificado.</p> </li> </ul> <p>Una vez resuelto el problema de optimizaci\u00f3n y obtenidos los \\(\\alpha_i\\) \u00f3ptimos, podemos observar que los coeficientes \\(\\mathbf{w}\\) se obtendr\u00edan como combinaci\u00f3n lineal \u00fanicamente de los vectores de soporte (entradas \\(\\mathbf{x_i}\\) para las que \\(\\alpha_i &gt; 0\\)):</p> \\[ \\mathbf{w} = \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i  \\] <p>Una vez obtenidos los coeficientes, para despejar \\(b\\), podemos utilizar cualquiera de los puntos del margen (\\(\\alpha_i &gt; 0\\), \\(\\xi_i = 0\\)) en la primera ecuaci\u00f3n de la restricci\u00f3n de complementariedad, aunque habitualmente se suele hacer una media de la estimaci\u00f3n de todos ellos para tener una mayor estabilidad num\u00e9rica. </p>"},{"location":"02-svm/#efecto-del-parametro-c","title":"Efecto del par\u00e1metro C","text":"<p>Es importante entender el rol del par\u00e1metro \\(C\\):</p> <ul> <li>Con valores altos de \\(C\\), se penalizar\u00e1n \\(\\xi_i\\) positivos, y podremos tender al overfitting. </li> <li>Por el contrario, con valores bajos de \\(C\\) se tender\u00e1 a valores peque\u00f1os de \\(\\lVert \\mathbf{w} \\rVert\\), lo que causar\u00e1 que la frontera sea m\u00e1s suave (ampliando el margen).</li> </ul>"},{"location":"02-svm/#primal-vs-dual","title":"Primal VS Dual","text":"<p>Como hemos visto, SVM lineal puede resolverse de ambas formas. Son dos formas complementarias que resuelven el mismo problema. La clave est\u00e1 en que en cada caso cambian las variables a optimizar. En el caso de la forma primal optimizamos directamente los par\u00e1metros del modelo \\((\\mathbf{w}, b)\\), donde \\(\\mathbf{w} \\in \\mathbb{R}^d\\) (siendo \\(d\\) el n\u00famero de features), mientras que en el caso de la forma dual estaremos optimizando los \\(N\\) multiplicadores \\(\\alpha_i\\) (uno para cada ejemplo de entrenamiento).</p> <p>Por lo tanto, la conclusi\u00f3n m\u00e1s inmediata es que si \\(N \\gg d\\) convendr\u00eda utilizar la forma primal, mientras que en el caso de tener \\(N &lt; d\\) ser\u00eda preferible la forma dual. </p> <p>La implementaci\u00f3n SVCLinear de sklearn nos permite elegir entre utilizar la forma dual o la forma primal, e incluso nos permite dejar que la implementaci\u00f3n seleccione el problema autom\u00e1ticamente en funci\u00f3n del n\u00famero de samples, el n\u00famero de features y otros par\u00e1metros. Esta implementaci\u00f3n utiliza internamente como optimizador el m\u00e9todo Coordinate Descent (Hsieh et al., 2008)5, implementado en la librer\u00eda LIBLINEAR. Recordemos que es un m\u00e9todo similar a descenso por gradiente, pero en el que se seleccionan caracter\u00edsticas una a una. Se fijan todas las variables excepto una, se optimiza para esa variable, y repite iterativamente para cada variable, iterando hasta la convergencia. </p> <p>Por otro lado, en caso de contar con un extenso conjunto de datos, puede ser conveniente utilizar Descenso por Gradiente Estoc\u00e1stico. En este caso, contamos con SGDClassifier que nos permite especificar diferentes funciones de p\u00e9rdida para diferentes modelos lineales. Por defecto, utiliza la funci\u00f3n de p\u00e9rdida <code>hinge</code> que equivale a SVM Lineal (maximizar el margen es equivamente a minimizar el hinge loss). En este caso estaremos resolviendo siempre el problema primal (hemos de tener en cuenta que utilizaremos esta implementaci\u00f3n cuando el n\u00famero de samples sea muy grande). Si como funci\u00f3n de p\u00e9rdida utilizamos <code>log_loss</code> entonces tendremos un clasificador de regresi\u00f3n log\u00edstica.   </p> <p>Adem\u00e1s del criterio de n\u00famero de samples frente a n\u00famero de features, otra ventaja de la forma dual es la esparsidad de la soluci\u00f3n, ya que solo unos pocos puntos tienen \\(\\alpha_i &gt; 0\\) y contribuyen.</p> <p>Pero la ventaja m\u00e1s destacada de utilizar la forma lineal es que nos permite aplicar el conocido como Kernel trick, con el que podremos transformar el modelo en no lineal, e incluso en no param\u00e9trico.</p>"},{"location":"02-svm/#kernel-trick","title":"Kernel trick","text":"<p>Hemos visto como mediante ingenier\u00eda de caracter\u00edsticas podemos proyectar las caracter\u00edsticas originales \\(\\mathbf{x}\\) en un nuevo espacio de caracter\u00edsticas \\(h(\\mathbf{x})\\). El clasificador SVM presenta una extensi\u00f3n de esta idea, que nos permite que la dimensi\u00f3n del nuevo espacio de caracter\u00edsticas sea muy alta, e incluso infinita en algunos casos.</p> <p>Para que esto sea abordable, la idea es representar el problema de optimizaci\u00f3n de forma que las caracter\u00edsticas de entrada se presenten solo como producto escalar. </p> <p>Podemos representar la funci\u00f3n dual de la siguiente forma:</p> \\[ L_D(\\alpha) =  \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j \\langle h(\\mathbf{x}_i), h(\\mathbf{x}_j) \\rangle \\] <p>Donde hemos sustituido el producto escalar de \\(\\mathbf{x_i}^T \\mathbf{x_j}\\) por el producto escalar entre las caracter\u00edsticas transformadas. </p> <p>De esta forma, la funci\u00f3n del hiperplano de separaci\u00f3n quedar\u00eda expresada de la siguiente forma:</p> \\[ f(x) = h(\\mathbf{x})^T \\mathbf{w} + b  \\] <p>Recordando que los pesos se calculan como una combinaci\u00f3n lineal de los vectores de soporte, tendr\u00edamos:</p> \\[ \\begin{align*} f(x) &amp;= h(\\mathbf{x})^T  \\sum_{i=1}^N \\alpha_i y_i h(\\mathbf{x}_i)  + b = \\\\ &amp;= \\sum_{i=1}^N \\alpha_i y_i \\langle h(\\mathbf{x}), h(\\mathbf{x}_i) \\rangle  + b \\end{align*} \\] <p>Podemos ver entonces que tanto en la formulaci\u00f3n del problema dual como en la funci\u00f3n de separaci\u00f3n soluci\u00f3n del problema las variables de entrada \\(h(\\mathbf{x})\\) est\u00e1n involucradas \u00fanicamente en forma de producto escalar, por lo que lo \u00fanico que necesitamos conocer de ellas es lo que conocemos como funci\u00f3n de Kernel:</p> \\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\langle h(\\mathbf{x}_i), h(\\mathbf{x}_j) \\rangle \\] <p>La funci\u00f3n \\(K\\) produce el producto escalar de las caracter\u00edsticas en el espacio transformado, y solo necesitamos conocer esta funci\u00f3n, no hace falta que trabajemos en el espacio transformado. Esto es lo que se conoce como Kernel trick (Boser et al., 1992)6. Hay que destacar que la aplicaci\u00f3n de este Kernel trick solo es posible con la formulaci\u00f3n del problema dual. Por lo tanto, en la implementaci\u00f3n SVC de sklearn siempre se resolver\u00e1 el problema dual, permitiendo de esta forma el uso de diferentes Kernels.</p> <p>Utilizando diferentes funciones de Kernel podremos aplicar de forma sencilla y eficiente diferentes transformaciones del espacio de caracter\u00edsticas (ver ). Vamos a ver los Kernels m\u00e1s comunes.</p> <p>Figura 3: Comparativa de SVM con diferentes Kernels aplicados a datos no separables linealmente: lineal (izquierda), polinomial (centro) y RBF (derecha) </p>"},{"location":"02-svm/#kernel-lineal","title":"Kernel lineal","text":"<p>Generalizando el uso de los Kernels, podemos ver que el modelo SVM lineal que hemos estudiado hasta el momento se podr\u00eda considerar un caso particular en el que se utiliza el siguiente Kernel:</p> \\[ K(\\mathbf{x}_i, \\mathbf{x}_j) =\\mathbf{x}_i^T \\mathbf{x}_j \\] <p>Este Kernel podr\u00e1 resultar adecuado cuando sepamos que los datos son linealmente separables o en casos en los que tengamos alta dimensionalidad. Este tipo de Kernel facilita la interpretabilidad.</p>"},{"location":"02-svm/#kernel-polinomial","title":"Kernel polinomial","text":"<p>Est\u00e1 relacionado con el uso de caracter\u00edsticas polinomiales, pero tiene la ventaja de que no es necesario definir las caracter\u00edsticas expl\u00edcitamente, sino que se apoya en el Kernel trick para obtener una mayor eficiencia. Tiene la siguiente forma:</p> \\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma  \\mathbf{x}_i^T \\mathbf{x}_j + r) ^d \\] <p>Donde \\(d\\) es el grado del polinomio, \\(\\gamma\\) controla la influencia de cada sample en la frontera de decisi\u00f3n y \\(r\\) es un t\u00e9rmino de sesgo en el polinomio. </p> <p>En este caso, el Kernel est\u00e1 mapeando las caracter\u00edsticas a un espacio de mayor dimensionalidad, pero la dimensi\u00f3n sigue siendo finita y fija, por lo que seguimos teniendo un modelo param\u00e9trico. En este caso la frontera de decisi\u00f3n puede que ya no sea lineal en el espacio original de caracter\u00edsticas (aunque lo seguir\u00e1 siendo en el transformado), tal como hemos visto anteriormente en el caso de ingenier\u00eda de caracter\u00edsticas.</p> <p>En la  podemos ver el efecto del par\u00e1metro de grado \\(d\\) en el Kernel polinomial. Con un mayor grado podemos tener fronteras m\u00e1s complejas, pero tambi\u00e9n mayor riesgo de overfitting.</p> <p>Figura 4: Efecto del grado en el Kernel polinomial </p>"},{"location":"02-svm/#kernel-radial-basis-function-rbf","title":"Kernel Radial Basis Function (RBF)","text":"<p>Se trata de uno de los Kernels m\u00e1s utilizados, y permite representar relaciones no lineales complejas. Se basa en crear campanas de similaridad alrededor de los puntos, generando algo parecido a un mapa de calor. Tiene la siguiente forma:</p> \\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = e^{- \\gamma \\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert^2} \\] <p>El par\u00e1metro \\(\\gamma\\) controla la influencia de cada sample en la frontera de decisi\u00f3n, permitiendo as\u00ed ajustar la suavidad de la frontera. </p> <p>En este caso, el Kernel Gaussiano mapea a un espacio de dimensi\u00f3n infinita.  Intuitivamente, esto significa que crea una funci\u00f3n base (una 'campana gaussiana') centrada en cada punto del conjunto de datos, permitiendo representar fronteras de decisi\u00f3n arbitrariamente complejas. El modelo seleccionar\u00e1 como vectores de soporte \u00fanicamente aquellos puntos necesarios para definir la frontera, lo que permite que la complejidad del modelo se adapte autom\u00e1ticamente a los datos de entrada. Por ello, en este caso el modelo pasa a ser no param\u00e9trico.</p> <p>En la figura  podemos ver el efecto del par\u00e1metro \\(\\gamma\\) en el Kernel RBF. Con valores altos de este par\u00e1metro (\\(\\gamma = 10\\)) podemos observar fronteras muy irregulares y un mayor n\u00famero de vectores de soporte, indicando tendencia al overfitting. Por el contrario, con valores bajos de \\(\\gamma\\) (por ejemplo \\(\\gamma=0.1\\)) se obtienen fronteras m\u00e1s suaves que generalizan mejor, aunque si es demasiado bajo podr\u00eda causar underfitting.  </p> <p>Figura 5: Efecto del par\u00e1metro \\(\\gamma\\) en el Kernel RBF </p>"},{"location":"02-svm/#kernel-sigmoide","title":"Kernel sigmoide","text":"<p>Tambi\u00e9n conocido como Kernel neural network, tiene un comportamiento similar a redes neuronales de una capa, y tiene la siguiente forma:</p> \\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh (\\gamma  \\mathbf{x}_i^T \\mathbf{x}_j + r) \\] <p>Donde, al igual que en los casos anteriores, \\(\\gamma\\) controla la influencia de cada sample en la frontera de decisi\u00f3n y \\(r\\) es un t\u00e9rmino de sesgo que desplaza los datos en una direcci\u00f3n y otra.</p> <p>Es menos utilizado actualmente, y su uso se limita a casos muy concretos en los que los datos tienen una forma sigmoidal. </p> <ol> <li> <p>Cortes, C., &amp; Vapnik, V. (1995). Support-vector networks. Machine Learning, 20(3), 273--297. https://doi.org/10.1007/BF00994018 \u21a9</p> </li> <li> <p>Boyd, S., &amp; Vandenberghe, L. (2004). Convex optimization. Cambridge University Press.\u00a0\u21a9\u21a9</p> </li> <li> <p>Platt, J. C. (1998). Sequential minimal optimization: A fast algorithm for training support vector machines (Nos. MSR-TR-98-14). Microsoft Research.\u00a0\u21a9</p> </li> <li> <p>Kuhn, H. W., &amp; Tucker, A. W. (1951). Nonlinear programming. Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability, 481--492.\u00a0\u21a9</p> </li> <li> <p>Hsieh, C.-J., Chang, K.-W., Lin, C.-J., Keerthi, S. S., &amp; Sundararajan, S. (2008). A dual coordinate descent method for large-scale linear SVM. Proceedings of the 25th International Conference on Machine Learning, 408--415.\u00a0\u21a9</p> </li> <li> <p>Boser, B. E., Guyon, I. M., &amp; Vapnik, V. N. (1992). A training algorithm for optimal margin classifiers. Proceedings of the Fifth Annual Workshop on Computational Learning Theory, 144--152. https://doi.org/10.1145/130385.130401 \u21a9</p> </li> </ol>"},{"location":"03-arboles-decision/","title":"3. \u00c1rboles de decisi\u00f3n","text":""},{"location":"03-arboles-decision/#sesion-3-arboles-de-decision","title":"Sesi\u00f3n 3: \u00c1rboles de decisi\u00f3n","text":"<p>Los \u00e1rboles de decisi\u00f3n son modelos de aprendizaje supervisado que nos permiten hacer tanto tareas de clasificaci\u00f3n como de regresi\u00f3n. Se basan en una estructura jer\u00e1rquica de decisiones, y su principal ventaja es la interpretabilidad del modelo, ya que podemos interpretarlos como un diagrama de flujo en el que en cada nodo debe tomarse una decisi\u00f3n (ver -izquierda).</p>"},{"location":"03-arboles-decision/#estructura","title":"Estructura","text":"<p>El \u00e1rbol tiene un nodo ra\u00edz que abarcar\u00e1 todo el espacio de caracter\u00edsticas, conteniendo todo el conjunto de datos de entrada. </p> <p>Cada nodo interno del \u00e1rbol divide el espacio de caracter\u00edsticas mediante una  condici\u00f3n sobre los atributos. Considerando que nuestros datos tienen un conjunto de \\(d\\) atributos o features \\(\\{x_1, x_2, \\ldots, x_d \\}\\), la condici\u00f3n de cada nodo tendr\u00e1 habitualmente una forma del tipo \\(x_i \\leq \\text{valor}\\). Es decir, podemos ver cada nodo como un modelo sencillo que separa los datos en dos subramas.</p> <p>El nodo tendr\u00e1 diferentes subramas que lo conectan con sus hijos. Estas subramas representan los resultados de la condici\u00f3n. Aplicando la condici\u00f3n a cada dato de entrada, se seleccionar\u00e1 una de las subramas y llegaremos al correspondiente nodo hijo. Los hijos, a su vez, podr\u00e1n definir nuevas condiciones que vuelvan a particionar el espacio de caracter\u00edsticas. </p> <p>Llegaremos finalmente a nodos hoja que no tienen hijos y corresponden a las predicciones finales, que podr\u00e1n corresponder a las diferentes clases en problemas de clasificaci\u00f3n o valores en problemas de regresi\u00f3n. </p> <p>De este forma, para un determinado ejemplo de entrada, el \u00e1rbol se recorrer\u00e1 desde la ra\u00edz, tomando en cada nodo la subrama que corresponda al resultado de aplicar la condici\u00f3n del nodo al ejemplo, hasta llegar a un nodo hoja. El nodo hoja que alcancemos nos dar\u00e1 la predicci\u00f3n que devolver\u00e1 el modelo.</p> <p>Desde el punto de vista de dos dimensiones, este \u00e1rbol estar\u00e1 dividiendo el espacio de caracter\u00edsticas en diferentes rect\u00e1ngulos (ver -centro). </p> <p>Figura 1: Estructura del \u00e1rbol de decisi\u00f3n (izquierda) y divisi\u00f3n del espacio de caracter\u00edsticas en regiones (centro y derecha) </p> <p>Esta divisi\u00f3n generar\u00e1 \\(J\\) regiones \\(R_1, R_2, \\ldots, R_J\\) no solapadas, de forma que cada datos de entrada \\(\\mathbf{x}\\) pertenecer\u00e1 a una, y solo una de estas regiones. Cada regi\u00f3n \\(R_j\\) corresponde a uno de los nodos hoja del \u00e1rbol, y estar\u00e1 asociada a una categor\u00eda en el caso de los \u00e1rboles de clasificaci\u00f3n, o a un valor en el caso de \u00e1rboles de regresi\u00f3n. Es importante destacar que la salida de los \u00e1rboles de decisi\u00f3n estar\u00e1 estratificada, ya que dentro de cada regi\u00f3n se generar\u00e1 siempre un valor constante. </p>"},{"location":"03-arboles-decision/#construccion","title":"Construcci\u00f3n","text":"<p>Vamos a ver en este punto c\u00f3mo construir el \u00e1rbol a partir de un conjunto de datos. Consideramos que nuestro conjunto de datos \\(\\mathcal{D}\\) contiene \\(N\\) ejemplos \\((\\mathbf{x}_i, y_i)\\) para \\(i =\u00a0\\{1, 2, \\ldots, N \\}\\), con \\(\\mathbf{x}_i = ( x_{i1}, x_{i2}, \\ldots, x_{id} )\\).</p> <p>Como hemos comentado, el \u00e1rbol dividir\u00e1 el espacio de caracter\u00edsticas en \\(J\\) regiones \\(R_1, R_2, \\ldots, R_J\\) no solapadas, de forma que cada dato \\(\\mathbf{x}_i\\) corresponder\u00e1 a una de estas regiones.</p> <p>Buscamos encontrar la divisi\u00f3n del espacio que se ajuste de forma \u00f3ptima a los datos. Por ejemplo, en caso de \u00e1rboles de regresi\u00f3n podemos tomar como criterio encontrar el particionamiento que minimice el error cuadr\u00e1tico medio (MSE) total del conjunto de datos:</p> \\[ \\frac{1}{N} \\sum_{j=1}^J \\sum_{i \\in R_j} (y_i - \\hat{y}_{R_j})^2 \\] <p>Donde \\(\\hat{y}_{R_j}\\) es la media de la salida de las observaciones pertenecientes a la regi\u00f3n \\(R_j\\). </p> <p>Dado que no es viable considerar todas las posibles particiones del espacio, se opta por un algoritmo voraz que va dividiendo el espacio recursivamente. Partiendo del conjunto de entrenamiento completo, este algoritmo funciona de la siguiente forma:</p> <ol> <li>Seleccionamos el mejor atributo y punto de corte para dividir el conjunto de datos actual.</li> <li>Creamos un nodo con una condici\u00f3n basada en los par\u00e1metros seleccionados.</li> <li>Particionamos el conjunto de datos en dos subconjuntos, en funci\u00f3n del resultado de la condici\u00f3n anterior.</li> <li>Repetimos este proceso recursivamente para cada uno de los subconjuntos anteriores hasta cumplir un criterio de parada (por ejemplo hasta conseguir regiones suficientemente homog\u00e9neas).</li> </ol> <p>Una de las cuestiones m\u00e1s cr\u00edtica es establecer un criterio de divisi\u00f3n de los datos para establecer cu\u00e1les son los mejores par\u00e1metros para particionar nuestros datos. Deberemos seleccionar tanto una caracter\u00edstica \\(j \\in \\{1, 2, \\ldots, d\\}\\) como un valor de corte \\(t\\). De esta forma, la condici\u00f3n dividir\u00eda el espacio en dos subregiones y separar\u00e1 los datos en dos subconjuntos: </p> \\[ \\mathcal{D}_L = \\{ \\{\\mathbf{x}_i, y_i \\} : x_{ij} \\leq t \\} \\\\ \\mathcal{D}_R = \\{ \\{\\mathbf{x}_i, y_i \\} : x_{ij} \\gt t \\} \\] <p>Definimos una funci\u00f3n de impureza \\(H\\) que nos indicar\u00e1 la calidad de la partici\u00f3n. Buscamos minimizar esta funci\u00f3n para conseguir que la divisi\u00f3n genere regiones lo m\u00e1s homog\u00e9neas posible. Las posibles funciones alternativas de impureza diferir\u00e1n seg\u00fan si las orientamos a problemas de clasificaci\u00f3n o de regresi\u00f3n. Veremos m\u00e1s adelante las funciones utilizadas com\u00fanmente para ambos tipos de problemas.</p> <p>Con esta funci\u00f3n \\(H\\), podemos calcular la impureza de la divisi\u00f3n (split) con par\u00e1metros \\((j, t)\\) de la siguiente forma:</p> \\[ H_{split}(j,t) = \\frac{N_L}{N} H(\\mathcal{D}_L) + \\frac{N_R}{N} H(\\mathcal{D}_R) \\] <p>Donde \\(N_L = |\\mathcal{D_L}|\\) y \\(N_R = |\\mathcal{D_R}|\\) indican el n\u00famero de ejemplos que quedar\u00edan en cada una de las particiones. Es decir, la impureza de la divisi\u00f3n se calcula como la suma de las impurezas de cada una de las particiones creadas, ponderada por el n\u00famero de ejemplos de cada partici\u00f3n.</p> <p>De esta forma, se deber\u00e1n buscar los par\u00e1metros \\((j,t)\\) que minimicen la funci\u00f3n \\(H_{split}(j,t)\\). Para ello, se puede realizar una b\u00fasqueda exhaustiva para todos los pares \\((j,t)\\), o realizar un muestreo aleatorio de algunos posibles valores para reducir el coste computacional. </p> <p>Vamos a continuaci\u00f3n a ver de forma espec\u00edfica las principales funciones de impureza utilizadas en problemas de regresi\u00f3n y clasificaci\u00f3n. </p>"},{"location":"03-arboles-decision/#arboles-de-regresion","title":"\u00c1rboles de regresi\u00f3n","text":"<p>En caso de \u00e1rboles de regresi\u00f3n, dentro de cada regi\u00f3n se devolver\u00e1 como predicci\u00f3n \\(\\hat{y}_i\\) siempre el mismo valor constante, que se calcular\u00e1 habitualmente como la media de todas las observaciones \\(\\bar{y}_i\\) del conjunto de entrenamiento que pertenezcan a dicha regi\u00f3n. </p> <p>Como funci\u00f3n de impureza en \u00e1rboles de regresi\u00f3n habitualmente se utiliza el error cuadr\u00e1tico medio (MSE):</p> \\[ H_{MSE}(\\mathcal{D}) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\bar{y}_i)^2 \\] <p>Vemos que en la funci\u00f3n calculamos la diferencia entre el valor observado \\(y_i\\) y la predicci\u00f3n, que en este caso es la media \\(\\bar{y}_i\\). Podemos observar tambi\u00e9n que esta funci\u00f3n coincide con la varianza de las salidas esperadas del conjunto \\(\\mathcal{D}\\). </p> <p>En la pr\u00e1ctica se utiliza tambi\u00e9n habitualmente la suma de los cuadrados de los errores (SSE), que es equivalente a la formulaci\u00f3n anterior, ya que la \u00fanica diferencia es que el valor no est\u00e1 promediado:</p> \\[ H_{SSE}(\\mathcal{D}) = \\sum_{i=1}^N (y_i - \\bar{y}_i)^2 \\] <p>Esta es la funci\u00f3n de impureza m\u00e1s comunmente utilizada, siendo el valor por defecto en las principales librer\u00edas. Esta funci\u00f3n asume ruido gaussiano y penaliza los errores grandes.</p> <p>Tambi\u00e9n puede utilizarse el error absoluto medio (MAE), aunque en estos casos como predicci\u00f3n utilizamos la mediana \\(\\tilde{y}_i\\) en lugar de la media:</p> \\[ H_{MAE}(\\mathcal{D}) = \\frac{1}{N} \\sum_{i=1}^N |y_i - \\tilde{y}_i| \\] <p>Esta funci\u00f3n es m\u00e1s robusta frente a outliers, y no penaliza tanto grandes errores como el caso anterior, aunque el proceso de optimizaci\u00f3n resulta m\u00e1s costoso. </p> <p>Tambi\u00e9n contamos con otras funciones como Poisson deviance, que se define de la siguiente forma:</p> \\[ H_{Poisson}(\\mathcal{D}) = \\frac{2}{N} \\sum_{i=1}^N (y_i \\log \\frac{y_i}{\\bar{y}_i} - y_i + \\bar{y}_i) \\] <p>Este criterio puede ser de inter\u00e9s cuando la variable objetivo sea un conteo (por ejemplo, n\u00famero de estudiantes aprobados) o frecuencia (por ejemplo, n\u00famero de suspensos por estudiante).</p>"},{"location":"03-arboles-decision/#arboles-de-clasificacion","title":"\u00c1rboles de clasificaci\u00f3n","text":"<p>En este caso buscamos clasificar cada ejemplo de entrada en un n\u00famero \\(K\\) de clases. </p> <p>Considerando un nodo \\(m\\) del \u00e1rbol que representa un regi\u00f3n \\(R_m\\) que contiene un conjunto de \\(N_m\\) ejemplos, calculamos la proporci\u00f3n de las observaciones de la clase \\(k\\) dentro del nodo \\(m\\) de la siguiente forma:</p> \\[ p_{mk} = \\frac{1}{N_m} \\sum_{\\mathbf{x}_i \\in R_m} I(y_i = k) \\] <p>Buscamos medir c\u00f3mo de mezcladas est\u00e1n las clases dentro de cada nodo. Para ello, varias de las funciones de impureza utilizadas est\u00e1n basadas en teor\u00eda de la informaci\u00f3n. </p> <p>Una de las funciones m\u00e1s utilizadas es el \u00edndice de Gini, que se define de la siguiente forma para un nodo \\(m\\):</p> \\[ H_{Gini} = 1 - \\sum_{k=1}^K p_{mk}^2 \\] <p>En este caso, si el nodo es puro y solo contiene una \u00fanica clase tendremos \\(H_{Gini} = 0\\). El valor ser\u00e1 m\u00e1ximo cuando todas las clases est\u00e9n equilibradas. Es utilizada por el algoritmo CART y en Random Forest.</p> <p>Tambi\u00e9n se utiliza habitualmente la entrop\u00eda:</p> \\[ H_{Entrop\u00eda} =  - \\sum_{k=1}^K p_{mk} \\log_2 (p_{mk}) \\] <p>Se trata de una medida basada en teor\u00eda de la informaci\u00f3n, que mide la incertidumbre del conjunto. Nos dar\u00e1 m\u00e1xima incertidumbre cuando las clases est\u00e1n equilibradas, y penaliza m\u00e1s que Gini. Se utiliza  en los algoritmos ID3 y C4.5.</p>"},{"location":"03-arboles-decision/#poda-de-arboles","title":"Poda de \u00e1rboles","text":"<p>Los \u00e1rboles individuales tienen bajo sesgo pero alta varianza, ya que al construirse de forma voraz tienen una alta dependencia con los datos de entrada. Un peque\u00f1o cambio en los datos puede producir \u00e1rboles muy diferentes. </p> <p>Cuando con el algoritmo descrito el \u00e1rbol se hace crecer en exceso, tendremos tendencia al overfitting. Clasificar\u00e1 bien los datos de entrenamiento, pero dar\u00e1 malos resultados de test. Reduciendo el n\u00famero de regiones podemos reducir la varianza y mejorar la capacidad de generalizaci\u00f3n y la interpretabilidad.</p> <p>Una posible forma de abordar esta tarea es evitar que el \u00e1rbol crezca en exceso, esto es lo que se conoce como poda previa.</p>"},{"location":"03-arboles-decision/#poda-previa","title":"Poda previa","text":"<p>La poda previa consiste en detener el crecimiento del \u00e1rbol antes de que alcance su tama\u00f1o m\u00e1ximo imponiendo criterior de parada. Algunos de estos criterios pueden ser:</p> <ul> <li>Limitar la profundidad m\u00e1xima</li> <li>Establecer un n\u00famero m\u00ednimo de muestras que debe tener un nodo</li> <li>Exigir que la mejora de la impureza se encuentre por encima de un m\u00ednimo</li> </ul> <p>Aunque es una forma r\u00e1pida y sencilla de simplificar el grafo, existe riesgo de underfitting. Por ejemplo, si  establecemos como criterio de parada dejar de crecer cuando la mejora de la medida de impureza est\u00e9 por debajo de cierto umbral, puede ocurrir que en un nivel tengamos una divisi\u00f3n que no mejora apenas, pero en el siguiente encontremos una gran mejora, con lo cual, si paramos de forma anticipada, nos estaremos perdiendo esa gran mejora en la clasificaci\u00f3n.</p> <p>Por ello, es mejor estrategia dejar crecer el \u00e1rbol hasta obtener un gran \u00e1rbol \\(T_0\\), y tras ello aplicar una poda posterior. </p>"},{"location":"03-arboles-decision/#poda-posterior","title":"Poda posterior","text":"<p>La poda posterior consiste en reducir el tama\u00f1o del \u00e1rbol eliminando nodos o sub\u00e1rboles que no aportan una mejora significativa en la capacidad de generalizaci\u00f3n. </p> <p>El proceso de poda se har\u00e1 siempre desde abajo hacia arriba, es decir, desde las hojas hacia la ra\u00edz. Para cada nodo interno, tendremos la opci\u00f3n de mantener el sub\u00e1rbol que depende de \u00e9l, o bien podarlo y convertir dicho nodo en un nodo hoja.</p> <p>Tras aplicar la poda, buscaremos quedarnos con aquel sub\u00e1rbol \\(T \\subset T_0\\) que haga que no empeore, o incluso consiga que mejore, su error de generalizaci\u00f3n respecto a \\(T_0\\). Este error de generalizaci\u00f3n ser\u00e1 el error obtenido con datos no vistos durante la construcci\u00f3n del \u00e1rbol.</p> <p>Dado un subarbol, podemos estimar su error de generalizaci\u00f3n mediante un conjunto de validaci\u00f3n separado o mediante validaci\u00f3n cruzada. Sin embargo, dado que normalmente existir\u00e1 un n\u00famero muy elevado de posibles sub\u00e1rboles, no ser\u00e1 posible computacionalmente evaluar el error de todos ellos. </p> <p>Necesitaremos tener un criterio que nos permita seleccionar un peque\u00f1o conjunto de sub\u00e1rboles para tener en consideraci\u00f3n. Una forma de hacer esto es mediante la poda basada en complejidad de coste (Breiman et al., 1984)1.</p>"},{"location":"03-arboles-decision/#poda-basada-en-complejidad-de-coste","title":"Poda basada en complejidad de coste","text":"<p>La idea es generar una secuencia de sub\u00e1rboles candidatos con la siguiente forma:</p> \\[ T_0 \\supset T_1 \\supset T_2 \\supset \\ldots \\supset T_k \\] <p>Para cada uno de estos \u00e1rboles candidatos, se calcular\u00e1 su error de validaci\u00f3n y seleccionaremos aquel con menor error de validaci\u00f3n. </p>"},{"location":"03-arboles-decision/#funcion-de-complejidad-de-coste","title":"Funci\u00f3n de complejidad de coste","text":"<p>Lo fundamental ser\u00e1 encontrar un criterio que nos permita generar la secuencia de \u00e1rboles a considerar. Para ello se define la siguiente funci\u00f3n de coste:</p> \\[ R_{\\alpha}(T) = R(T)+ \\alpha |\\tilde{T}| \\] <p>Donde \\(R(T)\\) es el error emp\u00edrico obtenido con el \u00e1rbol \\(T\\) con los datos de entrenamiento, \\(\\alpha \\geq 0\\) es un par\u00e1metro para controlar la complejidad del \u00e1rbol, y \\(|\\tilde{T}|\\) es el n\u00famero de hojas del \u00e1rbol \\(T\\). </p>"},{"location":"03-arboles-decision/#generacion-de-la-secuencia-de-subarboles","title":"Generaci\u00f3n de la secuencia de sub\u00e1rboles","text":"<p>Buscamos generar \u00e1rboles que minimicen el coste anterior con diferentes valores de \\(\\alpha\\). Podemos interpretar este par\u00e1metro como un factor de penalizaci\u00f3n por el n\u00famero de hojas. En el caso de \\(\\alpha = 0\\), al no existir penalizaci\u00f3n el error m\u00ednimo nos lo dar\u00e1 el \u00e1rbol completo, pero conforme incrementemos la penalizaci\u00f3n deberemos reducir el n\u00famero de hojas. </p> <p>Para cada nodo interno del \u00e1rbol \\(t\\), consideraremos el error \\(R(t)\\) (o impureza) dentro del propio nodo \\(t\\), como si se tratara de un hoja, y la suma de los errores \\(R(T_t)\\) de todas las hojas del sub\u00e1rbol con ra\u00edz en \\(t\\). Es decir, comparamos el error emp\u00edrico \\(R(T_t)\\) del sub\u00e1rbol completo con ra\u00edz en \\(t\\), con el error emp\u00edrico \\(R(t)\\) si dicho sub\u00e1rbol se reemplazase por una hoja, aplicando la poda. </p> <p>En general, \\(R(T_t) &lt; R(t)\\), ya que de no ser as\u00ed no deber\u00edan haberse seguido generando hijos durante la construcci\u00f3n del \u00e1rbol. Sin embargo, si introducimos la penalizaci\u00f3n con el par\u00e1metro \\(\\alpha\\) entonces tenemos:</p> \\[ R_\\alpha (t) = R(t) + \\alpha \\\\ R_\\alpha (T_t) = R(T_t) + \\alpha |\\tilde{T}_t| \\] <p>En este caso podemos buscar el valor de \\(\\alpha\\) que haga \\(R_{\\alpha}(T_t) = R_\\alpha (t)\\). Para cada uno de los nodos internos calculamos:</p> \\[ \\alpha_t = \\frac{R(t) - R(T_t)}{|\\tilde{T}_t| - 1} \\] <p>Buscaremos el nodo \\(t\\) con menor \\(\\alpha_t\\), y podaremos ese nodo, obteniendo as\u00ed \\(T_1\\).</p> <p>Aplicaremos el mismo proceso sobre \\(T_1\\) para obtener \\(T_2\\), y as\u00ed iterativamente hasta quedarnos con un \u00fanico nodo. </p>"},{"location":"03-arboles-decision/#seleccion-final-del-arbol","title":"Selecci\u00f3n final del \u00e1rbol","text":"<p>Una vez generados todos los \u00e1rboles candidatos \\(T_0, T_1, \\ldots, T_k\\), los evaluaremos mediante validaci\u00f3n cruzada y nos quedaremos con aquel que obtenga un m\u00ednimo error.</p>"},{"location":"03-arboles-decision/#algoritmos","title":"Algoritmos","text":"<p>Vamos a ver a continuaci\u00f3n los principales algoritmos para la construcci\u00f3n de \u00e1rboles de decisi\u00f3n: CART, ID3 y C4.5.</p>"},{"location":"03-arboles-decision/#cart","title":"CART","text":"<p>CART es un algoritmo para construir \u00e1rboles de decisi\u00f3n binarios, aplicable tanto a clasificaci\u00f3n como a regresi\u00f3n. Fue propuesto por Breiman et al. (1984) (Breiman et al., 1984)1 y es la base de m\u00e9todos como Random Forests.</p> <p>Es el algoritmo que encontramos implementado en las clases DecisionTreeClassifier y DecisionTreeRegressor en sklearn.</p>"},{"location":"03-arboles-decision/#criterio-de-division","title":"Criterio de divisi\u00f3n","text":"<p>CART construye el \u00e1rbol mediante un proceso recursivo y voraz, donde en cada nodo se selecciona la mejor divisi\u00f3n posible seg\u00fan los siguientes criterios de impureza:</p> <ul> <li>MSE, para \u00e1rboles de regresi\u00f3n.</li> <li>\u00cdndice de Gini, para \u00e1rboles de clasificaci\u00f3n.</li> </ul> <p>Para cada nodo se consideran todos los atributos \\(j\\) y todos los posibles puntos de corte \\(t\\), y para cada split candidato \\((j,t)\\) se calcula la ganancia de la siguiente forma:</p> \\[ \\Delta H(j,t) = H(\\mathcal{D}) - \\left[  \\frac{N_L}{N} H(\\mathcal{D}_L) + \\frac{N_R}{N} H(\\mathcal{D}_R) \\right] \\] <p>Es decir, se mide cuanto mejora la pureza de los conjuntos al realizar la divisi\u00f3n, respecto a la impureza del nodo padre. Se seleccionar\u00e1 el split \\((j,t)\\) que maximice la reducci\u00f3n de impureza.</p>"},{"location":"03-arboles-decision/#criterio-de-parada","title":"Criterio de parada","text":"<p>El \u00e1rbol se construir\u00e1 recursivamente hasta que se alcance un criterio de parada:</p> <ul> <li>Se ha obtenido un nodo totalmente puro</li> <li>Se ha alcanzado la profundidad m\u00e1xima del \u00e1rbol</li> <li>Se ha alcanzado el n\u00famero m\u00ednimo de observaciones por nodo</li> <li>No se ha obtenido mejora significativa respecto al nodo padre</li> </ul>"},{"location":"03-arboles-decision/#prediccion-en-las-hojas","title":"Predicci\u00f3n en las hojas","text":"<p>En problemas de regresi\u00f3n, la predicci\u00f3n ser\u00e1 la media del valor de todas las observaciones que pertenezcan al nodo hoja:</p> \\[ \\hat{y}_i = \\bar{y}_i \\] <p>En problemas de clasificaci\u00f3n, la predicci\u00f3n ser\u00e1 o bien la clase mayoritaria del nodo hoja, o bien una distribuci\u00f3n de probabilidades:</p> \\[ \\hat{p}_k = \\frac{N_k}{N} \\] <p>Donde \\(N_k\\) es el n\u00famero de observaciones del nodo que pertenecen a la clase \\(k\\), y \\(N\\) es el n\u00famero total de observaciones en el nodo.</p>"},{"location":"03-arboles-decision/#poda-del-arbol","title":"Poda del \u00e1rbol","text":"<p>El algoritmo CART utiliza poda posterior basada en complejidad de coste. Tal como se ha comentado anteriormente, generar\u00e1 con este criterio una secuencia de sub\u00e1rboles y seleccionar\u00e1 el mejor aplicando validaci\u00f3n cruzada.</p>"},{"location":"03-arboles-decision/#id3","title":"ID3","text":"<p>ID3 (Quinlan, 1986)2 es un algoritmo hist\u00f3rico para la construcci\u00f3n de \u00e1rboles de decisi\u00f3n destinado \u00fanicamente a la construcci\u00f3n de \u00e1rboles de clasificaci\u00f3n.</p>"},{"location":"03-arboles-decision/#estructura-del-arbol","title":"Estructura del \u00e1rbol","text":"<p>Este algoritmo fue dise\u00f1ado para la clasificaci\u00f3n con atributos de entrada categ\u00f3ricos. Es decir, cada atributo de entrada puede tomar un conjunto finito de valores. Por ejemplo, podr\u00edamos tener como entrada los siguientes atributos:</p> \\[ \\begin{align*} \\text{Viento} &amp;\\rightarrow \\{ \\text{D\u00e9bil}, \\text{Medio}, \\text{Fuerte} \\} \\\\ \\text{Clima} &amp;\\rightarrow \\{ \\text{Soleado}, \\text{Nublado}, \\text{Lluvia} \\} \\end{align*} \\] <p>En caso de contar con atributos num\u00e9ricos, deber\u00edamos discretizarlos previamente en una serie de categor\u00edas. </p> <p>Este algoritmo construye \u00e1rboles no binarios (multirama), ya que genera una subrama para cada cada posible valor del atributo seleccionado para la divisi\u00f3n. Por ejemplo, si en un nodo se selecciona el atributo \\(\\text{Viento}\\) como criterio de divisi\u00f3n, se crear\u00e1n \\(3\\) subramas: \\(\\text{D\u00e9bil}\\), \\(\\text{Medio}\\) y \\(\\text{Fuerte}\\).</p>"},{"location":"03-arboles-decision/#criterio-de-seleccion","title":"Criterio de selecci\u00f3n","text":"<p>En este caso se utiliza la entrop\u00eda como medida de impureza. Para un conjunto de datos \\(\\mathcal{D}\\) tenemos:</p> \\[ H_{Entrop\u00eda}(\\mathcal{D}) =  - \\sum_{k=1}^K p_{k} \\log_2 (p_{k}) \\] <p>Donde \\(p_k\\) se define como la proporci\u00f3n de observaciones de la clase \\(k\\) dentro de \\(\\mathcal{D}\\).</p> <p>Como criterio de selecci\u00f3n de atributo se utiliza la ganacia de informaci\u00f3n, basada en la entrop\u00eda. Busca con ello clasificar los ejemplos de entrenamiento reduciendo la incertidumbre sobre la clase.</p> <p>Consideremos que cada atributo de entrada \\(x_j\\) puede tomar un conjunto de posibles valores \\(\\text{Valores}(x_j)\\). La ganancia de informaci\u00f3n separando el conjunto de datos \\(\\mathcal{D}\\) con el atributo \\(x_j\\)  se define como:</p> \\[ GI(\\mathcal{D},j) = H(\\mathcal{D}) - \\sum_{v \\in \\text{Valores}(x_j)} \\frac{|\\mathcal{D}_v|}{|\\mathcal{D}|} H(\\mathcal{D}_v) \\] <p>Donde \\(\\mathcal{D}_v\\) es el subconjunto de \\(\\mathcal{D}\\) en el que el atributo \\(x_j\\) toma como valor la categor\u00eda \\(v\\).</p> <p>ID3 seleccionar\u00e1 el atributo que produzca una mayor ganancia de informaci\u00f3n. </p>"},{"location":"03-arboles-decision/#pasos-del-algoritmo","title":"Pasos del algoritmo","text":"<p>A continuaci\u00f3n se muestra el algoritmo paso a paso:</p> <ol> <li>Si todas las instancias pertenecen a la misma clase se crea una hoja.</li> <li>Si no quedan atributos se crea una hoja con la clase mayoritaria.</li> <li>Se calcula la ganancia de informaci\u00f3n para cada atributo.</li> <li>Se elige el atributo con m\u00e1xima ganancia.</li> <li>Se crea  una subrama por cada valor del atributo.</li> <li>Repite el proceso recursivamente en cada subrama.</li> </ol>"},{"location":"03-arboles-decision/#c45","title":"C4.5","text":"<p>El algoritmo C4.5 (Quinlan, 1993)3 se presenta como una evoluci\u00f3n de ID3, tambi\u00e9n destinado \u00fanicamente a \u00e1rboles de clasificaci\u00f3n. Las principales mejoras introducidas sobre su predecesor son:</p> <ul> <li>Introduce la medida Gain Ratio para evitar el sesgo de ID3 hacia atributos con muchos valores. </li> <li>Permite el uso de atributos continuos.</li> <li>Aplica poda post-pruning para reducir el overfitting.</li> <li>Permine manejar valores perdidos.</li> </ul>"},{"location":"03-arboles-decision/#gain-ratio","title":"Gain Ratio","text":"<p>ID3 utiliza la ganancia de informaci\u00f3n \\(GI(\\mathcal{D}, j)\\), pero esto produce un sesgo hacia atributos con muchos valores. Por ejemplo imaginemos un atributo que tiene un valor diferente para cada ejemplo de entrada. Este atributo ser\u00eda seleccionado ya que genera nodos hoja puros, pero no generaliza. </p> <p>Para evitar esto, se introduce la medida Gain Ratio. Para calcular esta medida, primero se calcula el factor de normalizaci\u00f3n Split Info:</p> \\[ \\text{SplitInfo}(\\mathcal{D}, x_j) = - \\sum_{v \\in \\text{Valores}(x_j)} \\frac{|\\mathcal{D}_v|}{|\\mathcal{D}|} \\log_2 \\frac{|\\mathcal{D}_v|}{|\\mathcal{D}|} \\] <p>Este factor de normalizaci\u00f3n es una medida de entrop\u00eda, pero no de las clases, sino sobre c\u00f3mo estar\u00edan repartidos los tama\u00f1os de las particiones en las diferentes subramas. Ser\u00e1 \\(0\\) cuando todos los elementos del conjunto de datos tengan la misma categor\u00eda para el atributo \\(x_j\\), mientras que ser\u00e1 m\u00e1ximo cuando los diferentes valores del atributo est\u00e9n presentes de forma homog\u00e9nea. Pero adem\u00e1s, tendr\u00e1 un valor bajo cuando haya pocas subramas, y un valor alto cuando haya muchas. </p> <p>El Gain Ratio se calcular\u00e1 de la siguiente forma:</p> \\[ \\text{GainRatio}(\\mathcal{D}, x_j) = \\frac{GI(\\mathcal{D}, j)}{\\text{SplitInfo}(\\mathcal{D}, x_j)} \\] <p>De esta forma, el factor Split Info se utiliza para penalizar subdivisiones excesivas y la falta de generalizaci\u00f3n. </p>"},{"location":"03-arboles-decision/#atributos-continuos","title":"Atributos continuos","text":"<p>Aunque internamente el algoritmo sigue manejando atributos categ\u00f3ricos, permite introducir como entrada atributos continuas. En estos casos, el algoritmo discretizar\u00e1 de forma autom\u00e1tica estos atributos. </p> <p>Para ello, dado un atributo num\u00e9rico \\(x_j\\), realiza lo siguiente:</p> <ul> <li>Ordena todos los valores de \\(x_j\\).</li> <li>Prueba diferentes splits binarios \\(t\\), para dividir en dos categor\u00edas: \\(x_j \\leq t\\) y \\(x_j &gt; t\\). </li> <li>Se elige el umbral \\(t\\) que maximiza el Gain Ratio.</li> </ul>"},{"location":"03-arboles-decision/#valores-perdidos","title":"Valores perdidos","text":"<p>En caso de que existan valores perdidos, C4.5 asigna una probabilidad a cada posible valor del atributo en funci\u00f3n de la distribuci\u00f3n de probabilidad observada. Durante el c\u00e1lculo de la entrop\u00eda de los splits cada ejemplo con un valor perdido contribuir\u00e1 proporcionalmente a cada subrama.</p>"},{"location":"03-arboles-decision/#poda-posterior_1","title":"Poda posterior","text":"<p>Otra de las mejoras que introduce el algoritmo C4.5 frente a ID3 es la poda.</p> <p>Este algoritmo utiliza poda posterior con estimaci\u00f3n del error. No utiliza un conjunto de validaci\u00f3n separado, sino que realiza de forma estad\u00edstica una estimaci\u00f3n pesimista del error a partir del error emp\u00edrico. </p> <p>El algoritmo recorre el \u00e1rbol de abajo a arriba, evaluando primero los sub\u00e1rboles m\u00e1s profundos. Para cada nodo interno, estima el error del sub\u00e1rbol, y estima el error si ese nodo fuera una hoja. Si la diferencia no es significativa, aplica la poda. </p>"},{"location":"03-arboles-decision/#pasos-del-algoritmo_1","title":"Pasos del algoritmo","text":"<p>Los pasos del algoritmo son similares a los del algoritmo ID3, con algunas modificaciones:</p> <ol> <li>Si todas las instancias pertenecen a la misma clase se crea una hoja.</li> <li>Si no quedan atributos se crea una hoja con la clase mayoritaria.</li> <li>Se calcula el Gain Ratio para cada atributo.</li> <li>Se elige el atributo con mayor Gain Ratio.</li> <li>Se crea una subrama por cada valor del atributo (o umbral si es un atributo continuo).</li> <li>Repite el proceso recursivamente en cada subrama.</li> <li>Aplica poda posterior para reducir el overfitting.</li> </ol>"},{"location":"03-arboles-decision/#limitaciones-de-los-arboles-de-decision","title":"Limitaciones de los \u00e1rboles de decisi\u00f3n","text":"<p>A pesar de su simplicidad e interpretabilidad, los \u00e1rboles de decisi\u00f3n presentan limitaciones importantes, especialmente su elevada varianza y su sensibilidad a peque\u00f1as perturbaciones en los datos de entrenamiento. Un \u00e1rbol profundo puede producir overfitting, mientras que una poda excesiva puede conducir al underfitting. </p> <p>Estas limitaciones han motivado el desarrollo de m\u00e9todos que combinan m\u00faltiples \u00e1rboles con el objetivo de mejorar la capacidad de generalizaci\u00f3n. Los m\u00e9todos de ensemble se basan precisamente en esta idea: combinar de forma adecuada un conjunto de modelos sencillos, habitualmente \u00e1rboles de decisi\u00f3n, buscando reducir tanto sesgo como varianza.</p> <ol> <li> <p>Breiman, L., Friedman, J., Stone, C. J., &amp; Olshen, R. A. (1984). Classification and regression trees. CRC Press.\u00a0\u21a9\u21a9</p> </li> <li> <p>Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1(1), 81--106. https://doi.org/10.1007/BF00116251 \u21a9</p> </li> <li> <p>Quinlan, J. R. (1993). C4.5: Programs for machine learning. Morgan Kaufmann.\u00a0\u21a9</p> </li> </ol>"},{"location":"04-random-forest/","title":"M\u00e9todos de ensemble","text":""},{"location":"04-random-forest/#metodos-de-ensemble","title":"M\u00e9todos de ensemble","text":""},{"location":"04-random-forest/#bagging","title":"Bagging","text":""},{"location":"04-random-forest/#boosting","title":"Boosting","text":""},{"location":"04-random-forest/#random-forests","title":"Random Forests","text":""},{"location":"05-adaboost/","title":"AdaBoost","text":""},{"location":"05-adaboost/#adaboost","title":"AdaBoost","text":""},{"location":"06-gradient-boosting/","title":"Gradient Boosting","text":""},{"location":"06-gradient-boosting/#gradient-boosting","title":"Gradient Boosting","text":""},{"location":"06-gradient-boosting/#xgboost","title":"XGBoost","text":""},{"location":"07-clustering/","title":"Clustering","text":""},{"location":"07-clustering/#clustering","title":"Clustering","text":""},{"location":"07-clustering/#introduccion-al-clustering","title":"Introducci\u00f3n al clustering","text":"<ul> <li>Concepto y objetivos</li> <li>Tipos de clustering: particionamiento, jer\u00e1rquico, basado en densidad, probabil\u00edstico</li> <li>M\u00e9tricas de distancia y similitud</li> <li>Evaluaci\u00f3n de clustering (silhouette score, \u00edndice Davies-Bouldin, etc.)</li> </ul>"},{"location":"07-clustering/#clustering-jerarquico","title":"Clustering Jer\u00e1rquico","text":"<ul> <li>Aglomerativo vs. divisivo</li> <li>M\u00e9tricas de linkage: single, complete, average, Ward</li> <li>Dendrogramas: interpretaci\u00f3n y corte</li> <li>Complejidad computacional</li> <li>Ventajas e inconvenientes</li> </ul>"},{"location":"07-clustering/#dbscan","title":"DBSCAN","text":"<ul> <li>Conceptos: core points, border points, noise</li> <li>Par\u00e1metros: epsilon, MinPts</li> <li>Ventajas: formas arbitrarias, detecci\u00f3n de ruido, no requiere especificar K</li> <li>Limitaciones: clusters con diferentes densidades</li> <li>Selecci\u00f3n de par\u00e1metros (k-distance plot)</li> <li>Aplicaciones pr\u00e1cticas</li> </ul>"},{"location":"07-clustering/#gaussian-mixture-models-gmm","title":"Gaussian Mixture Models (GMM)","text":"<ul> <li>De hard clustering a soft clustering</li> <li>Modelo probabil\u00edstico: mezcla de gaussianas</li> <li> <p>EM Algorithm (Expectation-Maximization)</p> <ul> <li>E-step: asignaci\u00f3n probabil\u00edstica</li> <li>M-step: actualizaci\u00f3n de par\u00e1metros</li> <li>Convergencia y garant\u00edas</li> </ul> </li> <li> <p>Selecci\u00f3n del n\u00famero de componentes (BIC, AIC)</p> </li> <li>Ventajas sobre K-Means: clusters el\u00edpticos, asignaci\u00f3n probabil\u00edstica</li> <li>Aplicaciones: densidad de probabilidad, inicializaci\u00f3n</li> </ul> <p>El EM algorithm es conceptualmente importante porque aparece en muchos otros contextos de ML. Est\u00e1 relacionado con K-Means, que es un caso especial de EM con varianzas fijas. La visualizaci\u00f3n de elipses vs. c\u00edrculos ayuda a la comprensi\u00f3n</p>"},{"location":"07-clustering/#clustering-espectral","title":"Clustering Espectral","text":"<ul> <li>Motivaci\u00f3n: limitaciones de K-Means con estructuras complejas</li> <li> <p>Conceptos de teor\u00eda de grafos</p> <ul> <li>Grafo de similitud (KNN graph, epsilon-neighborhood, fully connected)</li> <li>Matriz de adyacencia</li> <li>Matriz de grado</li> <li>Matriz laplaciana (unnormalized, normalized)</li> </ul> </li> <li> <p>Algoritmo de clustering espectral</p> <ul> <li>Construcci\u00f3n del grafo de similitud</li> <li>C\u00e1lculo de autovectores de la laplaciana</li> <li>Aplicaci\u00f3n de K-Means en el espacio transformado</li> </ul> </li> <li> <p>Intuici\u00f3n geom\u00e9trica: proyecci\u00f3n a espacio donde clusters son linealmente separables</p> </li> <li>Relaci\u00f3n con graph cuts (normalized cut, ratio cut)</li> <li>Ventajas: formas arbitrarias, fundamento te\u00f3rico s\u00f3lido</li> <li>Limitaciones: escalabilidad, selecci\u00f3n de par\u00e1metros del grafo</li> <li> <p>Aplicaciones: segmentaci\u00f3n de im\u00e1genes, an\u00e1lisis de redes sociales</p> </li> <li> <p>Ejemplos donde K-Means falla pero espectral funciona (c\u00edrculos conc\u00e9ntricos, espirales)</p> </li> <li>Relaci\u00f3n con graph neural networks </li> </ul>"},{"location":"07-clustering/#isolation-forest-y-deteccion-de-anomalias","title":"Isolation Forest y detecci\u00f3n de anomal\u00edas","text":"<ul> <li>Concepto: anomal\u00edas son f\u00e1ciles de aislar</li> <li>Construcci\u00f3n de \u00e1rboles aleatorios</li> <li>Path length como score de anomal\u00eda</li> <li>Ventajas: eficiente, no requiere asumir distribuci\u00f3n</li> <li>Relaci\u00f3n con Random Forest</li> </ul> <p>Uso de clustering para detecci\u00f3n de anomal\u00edas</p> <ul> <li>Puntos lejanos de centroides (K-Means)</li> <li>Noise points (DBSCAN)</li> <li>Baja probabilidad (GMM)</li> </ul> <p>Isolation Forest est\u00e1 relacionado con Random Forest, pero con una filosof\u00eda completamente diferente. Ambos utilizan conjuntos (ensemble) de \u00e1rboles de decisi\u00f3n, pero con objetivos y mecanismos distintos.</p> <p>Coinciden en que ambos usan ensemble de \u00e1rboles:</p> <ul> <li>Construyen m\u00faltiples \u00e1rboles de decisi\u00f3n</li> <li>Combinan sus resultados para la predicci\u00f3n final</li> <li>Usan aleatorizaci\u00f3n para crear diversidad entre \u00e1rboles</li> </ul> <p>Construcci\u00f3n aleatoria:</p> <ul> <li>Muestreo aleatorio de datos</li> <li>Selecci\u00f3n aleatoria de caracter\u00edsticas</li> </ul> <p>Las diferencias fundamentales son:</p> <ol> <li>Objetivo</li> </ol> <p>Random Forest:</p> <ul> <li>Aprendizaje supervisado (clasificaci\u00f3n o regresi\u00f3n)</li> <li>Objetivo: maximizar la precisi\u00f3n de predicci\u00f3n</li> <li>Reduce varianza mediante promediado</li> </ul> <p>Isolation Forest:</p> <ul> <li>Aprendizaje no supervisado (detecci\u00f3n de anomal\u00edas)</li> <li>Objetivo: identificar puntos an\u00f3malos</li> <li> <p>No hay etiquetas, no predice clases</p> </li> <li> <p>Filosof\u00eda de los \u00e1rboles</p> </li> </ul> <p>Random Forest:</p> <ul> <li>Construye \u00e1rboles profundos hasta conseguir hojas puras (o casi puras)</li> <li>Busca separar bien las clases en el espacio de caracter\u00edsticas</li> <li>Usa criterios de divisi\u00f3n basados en impureza (Gini, entrop\u00eda)</li> </ul> <p>Isolation Forest:</p> <ul> <li>Construye \u00e1rboles poco profundos (altura limitada)</li> <li>No intenta separar clases (no hay etiquetas)</li> <li>Hace divisiones aleatorias del espacio</li> <li>El objetivo NO es pureza, sino aislar puntos</li> </ul>"},{"location":"practicas/00-datos-y-visualizacion/","title":"0. Datos y visualizaci\u00f3n","text":""},{"location":"practicas/00-datos-y-visualizacion/#clase-0-flujo-completo-de-machine-learning-de-datos-a-modelo","title":"Clase 0: Flujo Completo de Machine Learning - De Datos a Modelo","text":"<p>Contexto</p> <p>Esta es la clase inaugural de Aprendizaje Avanzado. Cubriremos el flujo end-to-end de un problema de ML: desde datos crudos hasta un modelo desplegado y rastreado. No nos enfocamos en implementaci\u00f3n, sino en por qu\u00e9 cada paso importa.</p>"},{"location":"practicas/00-datos-y-visualizacion/#1-preprocesamiento-de-datos","title":"1. Preprocesamiento de Datos","text":"<p>El preprocesamiento es donde invertir\u00e1s el 60-70% del tiempo en un proyecto real de ML. Es tentador salt\u00e1rselo, pero un buen preprocesamiento es la diferencia entre un modelo mediocre y uno excelente.</p>"},{"location":"practicas/00-datos-y-visualizacion/#11-exploracion-inicial-y-tipos-de-datos","title":"1.1 Exploraci\u00f3n Inicial y Tipos de Datos","text":"<p>\u00bfQu\u00e9 hacemos aqu\u00ed?</p> <p>Lo primero es entender qu\u00e9 tenemos entre manos. Necesitas responder:</p> <ul> <li>\u00bfCu\u00e1ntas muestras y caracter\u00edsticas tengo?</li> <li>\u00bfQu\u00e9 tipo de datos son? (num\u00e9ricas, categ\u00f3ricas, de fecha, texto)</li> <li>\u00bfHay valores faltantes? \u00bfCu\u00e1ntos?</li> <li>\u00bfCu\u00e1l es mi variable objetivo?</li> </ul> <p>\u00bfPor qu\u00e9 importa?</p> <p>Diferentes tipos de datos requieren diferentes tratamientos. Una variable categ\u00f3rica no se puede alimentar directamente a sklearn. Los valores faltantes pueden sesgar tu modelo. Los tipos incorrectos causan errores silenciosos.</p> <p>En la pr\u00e1ctica:</p> <pre><code>import pandas as pd\nimport numpy as np\n# Cargar datos\ndf = pd.read_csv('./datasets/Iris.csv')\n# Dimensiones\nprint(\"*\" * 40)\nprint(f\"Shape: {df.shape}\")  # (150, 5)\nprint(\"*\" * 40)\n# Tipos de datos\nprint(df.dtypes)\nprint(\"*\" * 40)\n# Primeras filas\nprint(df.head())\nprint(\"*\" * 40)\n# Resumen r\u00e1pido\nprint(df.info())\nprint(\"*\" * 40)\n# Valores faltantes\nprint(df.isnull().sum())\n</code></pre> <p>Output esperado para Iris: </p><pre><code>****************************************\nShape: (150, 6)\n****************************************\nId                 int64\nSepalLengthCm    float64\nSepalWidthCm     float64\nPetalLengthCm    float64\nPetalWidthCm     float64\nSpecies           object\ndtype: object\n****************************************\n   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n0   1            5.1           3.5            1.4           0.2  Iris-setosa\n1   2            4.9           3.0            1.4           0.2  Iris-setosa\n2   3            4.7           3.2            1.3           0.2  Iris-setosa\n3   4            4.6           3.1            1.5           0.2  Iris-setosa\n4   5            5.0           3.6            1.4           0.2  Iris-setosa\n****************************************\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 6 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   Id             150 non-null    int64  \n 1   SepalLengthCm  150 non-null    float64\n 2   SepalWidthCm   150 non-null    float64\n 3   PetalLengthCm  150 non-null    float64\n 4   PetalWidthCm   150 non-null    float64\n 5   Species        150 non-null    object \ndtypes: float64(4), int64(1), object(1)\nmemory usage: 7.2+ KB\nNone\n****************************************\nId               0\nSepalLengthCm    0\nSepalWidthCm     0\nPetalLengthCm    0\nPetalWidthCm     0\nSpecies          0\ndtype: int64\n</code></pre><p></p> <p>Nota importante: Iris no tiene valores faltantes (es por eso que es un dataset \"juguete\"). En la vida real, rara vez ver\u00e1s datos tan limpios.</p>"},{"location":"practicas/00-datos-y-visualizacion/#12-manejo-de-valores-faltantes","title":"1.2 Manejo de Valores Faltantes","text":"<p>\u00bfQu\u00e9 son y por qu\u00e9 aparecen?</p> <p>Valores faltantes <code>(NaN, None)</code> aparecen por:</p> <ul> <li>Errores en la recopilaci\u00f3n</li> <li>Equipos que no registraron datos</li> <li>Confidencialidad (datos intencionalmente omitidos)</li> <li>Procesos de limpieza anteriores</li> </ul> <p>Estrategias de manejo:</p> Estrategia Cu\u00e1ndo usar Ventajas Desventajas Eliminaci\u00f3n de fila &lt;5% de filas con datos faltantes Simple, limpio Pierdes informaci\u00f3n Eliminaci\u00f3n de columna &gt;50% de valores faltantes Limpio Pierdes feature Imputaci\u00f3n por media/mediana Datos num\u00e9ricos, MCAR* R\u00e1pido Reduce varianza, sesgado Imputaci\u00f3n por forward fill Series temporales Preserva contexto Solo para datos ordenados M\u00e9todos avanzados (KNN, IterativeImputer) Datos con patrones M\u00e1s preciso Computacionalmente caro <p>*MCAR = Missing Completely At Random</p> <p>Ejemplo conceptual:</p> <pre><code># Chequear valores faltantes\nmissing = df.isnull().sum()\nprint(missing)\n# Estrategia 1: Eliminar filas con cualquier faltante\ndf_clean = df.dropna()\n# Estrategia 2: Eliminar columnas con &gt;50% faltantes\ndf_clean = df.dropna(thresh=len(df)*0.5, axis=1)\n# Estrategia 3: Imputaci\u00f3n\ndf_imputed = df.fillna(df.mean())  # num\u00e9ricas\ndf_imputed = df.fillna(df.mode()[0])  # categ\u00f3ricas\n# Estrategia 4: Imputaci\u00f3n avanzada (sklearn)\nfrom sklearn.impute import SimpleImputer, KNNImputer\nimputer = KNNImputer(n_neighbors=5)\ndf_imputed = imputer.fit_transform(df)\n</code></pre> <p>Decisi\u00f3n cr\u00edtica: La estrategia que elijas introduce sesgo. Documentalo siempre.</p>"},{"location":"practicas/00-datos-y-visualizacion/#13-tratamiento-de-outliers","title":"1.3 Tratamiento de Outliers","text":"<p>\u00bfQu\u00e9 son outliers?</p> <p>Valores que se desv\u00edan significativamente del resto de la distribuci\u00f3n. Pueden ser: - Errores reales: Tipogr\u00e1ficos, sensores rotos - Valores leg\u00edtimos: Comportamientos extremos pero reales (ej: un cliente que compra 1000 veces)</p> <p>M\u00e9todos de detecci\u00f3n:</p> M\u00e9todo F\u00f3rmula/Criterio Cu\u00e1ndo usar Desviaci\u00f3n est\u00e1ndar \u03bc \u00b1 3\u03c3 Datos normales IQR Q1 - 1.5\u00d7IQR, Q3 + 1.5\u00d7IQR Datos sesgados, robusta Z-score |z| &gt; 3 Datos normalizados Visualizaci\u00f3n Box plots, scatter plots Siempre, como primer paso <p>Ejemplo:</p> <p></p><pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n# Detecci\u00f3n por IQR\nQ1 = df['SepalLengthCm'].quantile(0.25)\nQ3 = df['SepalLengthCm'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = (df['SepalLengthCm'] &lt; Q1 - 1.5*IQR) | (df['SepalLengthCm'] &gt; Q3 + 1.5*IQR)\nprint(f\"Outliers detectados: {outliers.sum()}\")\n# Visualizaci\u00f3n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n# Box plot\naxes[0].boxplot(df['SepalLengthCm'])\naxes[0].set_title('Box Plot - Sepal Length')\naxes[0].set_ylabel('Valor')\n# Scatter plot\naxes[1].scatter(range(len(df)), df['SepalLengthCm'].sort_values())\naxes[1].set_title('Distribuci\u00f3n de valores')\naxes[1].set_ylabel('Sepal Length (ordenado)')\nplt.tight_layout()\nplt.savefig('./images/outliers_detection.png', dpi=300, bbox_inches='tight')\nplt.close()\n</code></pre> Output esperado:  <p></p> <p>\u00bfEliminar o transformar?</p> <ul> <li>Elimina si: Es claramente un error (ej: edad = -5 a\u00f1os)</li> <li>Transforma si: Es extremo pero leg\u00edtimo. Usa transformaciones (log, ra\u00edz cuadrada) para reducir su impacto sin perder informaci\u00f3n</li> </ul>"},{"location":"practicas/00-datos-y-visualizacion/#14-transformacion-de-variables","title":"1.4 Transformaci\u00f3n de Variables","text":"<p>Normalizaci\u00f3n vs Estandarizaci\u00f3n:</p> <p>Ambas escalan los datos, pero de formas diferentes:</p> T\u00e9cnica F\u00f3rmula Rango Cu\u00e1ndo usar Normalizaci\u00f3n (Min-Max) (x - min) / (max - min) [0, 1] Cuando sabes el rango y quieres limites fijos Estandarizaci\u00f3n (Z-score) (x - \u03bc) / \u03c3 T\u00edpicamente [-3, 3], sin l\u00edmites Por defecto para modelos lineales, SVM, NN Robust Scaling (x - Q2) / IQR - Cuando hay outliers (menos sensible) <p>\u00bfPor qu\u00e9 es cr\u00edtico?</p> <p>Algoritmos como regresi\u00f3n log\u00edstica, SVM y redes neuronales son sensibles a la escala. Una caracter\u00edstica con rango [0, 10000] domina sobre otra en [0, 1]. El modelo aprender\u00e1 m\u00e1s lentamente o converger\u00e1 a soluciones sub\u00f3ptimas.</p> <p>Ejemplo:</p> <pre><code>from sklearn.preprocessing import StandardScaler, MinMaxScaler\n# Valores originales\nprint(\"*\" * 40)\nprint(\"Valores originales:\")\nprint(f\"Min: {df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].min().values}\")\nprint(f\"Max: {df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].max().values}\")\nprint(\"*\" * 40)\n# Estandarizaci\u00f3n (recomendado para modelos lineales)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']])\n# Despu\u00e9s de estandarizar, cada feature tiene media=0 y std=1\nprint(f\"Media: {X_scaled.mean(axis=0)}\")  # ~0\nprint(f\"Std: {X_scaled.std(axis=0)}\")     # ~1\nprint(\"*\" * 40)\n# Normalizaci\u00f3n (alternativa)\nnormalizer = MinMaxScaler()\nX_normalized = normalizer.fit_transform(df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']])\n# Rango: [0, 1]\nprint(f\"Min: {X_normalized.min(axis=0)}\")\nprint(f\"Max: {X_normalized.max(axis=0)}\")\nprint(\"*\" * 40)\n</code></pre>"},{"location":"practicas/00-datos-y-visualizacion/#15-codificacion-de-variables-categoricas","title":"1.5 Codificaci\u00f3n de Variables Categ\u00f3ricas","text":"<p>El problema: Los modelos num\u00e9ricos no entienden \"setosa\", \"versicolor\", \"virginica\".</p> <p>Dos enfoques principales:</p> T\u00e9cnica Ejemplo Cu\u00e1ndo usar Label Encoding setosa \u2192 0, versicolor \u2192 1, virginica \u2192 2 Variables ordinales (peque\u00f1o &lt; medio &lt; grande) One-Hot Encoding setosa \u2192 [1,0,0], versicolor \u2192 [0,1,0], virginica \u2192 [0,0,1] Variables nominales (sin orden natural) <p>\u00bfPor qu\u00e9 importa la elecci\u00f3n?</p> <p>Con Label Encoding, le dices al modelo \"2 &gt; 1 &gt; 0\". Eso introduce un orden falso en variables sin orden natural, sesgando el aprendizaje.</p> <p>Ejemplo:</p> <pre><code>from sklearn.preprocessing import LabelEncoder\nimport pandas as pd\n# Label Encoding (solo si es ordinal)\nle = LabelEncoder()\ndf['species_encoded'] = le.fit_transform(df['Species'])\n# setosa \u2192 0, versicolor \u2192 1, virginica \u2192 2\n# One-Hot Encoding (para nominales - RECOMENDADO para Iris)\ndf_encoded = pd.get_dummies(df, columns=['Species'], drop_first=True)\n# Crea: species_versicolor, species_virginica (drop_first evita multicolinealidad)\nprint(df_encoded.tail())\n</code></pre> <p>Output: </p><pre><code>Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n145  146            6.7           3.0            5.2           2.3   \n146  147            6.3           2.5            5.0           1.9   \n147  148            6.5           3.0            5.2           2.0   \n148  149            6.2           3.4            5.4           2.3   \n149  150            5.9           3.0            5.1           1.8   \n\n     species_encoded  Species_encoded  Species_Iris-versicolor  \\\n145                2                2                    False   \n146                2                2                    False   \n147                2                2                    False   \n148                2                2                    False   \n149                2                2                    False   \n\n     Species_Iris-virginica  \n145                    True  \n146                    True  \n147                    True  \n148                    True  \n149                    True\n</code></pre><p></p> <p>One-Hot Encoding: drop_first=True</p> <p>Con 3 clases, necesitas solo 2 variables dummy. La tercera es redundante (si no es versicolor ni virginica, es setosa). Omitirla evita multicolinealidad perfecta.</p>"},{"location":"practicas/00-datos-y-visualizacion/#2-analisis-exploratorio-de-datos-eda","title":"2. An\u00e1lisis Exploratorio de Datos (EDA)","text":"<p>El EDA es donde empiezas a entender tus datos y a generar hip\u00f3tesis. Es detective work.</p>"},{"location":"practicas/00-datos-y-visualizacion/#21-estadistica-descriptiva","title":"2.1 Estad\u00edstica Descriptiva","text":"<p>\u00bfQu\u00e9 n\u00fameros resumidos nos dicen los datos?</p> <p></p><pre><code># Resumen r\u00e1pido\nprint(df.describe())\n</code></pre> Output para Iris: <pre><code>Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\ncount  150.000000     150.000000    150.000000     150.000000    150.000000\nmean    75.500000       5.843333      3.054000       3.758667      1.198667\nstd     43.445368       0.828066      0.433594       1.764420      0.763161\nmin      1.000000       4.300000      2.000000       1.000000      0.100000\n25%     38.250000       5.100000      2.800000       1.600000      0.300000\n50%     75.500000       5.800000      3.000000       4.350000      1.300000\n75%    112.750000       6.400000      3.300000       5.100000      1.800000\nmax    150.000000       7.900000      4.400000       6.900000      2.500000\n</code></pre><p></p> <p>\u00bfQu\u00e9 significa cada l\u00ednea?</p> <ul> <li>count: N\u00famero de valores no-nulos (verificar valores faltantes)</li> <li>mean: Promedio. Si es muy diferente de la mediana, hay sesgos</li> <li>std: Desviaci\u00f3n est\u00e1ndar. Mide dispersi\u00f3n. Alto = datos heterog\u00e9neos</li> <li>min/max: Rango. Busca valores imposibles o sospechosos</li> <li>25%, 50%, 75%: Cuartiles. Son robustos a outliers, a diferencia de media</li> </ul>"},{"location":"practicas/00-datos-y-visualizacion/#22-distribuciones-univariadas","title":"2.2 Distribuciones Univariadas","text":"<p>Un gr\u00e1fico vale m\u00e1s que 1000 n\u00fameros.</p> <p>Visualizar c\u00f3mo se distribuye cada variable te muestra: - Forma (normal, sesgada, bimodal) - Concentraci\u00f3n de valores - Presencia de outliers</p> <p></p><pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nfeatures = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\ncolors = ['skyblue', 'lightgreen', 'salmon', 'plum']\nfor idx, feature in enumerate(features):\nax = axes[idx // 2, idx % 2]\n# Histograma con KDE\nsns.histplot(data=df, x=feature, kde=True, bins=20, ax=ax, color=colors[idx])\nax.set_title(f'Distribuci\u00f3n de {feature}')\nax.set_xlabel('Valor')\nax.set_ylabel('Frecuencia')\nplt.tight_layout()\nplt.savefig('./images/distribuciones_univariadas.png', dpi=300, bbox_inches='tight')\nplt.close()\n</code></pre> Output esperado: <p></p> <ul> <li>Forma normal (campana): Indicador de datos bien comportados</li> <li>Sesgada (skewed): Cola larga en un lado. Petal length est\u00e1 sesgada a la izquierda (muchos flores peque\u00f1as)</li> <li>Bimodal: Dos picos. Sugiere dos subgrupos (en Iris: especies con flores grandes vs peque\u00f1as)</li> </ul>"},{"location":"practicas/00-datos-y-visualizacion/#23-box-plots","title":"2.3 Box Plots","text":"<p>Resumen r\u00e1pido de distribuci\u00f3n y outliers:</p> <pre><code>fig, axes = plt.subplots(1, 4, figsize=(14, 4))\nfor idx, feature in enumerate(features):\naxes[idx].boxplot(df[feature])\naxes[idx].set_title(feature)\naxes[idx].set_ylabel('Valor')\nplt.tight_layout()\nplt.savefig('images/boxplots.png', dpi=300, bbox_inches='tight')\nplt.close()\n</code></pre> <p>Output esperado: Figura 1: Box Plots</p> <p>Interpretaci\u00f3n: - L\u00ednea central: Mediana (Q2) - Caja: IQR (50% central de datos) - Bigotes: L\u00edmites (Q1 - 1.5\u00d7IQR hasta Q3 + 1.5\u00d7IQR) - Puntos: Outliers</p> <p>Para Iris, notar\u00e1s que petal_length y petal_width tienen distribuciones claramente bimodales \u2192 Diferentes especies tienen tama\u00f1os muy distintos.</p>"},{"location":"practicas/00-datos-y-visualizacion/#24-relaciones-bivariadas","title":"2.4 Relaciones Bivariadas","text":"<p>\u00bfC\u00f3mo se relacionan dos variables?</p> <p>La correlaci\u00f3n mide relaci\u00f3n lineal: - r = 1: Perfecta positiva (X aumenta \u2192 Y aumenta) - r = 0: Sin relaci\u00f3n lineal - r = -1: Perfecta negativa (X aumenta \u2192 Y disminuye)</p> <pre><code># Matriz de correlaci\u00f3n\ncorr_matrix = df[features].corr()\nprint(corr_matrix)\n# Visualizaci\u00f3n\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, \nsquare=True, ax=ax, cbar_kws={'label': 'Correlaci\u00f3n'})\nax.set_title('Matriz de Correlaci\u00f3n - Iris Features')\nplt.tight_layout()\nplt.savefig('./images/correlacion_matrix.png', dpi=300, bbox_inches='tight')\nplt.close()\n</code></pre> <p>Output esperado: </p><pre><code>SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\nSepalLengthCm       1.000000     -0.109369       0.871754      0.817954\nSepalWidthCm       -0.109369      1.000000      -0.420516     -0.356544\nPetalLengthCm       0.871754     -0.420516       1.000000      0.962757\nPetalWidthCm        0.817954     -0.356544       0.962757      1.000000\n</code></pre> <p></p> <p>Interpretaci\u00f3n: - Sepal_length vs petal_length: 0.87 \u2192 Fuerte relaci\u00f3n positiva (flores grandes en ambas medidas) - Sepal_width vs petal_length: -0.42 \u2192 Relaci\u00f3n negativa moderada (flores anchas tienden a ser peque\u00f1as en p\u00e9talos) - Petal_length vs petal_width: 0.96 \u2192 Casi perfecta (p\u00e9talos grandes implican anchos grandes)</p>"},{"location":"practicas/00-datos-y-visualizacion/#25-scatter-plots","title":"2.5 Scatter Plots","text":"<p>Para visualizar relaciones bivariadas directamente:</p> <p></p><pre><code>fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n# Scatter simple: sepal_length vs petal_length\naxes[0].scatter(df['sepal_length'], df['petal_length'], alpha=0.6)\naxes[0].set_xlabel('Sepal Length')\naxes[0].set_ylabel('Petal Length')\naxes[0].set_title('Correlaci\u00f3n: 0.87 (fuerte positiva)')\naxes[0].grid(True, alpha=0.3)\n# Scatter coloreado por species\nfor species in df['species'].unique():\nmask = df['species'] == species\naxes[1].scatter(df[mask]['sepal_length'], df[mask]['petal_length'], \nlabel=species, alpha=0.7)\naxes[1].set_xlabel('Sepal Length')\naxes[1].set_ylabel('Petal Length')\naxes[1].set_title('Por especie')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('./images/scatter_plots.png', dpi=300, bbox_inches='tight')\nplt.close()\n</code></pre> <p></p> <p>\u00bfPor qu\u00e9 colorear por species? Ves que Iris setosa (peque\u00f1a) se agrupa en esquina inferior-izquierda, mientras que Iris virginica (grande) en esquina superior-derecha. Las clases son linealmente separables \u2192 Un clasificador lineal funcionar\u00e1 bien.</p>"},{"location":"practicas/00-datos-y-visualizacion/#26-multicolinealidad","title":"2.6 Multicolinealidad","text":"<p>\u00bfPor qu\u00e9 importa?</p> <p>Multicolinealidad = Dos features muy correlacionadas. En regresi\u00f3n/clasificaci\u00f3n lineal, causa: - Coeficientes inestables (peque\u00f1os cambios en datos \u2192 grandes cambios en coeficientes) - Dificultad interpretando importancia de features - Overfitting potencial</p> <p>Detecci\u00f3n:</p> <pre><code># Mira correlaciones &gt; 0.9\nprint(corr_matrix[corr_matrix &gt; 0.9].sum())\n# En Iris: petal_length y petal_width est\u00e1n altamente correlacionadas (0.96)\n# Pero con 4 features, no es cr\u00edtico\n</code></pre> <p>Decisi\u00f3n: En Iris, no es problema (dataset peque\u00f1o, modelos simples). En datasets grandes, considera: - Eliminar una de las features correlacionadas - Usar regularizaci\u00f3n (L1/L2) - PCA (reducci\u00f3n de dimensionalidad)</p>"},{"location":"practicas/00-datos-y-visualizacion/#27-imbalance-y-representatividad","title":"2.7 Imbalance y Representatividad","text":"<p>\u00bfEst\u00e1n todas las clases bien representadas?</p> <pre><code># Conteo de clases\nprint(df['species'].value_counts())\n# Visualizaci\u00f3n\nfig, ax = plt.subplots(figsize=(8, 5))\ndf['species'].value_counts().plot(kind='bar', ax=ax, color=['blue', 'orange', 'green'])\nax.set_title('Distribuci\u00f3n de clases en Iris')\nax.set_xlabel('Especie')\nax.set_ylabel('Cantidad')\nax.set_xticklabels(ax.get_xticklabels(), rotation=0)\nplt.tight_layout()\nplt.savefig('./images/class_distribution.png', dpi=300, bbox_inches='tight')\nplt.close()\n</code></pre> <p>Output: </p><pre><code>species\nsetosa        50\nversicolor    50\nvirginica     50\n</code></pre><p></p> <p>Figura 2: Distribuci\u00f3n de Clases</p> <p>Interpretaci\u00f3n para Iris: Perfectamente balanceado (50 cada una). En la vida real, raro. Titanic, por ejemplo, tiene ~62% supervivientes, 38% no. En dataset muy desbalanceado (ej: detecci\u00f3n de fraude con 0.1% fraude), m\u00e9tricas como Accuracy enga\u00f1an.</p>"},{"location":"practicas/00-datos-y-visualizacion/#28-patrones-anomalias-e-insights","title":"2.8 Patrones, Anomal\u00edas e Insights","text":"<p>\u00daltimo paso del EDA: Storytelling.</p> <p>Preg\u00fantate: - \u00bfCu\u00e1l es la caracter\u00edstica m\u00e1s importante para predecir la clase? - \u00bfHay grupos naturales en los datos? - \u00bfQu\u00e9 anomal\u00edas notaste?</p> <p>Para Iris: - Petal_length es el mejor discriminante entre especies - Las clases son casi linealmente separables - No hay outliers obvios - Dataset es peque\u00f1o pero limpio</p> <p>Visualizaci\u00f3n final: PCA para contexto (opcional, solo visualizaci\u00f3n)</p> <p></p><pre><code>from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(df[features].values)\nfig, ax = plt.subplots(figsize=(10, 7))\nfor species in df['Species'].unique():\nmask = df['Species'] == species\nax.scatter(X_pca[mask, 0], X_pca[mask, 1], label=species, s=100, alpha=0.7)\nax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} varianza)')\nax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} varianza)')\nax.set_title('Proyecci\u00f3n PCA 2D - Iris Species')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('./images/pca_visualization.png', dpi=300, bbox_inches='tight')\nplt.close()\n</code></pre> <p></p>"},{"location":"practicas/00-datos-y-visualizacion/#3-train-test-split-y-estandarizacion","title":"3. Train-Test Split y Estandarizaci\u00f3n","text":""},{"location":"practicas/00-datos-y-visualizacion/#31-separacion-train-test","title":"3.1 Separaci\u00f3n Train-Test","text":"<p>\u00bfPor qu\u00e9 es cr\u00edtico?</p> <p>Entrenar y evaluar en los mismos datos es trampa. El modelo memoriza. La m\u00e9trica resultante no estima performance en datos nuevos (que es lo que importa).</p> <p>Analog\u00eda: Es como estudiante que memoriza las preguntas exactas de un examen pasado. Cuando llega el examen real con preguntas similares pero no id\u00e9nticas, se hunde.</p> <p></p><pre><code>from sklearn.model_selection import train_test_split\n# Split 80-20\nX = df[features].values\ny = df['species'].values\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.2, random_state=42, stratify=y\n)\nprint(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n# Train: (120, 4), Test: (30, 4)\n# stratify=y asegura que cada split tenga proporci\u00f3n similar de clases\nprint(f\"Train - setosa: {sum(y_train == 'setosa')}/120\")\nprint(f\"Test - setosa: {sum(y_test == 'setosa')}/30\")\n</code></pre> Output esperado: <pre><code>Train: (120, 4), Test: (30, 4)\nTrain - setosa: 0/120\nTest - setosa: 0/30\n</code></pre><p></p> <p>NUNCA hagas normalizaci\u00f3n/imputaci\u00f3n antes de splitear</p> <p>INCORRECTO: </p><pre><code>X_normalized = scaler.fit_transform(X)  # FIT EN TODO\nX_train, X_test, ... = train_test_split(X_normalized, y, ...)\n</code></pre><p></p> <p>CORRECTO: </p><pre><code>X_train, X_test, ... = train_test_split(X, y, ...)\nscaler.fit(X_train)  # FIT solo en train\nX_train_norm = scaler.transform(X_train)\nX_test_norm = scaler.transform(X_test)\n</code></pre><p></p>"},{"location":"practicas/00-datos-y-visualizacion/#32-estandarizacion-del-conjunto-de-entrenamiento","title":"3.2 Estandarizaci\u00f3n del Conjunto de Entrenamiento","text":"<p>Recuerda: La estandarizaci\u00f3n es cr\u00edtica para modelos lineales (logistic regression, SVM, etc.).</p> <pre><code>from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)  # FIT SOLO en train\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n# Verificar\nprint(f\"Train mean: {X_train_scaled.mean(axis=0)}\")  # ~0\nprint(f\"Train std: {X_train_scaled.std(axis=0)}\")    # ~1\nprint(f\"Test mean: {X_test_scaled.mean(axis=0)}\")    # No es ~0 (datos nuevos)\n</code></pre> <p>Output esperado: </p><pre><code>Train mean: [-1.20829273e-15 -2.53315887e-15  1.48029737e-16  1.55246186e-15]\nTrain std: [1. 1. 1. 1.]\nTest mean: [ 0.00995126  0.11078352 -0.03456365 -0.03615399]\n</code></pre><p></p> <p>\u00bfPor qu\u00e9 test no tiene mean=0?</p> <p>El scaler fue ajustado con estad\u00edsticas del train. Al aplicarlo al test (datos nuevos), el test conserva su propia distribuci\u00f3n. Esto es correcto: validamos con datos en su distribuci\u00f3n real.</p>"},{"location":"practicas/00-datos-y-visualizacion/#33-modelo-regresion-logistica","title":"3.3 Modelo: Regresi\u00f3n Log\u00edstica","text":"<p>\u00bfQu\u00e9 es?</p> <p>A pesar del nombre, es un clasificador, no regresi\u00f3n. Predice probabilidades de pertenencia a clases.</p> <p>Conceptualmente: - Para 2 clases: P(Y=1) = sigmoid(w\u2080 + w\u2081X\u2081 + w\u2082X\u2082 + ...) - Para 3+ clases (como Iris): Softmax (extensi\u00f3n de sigmoid)</p> <p>Interpretaci\u00f3n: - Coeficientes positivos: Aumentan probabilidad de la clase - Coeficientes negativos: Disminuyen probabilidad</p> <pre><code>from sklearn.linear_model import LogisticRegression\n# Crear modelo\nmodel = LogisticRegression(\nmax_iter=1000,\nrandom_state=42,\nmulti_class='multinomial'  # Para 3+ clases\n)\n# Entrenar\nmodel.fit(X_train_scaled, y_train)\n# Predicciones\ny_pred_train = model.predict(X_train_scaled)\ny_pred_test = model.predict(X_test_scaled)\n# Probabilidades\ny_proba_test = model.predict_proba(X_test_scaled)\nprint(y_proba_test[:5])  # [prob_setosa, prob_versicolor, prob_virginica]\n</code></pre> <p>Output esperado (probabilidades): </p><pre><code>Probabilidades de las primeras 5 muestras del test:\n[\n    [9.79355498e-01 2.06441434e-02 3.58223033e-07]\n    [3.77309225e-03 3.69498929e-01 6.26727979e-01]\n    [1.49647017e-01 8.41581277e-01 8.77170654e-03]\n    [9.58890418e-02 8.94120385e-01 9.99057342e-03]\n    [9.88807815e-01 1.11920151e-02 1.69552410e-07]\n]\n</code></pre><p></p>"},{"location":"practicas/00-datos-y-visualizacion/#34-metricas-de-clasificacion","title":"3.4 M\u00e9tricas de Clasificaci\u00f3n","text":"<p>\u26a0\ufe0f ATENCI\u00d3N</p> <p>Las m\u00e9tricas que ves a continuaci\u00f3n son para CLASIFICACI\u00d3N.</p> <p>Para REGRESI\u00d3N, usar\u00edas: - MSE (Mean Squared Error): Promedio de (y_true - y_pred)\u00b2 - RMSE (Root MSE): \u221aMSE (misma escala que y) - MAE (Mean Absolute Error): Promedio de |y_true - y_pred|</p> <p>Clasificaci\u00f3n - M\u00e9tricas principales:</p> <pre><code>from sklearn.metrics import (\naccuracy_score, precision_score, recall_score, f1_score, \nconfusion_matrix, classification_report\n)\n# Accuracy: % correcto\nacc = accuracy_score(y_test, y_pred_test)\nprint(f\"Accuracy: {acc:.4f}\")  # 1.0 (perfecto en este dataset)\n# Precision (para clase i): TP / (TP + FP) - \"de lo que predije positivo, cu\u00e1nto fue correcto\"\n# Recall (para clase i): TP / (TP + FN) - \"de los verdaderos positivos, cu\u00e1ntos encontr\u00e9\"\n#\n#  F1: Media arm\u00f3nica de Precision y Recall - \"balance entre ambos\"\nprint(classification_report(y_test, y_pred_test))\n</code></pre> <p>Output t\u00edpico: </p><pre><code>              Accuracy: 0.9333\n                 precision    recall  f1-score   support\n\n    Iris-setosa       1.00      1.00      1.00        10\nIris-versicolor       0.90      0.90      0.90        10\n Iris-virginica       0.90      0.90      0.90        10\n\n       accuracy                           0.93        30\n      macro avg       0.93      0.93      0.93        30\n   weighted avg       0.93      0.93      0.93        30\n</code></pre><p></p> <p>Matriz de confusi\u00f3n:</p> <p></p><pre><code>cm = confusion_matrix(y_test, y_pred_test)\nprint(cm)\n# Visualizaci\u00f3n\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\nxticklabels=model.classes_, yticklabels=model.classes_)\nax.set_xlabel('Predicho')\nax.set_ylabel('Real')\nax.set_title('Matriz de Confusi\u00f3n')\nplt.tight_layout()\nplt.savefig('./images/confusion_matrix.png', dpi=300, bbox_inches='tight')\nplt.close()\n</code></pre> <p></p> <p>Interpretaci\u00f3n: - Diagonal principal: Aciertos - Fuera de diagonal: Errores</p>"},{"location":"practicas/00-datos-y-visualizacion/#35-validacion-cruzada","title":"3.5 Validaci\u00f3n Cruzada","text":"<p>Problema: Un \u00fanico split train-test puede ser afortunado (test f\u00e1cil) o desafortunado (test dif\u00edcil).</p> <p>Soluci\u00f3n: Validaci\u00f3n cruzada (k-fold)</p> <p>Divide datos en k folds. Entrena k modelos, cada uno dejando un fold para validar:</p> <pre><code>from sklearn.model_selection import cross_validate\n# 5-fold cross-validation\ncv_results = cross_validate(\nmodel,\nX_train_scaled, y_train,\ncv=5,\nscoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n)\nprint(f\"CV Accuracy: {cv_results['test_accuracy'].mean():.4f} \u00b1 {cv_results['test_accuracy'].std():.4f}\")\nprint(f\"CV F1: {cv_results['test_f1_weighted'].mean():.4f} \u00b1 {cv_results['test_f1_weighted'].std():.4f}\")\n</code></pre> <p>Output: </p><pre><code>CV Accuracy: 0.9583 \u00b1 0.0264\nCV F1: 0.9580 \u00b1 0.0268\n</code></pre><p></p> <p>\u00bfQu\u00e9 significa?</p> <p>Media 95.83% \u00b1 2.64% \u2192 En promedio, 95.83%, pero var\u00eda \u00b12.68% entre folds. Desviaci\u00f3n baja = modelo estable.</p>"},{"location":"practicas/00-datos-y-visualizacion/#4-pipelines-en-machine-learning","title":"4. Pipelines en Machine Learning","text":"<p>Los pipelines encadenan m\u00faltiples transformaciones y modelos en un \u00fanico objeto reutilizable y reproducible. Es la forma profesional de hacer ML.</p>"},{"location":"practicas/00-datos-y-visualizacion/#41-por-que-pipelines","title":"4.1 \u00bfPor qu\u00e9 Pipelines?","text":"<p>Sin pipeline (Fr\u00e1gil):</p> <pre><code>scaler = StandardScaler()\nX_train_s = scaler.fit_transform(X_train)\nX_test_s = scaler.transform(X_test)\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train_s, y_train)\ny_pred = model.predict(X_test_s)\n</code></pre> <p>Problemas: - F\u00e1cil olvidar pasos - Data leakage accidental - C\u00f3digo repetitivo - Dif\u00edcil de reproducir</p> <p>Con pipeline (Profesional):</p> <pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\npipe = Pipeline([\n('scaler', StandardScaler()),\n('model', LogisticRegression(max_iter=1000, random_state=42))\n])\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\n</code></pre> <p>Ventajas: - C\u00f3digo limpio y legible - Imposible data leakage - F\u00e1cil cambiar componentes - Funciona autom\u00e1ticamente con validaci\u00f3n cruzada y grid search - Production-ready</p>"},{"location":"practicas/00-datos-y-visualizacion/#42-pipeline-basico","title":"4.2 Pipeline B\u00e1sico","text":"<pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score\n# Crear pipeline\npipe = Pipeline([\n('scaler', StandardScaler()),\n('logistic', LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial'))\n])\n# Entrenar\npipe.fit(X_train, y_train)\n# Predecir\ny_pred = pipe.predict(X_test)\ny_proba = pipe.predict_proba(X_test)\n# Evaluar\nacc = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred, average='weighted')\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1: {f1:.4f}\")\n</code></pre>"},{"location":"practicas/00-datos-y-visualizacion/#43-columntransformer-para-multiples-tipos-de-datos","title":"4.3 ColumnTransformer (para m\u00faltiples tipos de datos)","text":"<p>Aunque Iris solo tiene num\u00e9ricas, ense\u00f1amos el patr\u00f3n completo:</p> <pre><code>from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n# Definir tipos de features\nnumeric_features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n# Transformer para num\u00e9ricas\nnumeric_transformer = Pipeline([\n('scaler', StandardScaler())\n])\n# Combinar transformers\npreprocessor = ColumnTransformer([\n('num', numeric_transformer, numeric_features)\n])\n# Pipeline completo\nfull_pipeline = Pipeline([\n('preprocessor', preprocessor),\n('model', LogisticRegression(max_iter=1000, random_state=42))\n])\n# Usar igual que antes\nfull_pipeline.fit(X_train, y_train)\ny_pred = full_pipeline.predict(X_test)\n</code></pre>"},{"location":"practicas/00-datos-y-visualizacion/#44-pipeline-con-gridsearchcv","title":"4.4 Pipeline con GridSearchCV","text":"<p>Tunear autom\u00e1ticamente TODO:</p> <pre><code>from sklearn.model_selection import GridSearchCV\npipe = Pipeline([\n('scaler', StandardScaler()),\n('model', LogisticRegression(random_state=42))\n])\n# Par\u00e1metros a tunear\nparam_grid = {\n'model__C': [0.01, 0.1, 1, 10],\n'model__max_iter': [1000, 2000]\n}\n# Grid search\ngrid = GridSearchCV(pipe, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1)\ngrid.fit(X_train, y_train)\nprint(f\"Mejor C: {grid.best_params_['model__C']}\")\nprint(f\"Mejor max_iter: {grid.best_params_['model__max_iter']}\")\nprint(f\"Mejor score CV: {grid.best_score_:.4f}\")\n# Predecir con mejor modelo\ny_pred = grid.predict(X_test)\n</code></pre>"},{"location":"practicas/00-datos-y-visualizacion/#45-ventajas-de-pipelines-tabla-resumen","title":"4.5 Ventajas de Pipelines - Tabla Resumen","text":"Aspecto Sin Pipeline Con Pipeline Data leakage F\u00e1cil olvidarse Imposible Reproducibilidad Fr\u00e1gil Robusta C\u00f3digo Repetitivo Limpio Cambiar componentes Tedioso Una l\u00ednea CV/GridSearch Manual Autom\u00e1tico Producci\u00f3n Fr\u00e1gil Ready"},{"location":"practicas/00-datos-y-visualizacion/#5-tareas-para-casa","title":"5. Tareas para Casa","text":""},{"location":"practicas/00-datos-y-visualizacion/#objetivo","title":"Objetivo","text":"<p>Replicar el flujo completo de ML en un dataset de tu elecci\u00f3n.</p>"},{"location":"practicas/00-datos-y-visualizacion/#opciones-de-dataset","title":"Opciones de Dataset","text":"<p>Opci\u00f3n 1: Titanic (Recomendado para empezar) - 890 registros, 11 features - Objetivo: Supervivencia (binaria) - Desaf\u00edo: Valores faltantes, mezcla de tipos - Descarga: https://www.kaggle.com/c/titanic/data</p> <p>Opci\u00f3n 2: Wine Quality - 1,600 registros, 12 features - Objetivo: Calidad (0-10) - Descarga: https://www.kaggle.com/datasets/yasserh/wine-quality-dataset</p> <p>Opci\u00f3n 3: Adult Income - 32,500 registros, 14 features - Objetivo: Ingresos &gt;50K (binaria) - Descarga: https://archive.ics.uci.edu/ml/datasets/Adult</p> <p>Opci\u00f3n 4: Tu propio dataset - M\u00ednimo 2,000-4,000 registros - M\u00ednimo 10+ columnas - Mezcla de num\u00e9ricas y categ\u00f3ricas - Variable objetivo clara</p>"},{"location":"practicas/00-datos-y-visualizacion/#estructura-de-entrega","title":"Estructura de Entrega","text":"<p>MEMOR\u00cdA EN LATEX (Overleaf)</p>"}]}