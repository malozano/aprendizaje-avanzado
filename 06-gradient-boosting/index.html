<!DOCTYPE html><html lang="es" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Apuntes de la asignatura Aprendizaje Avanzado del Grado en Ingeniería en Inteligencia Artificial de la Universidad de Alicante">
      
      
        <meta name="author" content="Miguel Angel Lozano">
      
      
        <link rel="canonical" href="https://malozano.github.io/aprendizaje-avanzado/06-gradient-boosting/">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.1.1">
    
    
      
        <title>Gradient Boosting - Aprendizaje Avanzado</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.402914a4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.a0c5b2b5.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#gradient-boosting" class="md-skip">
          Saltar a contenido
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Cabecera">
    <a href=".." title="Aprendizaje Avanzado" class="md-header__button md-logo" aria-label="Aprendizaje Avanzado" data-md-component="logo">
      
  <img src="../images/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Aprendizaje Avanzado
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Gradient Boosting
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo" aria-label="Modo oscuro" type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Modo oscuro" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
            </label>
          
        
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="white" data-md-color-accent="blue" aria-label="Modo claro" type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Modo claro" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Búsqueda" placeholder="Búsqueda" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Buscar">
        
        <button type="reset" class="md-search__icon md-icon" title="Limpiar" aria-label="Limpiar" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Inicializando búsqueda
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navegación" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Aprendizaje Avanzado" class="md-nav__button md-logo" aria-label="Aprendizaje Avanzado" data-md-component="logo">
      
  <img src="../images/logo.png" alt="logo">

    </a>
    Aprendizaje Avanzado
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Introducción
      </a>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2">
      
      
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Bloque I. Aprendizaje supervisado
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Bloque I. Aprendizaje supervisado
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01-modelos-no-param/" class="md-nav__link">
        1. Modelos paramétricos y no paramétricos. Logistic Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02-svm/" class="md-nav__link">
        2. SVM
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03-arboles-decision/" class="md-nav__link">
        3. Árboles de decisión
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04-random-forest/" class="md-nav__link">
        4. Métodos de ensemble. Random Forest
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05-adaboost/" class="md-nav__link">
        5. Boosting. Adaboost
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3">
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          Prácticas
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Prácticas
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../practicas/00-datos-y-visualizacion/" class="md-nav__link">
        0. Datos y visualización
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../practicas/01-aprendizaje-supervisado/" class="md-nav__link">
        1. Aprendizaje supervisado
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="gradient-boosting">Gradient Boosting<a class="headerlink" href="#gradient-boosting" title="Permanent link">¶</a></h1>
<p>Gradient Boosting (Friedman, 2001)<sup id="fnref:friedman2001greedy"><a class="footnote-ref" href="#fn:friedman2001greedy">1</a></sup> generaliza la idea de boosting mediante una perspectiva de <strong>optimización numérica</strong>. En lugar de ajustar pesos de ejemplos (como AdaBoost), cada nuevo modelo se ajusta al <strong>gradiente negativo de la función de pérdida</strong>. Gradient Boosting es la base teórica de métodos modernos como XGBoost (Chen &amp; Guestrin, 2016)<sup id="fnref:chen2016xgboost"><a class="footnote-ref" href="#fn:chen2016xgboost">2</a></sup>, LightGBM (Ke et al., 2017)<sup id="fnref:ke2017lightgbm"><a class="footnote-ref" href="#fn:ke2017lightgbm">3</a></sup> y CatBoost (Prokhorenkova et al., 2018)<sup id="fnref:prokhorenkova2018catboost"><a class="footnote-ref" href="#fn:prokhorenkova2018catboost">4</a></sup>.</p>
<h2 id="marco-conceptual">Marco Conceptual<a class="headerlink" href="#marco-conceptual" title="Permanent link">¶</a></h2>
<p>La idea clave tras Gradient Boosting es considerar el <em>boosting</em> como un problema de optimización en el espacio de funciones. Tratamos de encontrar una función <span class="arithmatex">\(F(x)\)</span> que minimice la pérdida esperada:</p>
<div class="arithmatex">\[F^* = \arg\min_F \mathbb{E}_{x,y}[L(y, F(x))]\]</div>
<p>Donde <span class="arithmatex">\(L\)</span> es una función de pérdida diferenciable.</p>
<p>Como aproximación, construiremos <span class="arithmatex">\(F\)</span> como una suma de funciones más simples:</p>
<div class="arithmatex">\[F(x) = \sum_{m=0}^{M} f_m(x)\]</div>
<p>Donde cada <span class="arithmatex">\(f_m\)</span> es un modelo débil (típicamente un árbol).</p>
<p>Para entender el funcionamiento de Gradient Boosting, podemos establecer una <strong>analogía con el método de descenso por gradiente</strong>. Con este método buscamos optimizar un conjunto de parámetros <span class="arithmatex">\(\theta\)</span>, y para ello en cada iteración obtenemos el gradiente de la función de pérdida respecto a estos parámetros, y optimizamos los parámetros en la dirección contraria al gradiente:</p>
<div class="arithmatex">\[
\begin{align*}
&amp;\theta \leftarrow \theta_0 \quad \text{(Inicializamos los parámetros)} \\
&amp; \text{Para cada } t \in \{1, \ldots, T\} \\
&amp; \quad G \leftarrow \nabla L(\theta) \quad \text{(Gradiente respecto a los parámetros)}\\
&amp; \quad \theta \leftarrow \theta - \eta G \quad \text{(Actualización de los parámetros)}
\end{align*}
\]</div>
<p>En Gradient Boosting haremos algo similar, pero en el <strong>espacio de funciones</strong>. Recordemos que al tratarse de un método de <em>boosting</em>, iremos añadiendo estimadores al <em>ensemble</em> de forma secuencial. De esta forma, en cada iteración <span class="arithmatex">\(m\)</span> calculamos el gradiente de la función de pérdida con el <em>ensemble</em> obtenido hasta el momento, y añadiremos un nuevo estimador <span class="arithmatex">\(h_m\)</span> que será entrenado a partir de los gradientes obtenidos.</p>
<div class="arithmatex">\[
\begin{align*}
&amp;F \leftarrow F_0 \quad \text{(Inicializamos el ensemble)} \\
&amp; \text{Para cada } m \in \{1, \ldots, M\} \\
&amp; \quad G \leftarrow \nabla L(F) \quad \text{(Gradiente respecto a las predicciones)}\\
&amp; \quad f_m \leftarrow fit(G) \quad \text{(Ajustamos un nuevo modelo a los gradientes)}\\
&amp; \quad F \leftarrow F - \eta h_m \quad \text{(Actualizamos el ensemble)}
\end{align*}
\]</div>
<h2 id="algoritmo-de-gradient-boosting">Algoritmo de Gradient Boosting<a class="headerlink" href="#algoritmo-de-gradient-boosting" title="Permanent link">¶</a></h2>
<p>Vamos a continuación a detallar los diferentes pasos del algoritmo.</p>
<h3 id="inicializacion">Inicialización<a class="headerlink" href="#inicializacion" title="Permanent link">¶</a></h3>
<p>Como primer paso deberemos <strong>inicializar</strong> el <em>ensemble</em>. Consideremos que tenemos un <em>dataset</em> <span class="arithmatex">\(\mathcal{D}\)</span> con <span class="arithmatex">\(N\)</span> ejemplos de entrenamiento <span class="arithmatex">\((\mathbf{x}_i, y_i)\)</span>, con <span class="arithmatex">\(i = 1, 2, \ldots, N\)</span>. Vamos a buscar un valor constante <span class="arithmatex">\(\gamma\)</span> que minimice la pérdida para el conjunto completo de observaciones <span class="arithmatex">\(y_i\)</span>:</p>
<div class="arithmatex">\[
F_0 = \arg \min_\gamma \sum_{i=1}^N L(y_i, \gamma)
\]</div>
<p>La forma de buscar este valor diferirá según si se enfoca a un problema de clasificación o de regresión. Tipicamente utilizaremos las siguientes funciones como inicialización:</p>
<h4 id="regresion">Regresión<a class="headerlink" href="#regresion" title="Permanent link">¶</a></h4>
<p>Considerando que la pérdida es MSE, minimizaremos el error si devolvemos como constante la media de las observaciones del conjunto de entrenamiento:
$$
F_0 = \frac{1}{N} \sum_{i=1}^N y_i = \bar{y} 
$$ </p>
<h4 id="clasificacion-binaria">Clasificación binaria<a class="headerlink" href="#clasificacion-binaria" title="Permanent link">¶</a></h4>
<p>En este caso, debemos tener en consideración la función de pérdida utilizada. Consideremos el caso de <em>log-loss</em> para clasificación binaria, con <span class="arithmatex">\(y_i \in \{0, 1\}\)</span>:</p>
<div class="arithmatex">\[
L(y_i, F) = - [y_i \ln p_i + (1-y_i) \ln(1-p_i)], \quad p_i = \sigma(F(\mathbf{x}_i))
\]</div>
<p>Nótese que en este caso la función <span class="arithmatex">\(F(\mathbf{x}_i)\)</span> clasificará los ejemplos en función de <span class="arithmatex">\(\text{signo}(F(\mathbf{x}_i))\)</span>, al igual que ocurría con la clasificación mediante un hiperplano. Si aplicamos la función Sigmoide (<span class="arithmatex">\(\sigma\)</span>) entonces podemos interpretar <span class="arithmatex">\(p_i\)</span> como la probabilidad de el ejemplo de entrada <span class="arithmatex">\(\mathbf{x}_i\)</span> pertenezca a la clase positiva, al igual que ocurría en el caso de Regresión Logística.</p>
<p>Si minimizamos esta función de pérdida <em>log-loss</em> respecto a una constante <span class="arithmatex">\(\gamma\)</span>, obtenemos el siguiente valor óptimo para la inicialización:</p>
<div class="arithmatex">\[
F_0 = \ln \left( \frac{\bar{p}}{1-\bar{p}} \right) 
\]</div>
<p>Donde <span class="arithmatex">\(\bar{p} = \bar{y}\)</span> (media de todas las observaciones) sería la proporción de ejemplos que pertenecen a la clase positiva.</p>
<h4 id="clasificacion-multiclase">Clasificación multiclase<a class="headerlink" href="#clasificacion-multiclase" title="Permanent link">¶</a></h4>
<p>En este caso se generaliza con <span class="arithmatex">\(K\)</span> funciones, una para cada clase, inicializadas de la siguiente forma:</p>
<div class="arithmatex">\[
F_0^{(k)}  = \ln \frac{N_k}{N}, \quad k = 1, \ldots, K
\]</div>
<p>Donde <span class="arithmatex">\(N_k\)</span> es el número de ejemplos que pertenecen a la clase <span class="arithmatex">\(k\)</span>. De este forma, estamos inicializando a partir de la probabilidad de que un ejemplo pertenezca a cada clase.</p>
<p>Es decir, estamos en este caso siguiendo una estrategia <em>One-vs-Rest</em> implícita, en la que entrenamos <span class="arithmatex">\(K\)</span> clasificadores binarios en paralelo y obtenemos la predicción final aplicando una función <em>softmax</em> a todos ellos.</p>
<h3 id="obtencion-del-siguiente-modelo">Obtención del siguiente modelo<a class="headerlink" href="#obtencion-del-siguiente-modelo" title="Permanent link">¶</a></h3>
<p>Una vez inicializada la función del <em>ensemble</em>, deberemos generar <span class="arithmatex">\(M\)</span> modelos de forma iterativa. El siguiente modelo <span class="arithmatex">\(h_m\)</span>, con <span class="arithmatex">\(m \in \{1, 2, \ldots, M \}\)</span>, se entrenará teniendo en cuenta los gradientes de la función de pérdida con la función <span class="arithmatex">\(F_{m-1}\)</span> actualizada hasta la iteración anterior, de forma que el nuevo modelo se centre en corregir los errores existentes. Esta función se calcula como:</p>
<div class="arithmatex">\[
F_{m-1}(\mathbf{x}) = F_0 + \sum_{j=1}^{m-1} \eta h_j(\mathbf{x}) 
\]</div>
<p>Donde <span class="arithmatex">\(\eta \in (0, 1]\)</span> es el <strong><em>learning rate</em></strong>, conocido como <em>shrinkage</em>, debido a que tiene el efecto de "encoger" la contribución de cada modelo. Cuando el valor es alto, cercano a <span class="arithmatex">\(1\)</span>, el algoritmo convergerá rápido con pocos estimadores, con lo cual tendremos riesgo de <em>overfitting</em>. Sin embargo, si tenemos un valor pequeño, cercano a <span class="arithmatex">\(0\)</span>, cada estimador contribuirá muy poco y al necesitar más estimadores para converger tendremos también mejor generalización. Básicamente, con <em>learning rates</em> más pequeños tendremos modelos más robustos, pero a costa de un mayor tiempo de entrenamiento. </p>
<p>Vamos a continuación a ver los pasos a seguir para obtener el siguiente modelo <span class="arithmatex">\(h_m\)</span>. Nótese que en la primera iteración <span class="arithmatex">\(m=1\)</span> nuestro modelo anterior será unicamente <span class="arithmatex">\(F_0\)</span>, tal como se ha inicializado en el paso anterior.</p>
<h4 id="calcular-los-pseudo-residuos">Calcular los pseudo-residuos<a class="headerlink" href="#calcular-los-pseudo-residuos" title="Permanent link">¶</a></h4>
<p>Cuando hablamos de pseudo-residuos nos referiremos a los errores que el siguiente modelo <span class="arithmatex">\(h_m\)</span> debe corregir, y que obtendremos a partir del gradiente negativo. Para cada ejemplo de entrada <span class="arithmatex">\(i \in \{1, 2, \ldots, N\}\)</span> tenemos:</p>
<div class="arithmatex">\[
r_{im} = - \left[ \frac{\partial L(y_i, F(\mathbf{x}_i))}{ \partial F(\mathbf{x}_i)} \right]_{F=F_{m-1}}
\]</div>
<p>Por ejemplo, en caso de <strong>clasificación binaria</strong> con <em>log-loss</em>, donde <span class="arithmatex">\(p_i = \sigma(F_{m-1}(\mathbf{x}_i))\)</span>, tendríamos:</p>
<div class="arithmatex">\[
r_{im} = y_i - p_i
\]</div>
<p>Es decir, la diferencia entre la etiqueta observada y la probabilidad predicha por el modelo. </p>
<p>En el caso de <strong>regresión</strong> con MSE tendríamos:</p>
<div class="arithmatex">\[
r_{im} = y_i - F_{m-1}(\mathbf{x}_i)
\]</div>
<p>En este caso coincide con los residuos clásicos en un problema de regresión.</p>
<h4 id="ajuste-del-modelo-a-los-pseudo-residuos">Ajuste del modelo a los pseudo-residuos<a class="headerlink" href="#ajuste-del-modelo-a-los-pseudo-residuos" title="Permanent link">¶</a></h4>
<p>Deberemos ajustar el siguiente modelo débil <span class="arithmatex">\(h_m\)</span> a estos residuos. Para ello tenemos un nuevo <em>dataset</em> de entrenamiento, donde los valores de entrada son los mismos pero las salidas serán los residuos:</p>
<div class="arithmatex">\[
\mathcal{D}_m = \{(\mathbf{x}_1, r_{1m}), (\mathbf{x}_2, r_{2m}), \ldots, (\mathbf{x}_N, r_{Nm}) \}
\]</div>
<p>Entrenaremos el nuevo modelo <span class="arithmatex">\(h_m\)</span> sobre este <em>dataset</em>, minimizando:</p>
<div class="arithmatex">\[
h_m = \arg \min_h \sum_{i=1}^N (r_{im} - h(\mathbf{x}_i))^2
\]</div>
<p>Nótese que aunque el problema original fuera de clasificación, en este caso estamos entrenando siempre un modelo base de regresión, ya que los residuos son valores continuos. Como hemos comentado, normalmente utilizaremos árboles como modelos base, por lo que concretamente se tratará de <strong>árboles de regresión</strong>.</p>
<p>Como ya tratamos en el tema sobre árboles de decisión, cada hoja del árbol corresponde a una región, de forma que el árbol <span class="arithmatex">\(h_m\)</span> dividirá el espacio en <span class="arithmatex">\(J\)</span> regiones <span class="arithmatex">\(R_{jm}\)</span>. Dentro de cada región (hoja), el árbol de regresión asignaría la media de los pseudo-residuos como predicción, pero en la práctica dentro de Gradient Boosting se podrá refinar este valor buscando el valor <span class="arithmatex">\(\gamma_{jm}\)</span> que minimice la función de pérdida original, como se hizo a la hora de obtener <span class="arithmatex">\(F_0\)</span>:</p>
<div class="arithmatex">\[
\gamma_{jm} = \arg \min_\gamma \sum_{\mathbf{x}_i \in R_{jm}} L(y_i, F_{m-1}(\mathbf{x}_i)+ \gamma)
\]</div>
<p>Esto será especialmente relevante en el caso de tener como función de pérdida <em>log-loss</em>, donde la media de los pseudo-residuos no es el óptimo para la pérdida original. En caso de que la función de pérdida fuera MSE, <span class="arithmatex">\(\gamma_{jm}\)</span> se obtendría como la media de los pseudo-residuos en cada hoja, al igual que en el caso de los árboles de regresión estándar.</p>
<h3 id="actualizacion-del-modelo">Actualización del modelo<a class="headerlink" href="#actualizacion-del-modelo" title="Permanent link">¶</a></h3>
<p>Considerando que utilizamos árboles de regresión como modelo base, ajustados tal como se ha visto en el paso anterior, la actualización del <em>ensemble</em> se hará de la siguiente forma:</p>
<div class="arithmatex">\[
F_m(\mathbf{x}) = F_{m-1}(\mathbf{x}) + \eta \sum_{j=1}^J \gamma_{jm} 1[\mathbf{x} \in R_{jm}]
\]</div>
<p>Es decir, la función <span class="arithmatex">\(F_{m-1}\)</span> de la iteración anterior se "corrige" con un árbol de regresión que ha aprendido los residuales (las diferencias entre el valor observado y el valor predicho por la función anterior)</p>
<blockquote>
<p><strong>Nota</strong>: En el algoritmo original de Friedman se contempla un multiplicador global <span class="arithmatex">\(\rho_m\)</span> para cada árbol, de forma análoga a los multiplicadores <span class="arithmatex">\(\alpha_m\)</span> de AdaBoost. De esta forma en cada iteración se actualizaría el <em>ensemble</em> de la siguiente forma:
$$
F_m(\mathbf{x}) = F_{m-1}(\mathbf{x}) + \eta \rho_m h_m(\mathbf{x}) $$
Este multiplicador se obtiene de la siguiente forma:
$$
\rho_m = \arg \min_\rho \sum_{i=1}^N L(y_i, F_{m-1}(\mathbf{x}_i) + \rho h_m(\mathbf{x}_i)) $$
Es decir, una vez entrenado el árbol h_m se busca el escalar óptimo que más reduce la pérdida en esa dirección. Sin embargo, en la práctica la optimización por hoja <span class="arithmatex">\(\gamma_{jm}\)</span> lo hace redundante. Si ya ajustamos un valor óptimo por hoja, el multiplicador global no añade nada, por lo que habitualmente en la práctica se fija <span class="arithmatex">\(\rho_m = 1\)</span>.</p>
</blockquote>
<h3 id="prediccion-final">Predicción final<a class="headerlink" href="#prediccion-final" title="Permanent link">¶</a></h3>
<p>Una vez obtenidos los <span class="arithmatex">\(M\)</span> árboles, el modelo final será la suma de todas las contribuciones:</p>
<div class="arithmatex">\[
\hat{y} = F_M(\mathbf{x}) = F_0(\mathbf{x}) + \eta \sum_{m=1}^M h_m(\mathbf{x})
\]</div>
<h4 id="regresion_1">Regresión<a class="headerlink" href="#regresion_1" title="Permanent link">¶</a></h4>
<p>En el caso de <strong>regresión</strong>, la predicción final será directamente el valor de <span class="arithmatex">\(F_M(\mathbf{x})\)</span>:</p>
<div class="arithmatex">\[
\hat{y} = F_M(\mathbf{x})
\]</div>
<h4 id="clasificacion-binaria_1">Clasificación binaria<a class="headerlink" href="#clasificacion-binaria_1" title="Permanent link">¶</a></h4>
<p>En caso de <strong>clasificación binaria</strong> aplicaremos la Sigmoide para obtener la probabilidad de pertenencia a la clase positiva <span class="arithmatex">\(\hat{p}\)</span>:</p>
<div class="arithmatex">\[
\hat{p} = \sigma(F_M(\mathbf{x}))
\]</div>
<p>Si <span class="arithmatex">\(\hat{p} &gt; 0.5\)</span> se predecirá la clase positiva, y en caso contrario se predecirá la clase negativa.</p>
<h4 id="clasificacion-multiclase_1">Clasificación multiclase<a class="headerlink" href="#clasificacion-multiclase_1" title="Permanent link">¶</a></h4>
<p>En caso de <strong>clasificación multiclase</strong>, tendremos <span class="arithmatex">\(K\)</span> funciones acumuladas <span class="arithmatex">\(F_M^{(k)}(\mathbf{x})\)</span>, y aplicamos <em>softmax</em> sobre ellas para obtener la probabilidad <span class="arithmatex">\(\hat{p}_k\)</span> de pertenencia a cada clase <span class="arithmatex">\(k\)</span>:</p>
<div class="arithmatex">\[
\hat{p}_k = \frac{e^{F_M^{(k)}(\mathbf{x})}}{\sum_{j=1}^K e^{F_M^{(j)}(\mathbf{x})}}
\]</div>
<p>Se predecirá la clase con mayor probabilidad.</p>
<h3 id="algoritmo-completo">Algoritmo completo<a class="headerlink" href="#algoritmo-completo" title="Permanent link">¶</a></h3>
<p>Con todo lo anterior, podemos escribir de forma completa el algoritmo de Gradient Boosting como se muestra a continuación:</p>
<div class="arithmatex">\[
\begin{align*}
&amp; \text{Entrada: } \text{Conjunto de entrenamiento } \mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^N, \text{ Tasa de aprendizaje } \eta \\
&amp;F_0(\mathbf{x}) \leftarrow \arg \min_\gamma \sum_{i=1}^N L(y_i, \gamma) \quad \text{(Inicializamos el ensemble)} \\
&amp; \text{Para cada } m \in \{1, \ldots, M\}: \\
&amp; \quad r_{im} \leftarrow - \left[ \frac{\partial L(y_i, F(\mathbf{x}_i))}{ \partial F(\mathbf{x}_i)} \right]_{F=F_{m-1}}, \quad \forall i \text{ (Obtenemos los pseudo-residuos)}
\\
&amp; \quad 
h_m \leftarrow \arg \min_h \sum_{i=1}^N (r_{im} - h(\mathbf{x}_i))^2 \quad \text{ (Entrenamos un nuevo modelo que aprenda los residuos)} \\
&amp; \quad 
\gamma_{jm} \leftarrow \arg \min_\gamma \sum_{\mathbf{x}_i \in R_{jm}} L(y_i, F_{m-1}(\mathbf{x}_i)+ \gamma) \quad \forall j \text{ (Optimizamos el valor de cada hoja del nuevo modelo)} \\
&amp; \quad 
F_m(\mathbf{x}) \leftarrow F_{m-1}(\mathbf{x}) + \eta \sum_{j=1}^J \gamma_{jm} 1[\mathbf{x} \in R_{jm}] \quad  \text{ (Actualizamos el modelo)} \\
&amp; \text{Devuelve: } F_M(\mathbf{x})
\end{align*}
\]</div>
<h2 id="funciones-de-perdida">Funciones de pérdida<a class="headerlink" href="#funciones-de-perdida" title="Permanent link">¶</a></h2>
<p>Hemos visto el caso de dos funciones de pérdida típicas como son MSE en caso de regresión y <em>log-loss</em> para clasificación binaria. Vamos a ver ahora con mayor detalle las diferentes funciones de pérdida utilizadas comúnmente en Gradiente Boosting.</p>
<h3 id="regresion_2">Regresión<a class="headerlink" href="#regresion_2" title="Permanent link">¶</a></h3>
<p>Comenzamos con las funciones utilizadas habitualmente en regresión. </p>
<h4 id="mse-l2">MSE (L2)<a class="headerlink" href="#mse-l2" title="Permanent link">¶</a></h4>
<p>Es la más común, y óptima cuando los errores siguen una distribución normal. Tiene la desventaja de que es muy sensible a <em>outliers</em> porque penaliza de forma cuadrática</p>
<div class="arithmatex">\[
L(y_i, F(\mathbf{x}_i)) = \frac{1}{2}(y_i - F(\mathbf{x}_i))^2
\]</div>
<ul>
<li><strong>Inicialización</strong>: </li>
</ul>
<div class="arithmatex">\[
F_0 = \frac{1}{N} \sum_{i=1}^N y_i = \bar{y} 
\]</div>
<ul>
<li><strong>Pseudo-residuos</strong> (en este caso se trata del residuo ordinario):</li>
</ul>
<div class="arithmatex">\[
r_i = y_i-F_{m-1}(\mathbf{x}_i)
\]</div>
<ul>
<li><strong>Valor óptimo por hoja</strong> (media de los residuos):</li>
</ul>
<div class="arithmatex">\[
\gamma_{jm} = \frac{1}{|R_{jm}|} r_i
\]</div>
<h4 id="mae-l1">MAE (L1)<a class="headerlink" href="#mae-l1" title="Permanent link">¶</a></h4>
<p>Es más robusta a <em>outliers</em>, pero no es diferenciable en <span class="arithmatex">\(0\)</span>, lo que complica su optimización. Es menos utilizada en la práctica.</p>
<div class="arithmatex">\[
L(y_i, F(\mathbf{x}_i)) = |y_i - F(\mathbf{x}_i)|
\]</div>
<ul>
<li><strong>Inicialización</strong>: </li>
</ul>
<div class="arithmatex">\[
F_0 = \text{mediana}(\mathbf{y}) 
\]</div>
<ul>
<li>
<p><strong>Pseudo-residuos</strong> (en este caso tienen signo pero no magnitud, siempre serán <span class="arithmatex">\(1\)</span> o <span class="arithmatex">\(-1\)</span>):
$$
r_i = \text{signo} (y_i-F_{m-1}(\mathbf{x}_i))
$$</p>
</li>
<li>
<p><strong>Valor óptimo por hoja</strong>:</p>
</li>
</ul>
<div class="arithmatex">\[ \gamma_{jm} = \text{mediana}(y_i - F_{m-1}(\mathbf{x}_i): \mathbf{x}_i \in R_{jm}) \]</div>
<p>En este caso el valor por hoja se obtiene con la mediana de residuos, y no con la media. Es más robusto que MSE frente a <em>outliers</em> pero más lento de entrenar.</p>
<h4 id="huber">Huber<a class="headerlink" href="#huber" title="Permanent link">¶</a></h4>
<p>Combina L2 para errores pequeños y L1 para errores grandes.  Esto se controla mediante un umbral <span class="arithmatex">\(\delta\)</span>. </p>
<div class="arithmatex">\[
L(y_i, F(\mathbf{x}_i)) = \begin{cases}
\frac{1}{2}(y_i-F(\mathbf{x}_i))^2  &amp; \quad \text{si } |y_i - F(\mathbf{x}_i)| \leq \delta \\
\delta (|y_i-F(\mathbf{x}_i)| - \frac{\delta}{2})  &amp; \quad \text{si } |y_i - F(\mathbf{x}_i)| &gt; \delta
\end{cases}
\]</div>
<ul>
<li><strong>Inicialización</strong>: </li>
</ul>
<div class="arithmatex">\[
F_0 = \text{mediana}(\mathbf{y}) 
\]</div>
<ul>
<li><strong>Pseudo-residuos</strong> (en este caso se trata del residuo ordinario):</li>
</ul>
<div class="arithmatex">\[
r_i = \begin{cases}
y_i-F_{m-1}(\mathbf{x}_i)  &amp; \quad \text{si } |y_i - F_{m-1}(\mathbf{x}_i)| \leq \delta \\
\delta \cdot \text{signo} (y_i-F_{m-1}(\mathbf{x}_i))  &amp; \quad \text{si } |y_i - F_{m-1}(\mathbf{x}_i)| &gt; \delta
\end{cases}
\]</div>
<ul>
<li><strong>Valor óptimo por hoja</strong>: Se utiliza un proceso iterativo, ya que no tiene una forma cerrada simple.</li>
</ul>
<p>El hiperparámetro <span class="arithmatex">\(\delta\)</span> marca la frontera entre el comportamiento como L2 y L1. En la implementación de sklearn se actualiza adaptativamente en cada iteración a partir de los residuos.</p>
<h4 id="quantile-loss">Quantile Loss<a class="headerlink" href="#quantile-loss" title="Permanent link">¶</a></h4>
<p>Permite estimar percentiles/cuantiles en lugar de la media. </p>
<div class="arithmatex">\[
L_{\tau}(y_i, F(\mathbf{x}_i)) = \begin{cases}
\tau&nbsp;(y_i - F(\mathbf{x}_i)) &amp; \text{si } y_i \geq F(\mathbf{x}_i) \\
(1-\tau)&nbsp;(F(\mathbf{x}_i)-y_i) &amp; \text{si } y_i &lt; F(\mathbf{x}_i)
\end{cases} 
\]</div>
<p>Donde <span class="arithmatex">\(\tau \in (0,1)\)</span> es el cuantil objetivo. Por ejemplo, con <span class="arithmatex">\(\tau=0.5\)</span> tendríamos la mediana (equivale a MAE), mientras que con <span class="arithmatex">\(\tau=0.9\)</span> estimaríamos el percentil <span class="arithmatex">\(90\)</span> (es decir, penaliza más la infravaloraciones que laqs sobrevaloraciones, por lo que forzará al modelo a predecir alto). </p>
<ul>
<li><strong>Inicialización</strong> (de forma coherente con el caso del MAE que correspondería a <span class="arithmatex">\(\tau=0.5\)</span>, se inicializa como el cuantil de la variable objetivo): </li>
</ul>
<div class="arithmatex">\[
F_0 = Q_\tau (\mathbf{y})
\]</div>
<ul>
<li><strong>Pseudo-residuos</strong> (al igual que en el MAE, son valores discretos y no magnitudes continuas):</li>
</ul>
<div class="arithmatex">\[
r_i = \begin{cases}
\tau&nbsp; &amp; \text{si } y_i \geq F_{m-1}(\mathbf{x}_i) \\
-(1-\tau)&nbsp; &amp; \text{si } y_i &lt; F_{m-1}(\mathbf{x}_i)
\end{cases} 
\]</div>
<ul>
<li><strong>Valor óptimo por hoja</strong>: </li>
</ul>
<div class="arithmatex">\[
\gamma_{jm} = Q_{\tau} \{y_i - F_{m-1}(\mathbf{x}_i): \mathbf{x}_i \in R_{jm}\}
\]</div>
<p>El uso principal de esta función de pérdida es entrenar múltiples modelos con diferentes <span class="arithmatex">\(\tau\)</span>, por ejemplo <span class="arithmatex">\(0.1\)</span>, <span class="arithmatex">\(0.5\)</span> y <span class="arithmatex">\(0.9\)</span>, para así construir intervalos de predicción.</p>
<h3 id="clasificacion-binaria_2">Clasificación Binaria<a class="headerlink" href="#clasificacion-binaria_2" title="Permanent link">¶</a></h3>
<h4 id="logistic-loss-deviance-binomial">Logistic Loss (Deviance binomial)<a class="headerlink" href="#logistic-loss-deviance-binomial" title="Permanent link">¶</a></h4>
<p>Se trata de la pérdida estándar para clasificación binaria que hemos estado utilizando:</p>
<div class="arithmatex">\[
L(y_i, F(\mathbf{x}_i)) = - [y_i \ln p_i + (1-y_i) \ln(1-p_i)], \quad p_i = \sigma(F(\mathbf{x}_i))
\]</div>
<ul>
<li><strong>Inicialización</strong>: </li>
</ul>
<div class="arithmatex">\[
F_0 = \ln \left( \frac{\bar{p}}{1-\bar{p}} \right), \quad \bar{p} = \bar{y}
\]</div>
<ul>
<li><strong>Pseudo-residuos</strong> (en este caso se trata del residuo ordinario):</li>
</ul>
<div class="arithmatex">\[
r_i = y_i - p_i, \quad p_i = \sigma(F_{m-1}(\mathbf{x}_i))
\]</div>
<ul>
<li><strong>Valor óptimo por hoja</strong>: </li>
</ul>
<div class="arithmatex">\[
\gamma_{jm} = \frac{\sum_{\mathbf{x}_i \in R_{jm}}(y_i-p_i)}{\sum_{\mathbf{x}_i \in R_{jm}} p_i(1-p_i)}
\]</div>
<h4 id="exponential-loss">Exponential Loss<a class="headerlink" href="#exponential-loss" title="Permanent link">¶</a></h4>
<p>Equivale a AdaBoost cuando se utiliza con árboles. Como ya vimos, es menos robusta que <em>log-loss</em> porque penaliza los <em>outliers</em> de forma exponencial.</p>
<div class="arithmatex">\[
L(y_i, F(\mathbf{x}_i)) = e^{-y_i F(\mathbf{x}_i)}, \quad y_i \in \{-1, +1\}
\]</div>
<ul>
<li><strong>Inicialización</strong>: </li>
</ul>
<div class="arithmatex">\[
F_0 = \frac{1}{2} \ln \frac{\sum y_i^+}{\sum y_i^-}, \quad y_i \in \{-1, +1\}
\]</div>
<ul>
<li><strong>Pseudo-residuos</strong> (son proporcionales a los pesos de AdaBoost):</li>
</ul>
<div class="arithmatex">\[
r_i = y_i e^{-y_i F_{m-1}(\mathbf{x}_i)}
\]</div>
<ul>
<li><strong>Valor óptimo por hoja</strong>: </li>
</ul>
<div class="arithmatex">\[
\gamma_{jm} = \frac{\sum_{\mathbf{x}_i \in R_{jm}}r_i}{\sum_{\mathbf{x}_i \in R_{jm}} e^{-y_i F_{m-1}(\mathbf{x}_i)}}
\]</div>
<h3 id="clasificacion-multiclase_2">Clasificación Multiclase<a class="headerlink" href="#clasificacion-multiclase_2" title="Permanent link">¶</a></h3>
<h4 id="softmax-deviance-multinomial">Softmax (Deviance multinomial)<a class="headerlink" href="#softmax-deviance-multinomial" title="Permanent link">¶</a></h4>
<p>Para la clasificación multiclase se generaliza con <em>softmax</em> entrenando <span class="arithmatex">\(K\)</span> clasificadores binarios en paralelo (uno para cada clase). </p>
<div class="arithmatex">\[
L(y_i, F(\mathbf{x}_i)) = -\sum_{k=1}^K y_i^{(k)} \ln p_i^{(k)} , \quad p_i^{(k)} = \frac{e^{F^{(k)} (\mathbf{x}_i)}}{\sum_j e^{F^{(j)} (\mathbf{x}_i)}}
\]</div>
<p>Donde <span class="arithmatex">\(y_i^{(k)} \in \{0, 1\}\)</span> es la codificación <em>one-hot</em> de la clase real, y <span class="arithmatex">\(p_i^{(k)}\)</span> es la probabilidad predicha para la clase <span class="arithmatex">\(k\)</span> obtenida mediante <em>softmax</em>. </p>
<ul>
<li><strong>Inicialización</strong> (para cada clase <span class="arithmatex">\(k\)</span>): </li>
</ul>
<div class="arithmatex">\[
F_0^{(k)} = \ln \hat{p}_k, \quad \hat{p}_k = \frac{N_k}{N}
\]</div>
<ul>
<li><strong>Pseudo-residuos</strong> (para cada clase <span class="arithmatex">\(k\)</span>):</li>
</ul>
<div class="arithmatex">\[
r_i^{(k)} = y_i^{(k)} - p_i^{(k)}, \quad p_i^{(k)} = \frac{e^{F_{m-1}^{(k)} (\mathbf{x}_i)}}{\sum_j e^{F_{m-1}^{(j)} (\mathbf{x}_i)}}
\]</div>
<ul>
<li><strong>Valor óptimo por hoja</strong>: </li>
</ul>
<div class="arithmatex">\[
\gamma_{jm}^{(k)} = \frac{K-1}{K} \cdot \frac{\sum_{\mathbf{x}_i \in R_{jm}}r_i^{(k)}}{\sum_{\mathbf{x}_i \in R_{jm}} p_i^{(k)} (1-p_i^{(k)}) }
\]</div>
<h2 id="stochastic-gradient-boosting">Stochastic Gradient Boosting<a class="headerlink" href="#stochastic-gradient-boosting" title="Permanent link">¶</a></h2>
<p>El algoritmo original de Gradient Boosting utiliza en cada iteración todos los ejemplos de entrenamiento para calcular los pseudo-residuos y ajustar el nuevo modelo. Aunque esto garantiza una estimación precisa del gradiente, tiene dos inconvenientes:</p>
<ul>
<li>Tiene un alto coste computacional si el <em>dataset</em> es grande.</li>
<li>Al ver siempre los mismos datos, tiende a haber una alta correlación entre árboles sucesivos, lo cual puede llevar al <em>overfitting</em>.</li>
</ul>
<p>Para abordar estos problemas, se propuso una variante estocástica (Friedman, 2002)<sup id="fnref:friedman2002stochastic"><a class="footnote-ref" href="#fn:friedman2002stochastic">5</a></sup>: <strong>Stochastic Gradient Descent (SGB)</strong>. </p>
<p>La modificación es conceptualmente sencilla. En cada iteración <span class="arithmatex">\(m\)</span>, en lugar de usar el conjunto de entrenamiento completo se selecciona una <strong>muestra aleatoria sin reemplazo</strong> <span class="arithmatex">\(\mathcal{S_m} \subset \{1, \ldots, N\}\)</span> cuyo tamaño <span class="arithmatex">\(N'\)</span> se calcula como:</p>
<div class="arithmatex">\[
N' = \lfloor fN \rfloor
\]</div>
<p>Donde <span class="arithmatex">\(f \in (0,1]\)</span> es la <strong>fracción de subsampling</strong> y <span class="arithmatex">\(N\)</span> es el tamaño del <em>dataset</em> completo. </p>
<p>La única modificación en el algoritmo es que los residuos se calcularán solo sobre el <em>subsample</em>, y el nuevo modelo se ajustará únicamente mediante a partir de los ejemplos de este <em>subsample</em>, pero tras ello actualiza <span class="arithmatex">\(F\)</span> con todo el <em>dataset</em>:</p>
<div class="arithmatex">\[
\begin{align*}
&amp; \text{Entrada: } \text{Conjunto de entrenamiento } \mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^N, \text{ Tasa de aprendizaje } \eta, \text{ Fracción de subsampling } f \\
&amp;F_0(\mathbf{x}) \leftarrow \arg \min_\gamma \sum_{i=1}^N L(y_i, \gamma) \quad \text{(Inicializamos el ensemble)} \\
&amp; \text{Para cada } m \in \{1, \ldots, M\}: \\
&amp; \quad \mathcal{S_m} \leftarrow \text{Muestreamos sin reemplazo subconjunto } \mathcal{S_m} \subset \{1, \ldots, N\} \text{ de tamaño } N' = \lfloor fN \rfloor \\
&amp; \quad r_{im} \leftarrow - \left[ \frac{\partial L(y_i, F(\mathbf{x}_i))}{ \partial F(\mathbf{x}_i)} \right]_{F=F_{m-1}}, \quad \forall i \in \mathcal{S}_m \text{ (Obtenemos los pseudo-residuos del subsample)}
\\
&amp; \quad 
h_m \leftarrow \arg \min_h \sum_{i \in \mathcal{S}_m} (r_{im} - h(\mathbf{x}_i))^2 \quad \text{ (Entrenamos un nuevo modelo que aprenda los residuos)} \\
&amp; \quad 
\gamma_{jm} \leftarrow \arg \min_\gamma \sum_{\mathbf{x}_i \in R_{jm}: i \in \mathcal{S}_m} L(y_i, F_{m-1}(\mathbf{x}_i)+ \gamma) \quad \forall j \text{ (Optimizamos el valor de cada hoja del nuevo modelo)} \\
&amp; \quad 
F_m(\mathbf{x}) \leftarrow F_{m-1}(\mathbf{x}) + \eta \sum_{j=1}^J \gamma_{jm} 1[\mathbf{x} \in R_{jm}] \quad  \text{ (Actualizamos el modelo)} \\
&amp; \text{Devuelve: } F_M(\mathbf{x})
\end{align*}
\]</div>
<p>El subsampling </p>
<p><strong>Ventajas:</strong>
- ✅ Reduce overfitting (más regularización)
- ✅ Más rápido (menos datos por árbol)
- ✅ Introduce diversidad entre árboles</p>
<p><strong>Subsample típico:</strong> 0.5-0.8</p>
<h2 id="implementacion">Implementación<a class="headerlink" href="#implementacion" title="Permanent link">¶</a></h2>
<p>En sklearn contamos con las clases <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">GradientBoostingClassifier</a> y <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html">GradientBoostingRegressor</a> para problemas de clasificación y regresión respectivamente.</p>
<p>A continuación mostramos un ejemplo para el casi de clasificación:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="c1"># Gradient Boosting para clasificación</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="n">gb_clf</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>         <span class="c1"># Número de árboles</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>        <span class="c1"># Shrinkage</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>              <span class="c1"># Profundidad de árboles (por defecto)</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>     <span class="c1"># Mínimo para split</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>      <span class="c1"># Mínimo en hojas</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    <span class="n">subsample</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>            <span class="c1"># Stochastic GB (fracción de muestras)</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    <span class="n">max_features</span><span class="o">=</span><span class="s1">'sqrt'</span><span class="p">,</span>      <span class="c1"># Features por split</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    <span class="n">loss</span><span class="o">=</span><span class="s1">'log_loss'</span>          <span class="c1"># Función de pérdida (por defecto)</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span class="p">)</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a><span class="n">gb_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<p>Una de las cuestiones a destacar es que en Gradient Boosting utilizamos normalmente <strong>árboles poco profundos</strong> (típicamente entre 3 y 8 niveles), lo cual contrasta con Random Forest que utiliza árboles con profundidad máxima. Vemos que en la implementación por defecto tenemos una profundidad máxima (<code>max_depth</code>) de 3 niveles.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a># Gradient Boosting para regresión
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>gb_reg = GradientBoostingRegressor(
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>    n_estimators=100,
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>    learning_rate=0.1,
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>    max_depth=3,
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>    loss='squared_error',     # Para regresión (default)
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>    random_state=42
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>)
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>gb_reg.fit(X_train, y_train)
</code></pre></div>
<p>`</p>
<h2 id="learning-rate-shrinkage">Learning Rate (Shrinkage)<a class="headerlink" href="#learning-rate-shrinkage" title="Permanent link">¶</a></h2>
<p>El learning rate <span class="arithmatex">\(\eta\)</span> controla la contribución de cada árbol:</p>
<div class="arithmatex">\[F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)\]</div>
<p><strong>Efecto:</strong>
- <strong>ν grande (ej: 1.0):</strong> Convergencia rápida, riesgo de overfitting
- <strong>ν pequeño (ej: 0.01-0.1):</strong> Convergencia lenta, mejor generalización</p>
<p><strong>Trade-off learning rate vs número de árboles:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>ν = 1.0 → 50 árboles suficientes, pero puede overfit
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>ν = 0.1 → 500 árboles necesarios, mejor generalización
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>ν = 0.01 → 5000 árboles necesarios, excelente generalización
</code></pre></div><p></p>
<p><strong>Recomendación práctica:</strong>
- Usar ν pequeño (0.01-0.1)
- Aumentar n_estimators proporcionalmente
- Usar early stopping para encontrar el número óptimo</p>
<h3 id="arboles-de-decision-en-gradient-boosting">Árboles de Decisión en Gradient Boosting<a class="headerlink" href="#arboles-de-decision-en-gradient-boosting" title="Permanent link">¶</a></h3>
<p><strong>Características típicas:</strong>
- <strong>Poco profundos:</strong> max_depth = 3-8 (vs Random Forest: sin límite)
- <strong>Capturam interacciones:</strong> Profundidad k → interacciones de orden k
- <strong>Rápidos de entrenar:</strong> Árboles pequeños
- <strong>Complementarios:</strong> Cada árbol captura un patrón residual diferente</p>
<p><strong>Ejemplo:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a><span class="c1"># Árbol típico en Gradient Boosting</span>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a><span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>           <span class="c1"># Poco profundo</span>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>  <span class="c1"># Evitar splits en pocas muestras</span>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">10</span>    <span class="c1"># Hojas razonablemente grandes</span>
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a><span class="p">)</span>
</code></pre></div><p></p>
<h3 id="hiperparametros-importantes">Hiperparámetros Importantes<a class="headerlink" href="#hiperparametros-importantes" title="Permanent link">¶</a></h3>
<h4 id="control-de-complejidad">Control de complejidad:<a class="headerlink" href="#control-de-complejidad" title="Permanent link">¶</a></h4>
<p><strong>1. n_estimators:</strong> Número de árboles
- Más árboles → más capacidad
- Usar con learning_rate bajo y early stopping
- Típico: 100-1000</p>
<p><strong>2. learning_rate:</strong> Tasa de aprendizaje
- Controla contribución de cada árbol
- Típico: 0.01-0.1
- Trade-off con n_estimators</p>
<p><strong>3. max_depth:</strong> Profundidad máxima de árboles
- Controla interacciones capturadas
- Típico: 3-8
- Profundidad k → interacciones de orden k</p>
<p><strong>4. min_samples_split / min_samples_leaf:</strong>
- Previene splits en pocas muestras
- Regularización importante
- Típico: 20/10</p>
<p><strong>5. subsample:</strong> Fracción de muestras por árbol
- Stochastic GB
- Típico: 0.8
- Reduce overfitting</p>
<p><strong>6. max_features:</strong> Features consideradas por split
- Similar a Random Forest
- Típico: 'sqrt' o None
- Añade diversidad</p>
<h3 id="early-stopping">Early Stopping<a class="headerlink" href="#early-stopping" title="Permanent link">¶</a></h3>
<p>Detener el entrenamiento cuando el rendimiento en validación deja de mejorar:
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="n">gb</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>        <span class="c1"># Número grande</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>       <span class="c1"># Learning rate bajo</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a>    <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>  <span class="c1"># Fracción para validación</span>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a>    <span class="n">n_iter_no_change</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>      <span class="c1"># Parar si no mejora en 50 iteraciones</span>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a>    <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>                 <span class="c1"># Tolerancia de mejora</span>
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a><span class="p">)</span>
<a id="__codelineno-4-9" name="__codelineno-4-9" href="#__codelineno-4-9"></a>
<a id="__codelineno-4-10" name="__codelineno-4-10" href="#__codelineno-4-10"></a><span class="n">gb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-4-11" name="__codelineno-4-11" href="#__codelineno-4-11"></a>
<a id="__codelineno-4-12" name="__codelineno-4-12" href="#__codelineno-4-12"></a><span class="c1"># Número óptimo de árboles encontrado</span>
<a id="__codelineno-4-13" name="__codelineno-4-13" href="#__codelineno-4-13"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Número óptimo de árboles: </span><span class="si">{</span><span class="n">gb</span><span class="o">.</span><span class="n">n_estimators_</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div><p></p>
<h3 id="ventajas-de-gradient-boosting">Ventajas de Gradient Boosting<a class="headerlink" href="#ventajas-de-gradient-boosting" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Muy efectivo:</strong> Estado del arte en muchos problemas</li>
<li><strong>Flexible:</strong> Cualquier función de pérdida diferenciable</li>
<li><strong>Reduce sesgo y varianza:</strong> Mejor que bagging o boosting básico</li>
<li><strong>Maneja features mixtas:</strong> Numéricas y categóricas</li>
<li><strong>Robusto:</strong> Con funciones de pérdida apropiadas</li>
<li><strong>Interpretable:</strong> Feature importance disponible</li>
</ul>
<h3 id="limitaciones-de-gradient-boosting">Limitaciones de Gradient Boosting<a class="headerlink" href="#limitaciones-de-gradient-boosting" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Propenso a overfitting:</strong> Requiere tuning cuidadoso</li>
<li><strong>No paralelizable:</strong> Entrenamiento secuencial</li>
<li><strong>Lento:</strong> Comparado con Random Forest</li>
<li><strong>Sensible a hiperparámetros:</strong> Más que Random Forest</li>
<li><strong>Sensible a outliers:</strong> Con squared loss</li>
</ul>
<h3 id="prevencion-de-overfitting">Prevención de Overfitting<a class="headerlink" href="#prevencion-de-overfitting" title="Permanent link">¶</a></h3>
<p>Estrategias múltiples:
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="n">gb</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>    <span class="c1"># 1. Reduce learning rate, aumenta árboles</span>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a>
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a>    <span class="c1"># 2. Limita complejidad de árboles</span>
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a>    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<a id="__codelineno-5-9" name="__codelineno-5-9" href="#__codelineno-5-9"></a>    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<a id="__codelineno-5-10" name="__codelineno-5-10" href="#__codelineno-5-10"></a>
<a id="__codelineno-5-11" name="__codelineno-5-11" href="#__codelineno-5-11"></a>    <span class="c1"># 3. Stochastic GB</span>
<a id="__codelineno-5-12" name="__codelineno-5-12" href="#__codelineno-5-12"></a>    <span class="n">subsample</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
<a id="__codelineno-5-13" name="__codelineno-5-13" href="#__codelineno-5-13"></a>    <span class="n">max_features</span><span class="o">=</span><span class="s1">'sqrt'</span><span class="p">,</span>
<a id="__codelineno-5-14" name="__codelineno-5-14" href="#__codelineno-5-14"></a>
<a id="__codelineno-5-15" name="__codelineno-5-15" href="#__codelineno-5-15"></a>    <span class="c1"># 4. Early stopping</span>
<a id="__codelineno-5-16" name="__codelineno-5-16" href="#__codelineno-5-16"></a>    <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
<a id="__codelineno-5-17" name="__codelineno-5-17" href="#__codelineno-5-17"></a>    <span class="n">n_iter_no_change</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
<a id="__codelineno-5-18" name="__codelineno-5-18" href="#__codelineno-5-18"></a>
<a id="__codelineno-5-19" name="__codelineno-5-19" href="#__codelineno-5-19"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-5-20" name="__codelineno-5-20" href="#__codelineno-5-20"></a><span class="p">)</span>
</code></pre></div><p></p>
<h3 id="comparacion-adaboost-vs-gradient-boosting">Comparación: AdaBoost vs Gradient Boosting<a class="headerlink" href="#comparacion-adaboost-vs-gradient-boosting" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Aspecto</th>
<th>AdaBoost</th>
<th>Gradient Boosting</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Actualización</strong></td>
<td>Pesos de ejemplos</td>
<td>Ajuste a residuos/gradientes</td>
</tr>
<tr>
<td><strong>Función de pérdida</strong></td>
<td>Exponential (fija)</td>
<td>Cualquiera (flexible)</td>
</tr>
<tr>
<td><strong>Clasificador débil</strong></td>
<td>Cualquiera</td>
<td>Típicamente árboles</td>
</tr>
<tr>
<td><strong>Robustez a outliers</strong></td>
<td>Baja</td>
<td>Media-Alta (depende de loss)</td>
</tr>
<tr>
<td><strong>Flexibilidad</strong></td>
<td>Menor</td>
<td>Mayor</td>
</tr>
<tr>
<td><strong>Teoría</strong></td>
<td>Más simple</td>
<td>Más general</td>
</tr>
<tr>
<td><strong>Rendimiento</strong></td>
<td>Bueno</td>
<td>Excelente</td>
</tr>
</tbody>
</table>
<h3 id="ejemplo-completo">Ejemplo Completo<a class="headerlink" href="#ejemplo-completo" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_val_score</span>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a>
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a><span class="c1"># Datos</span>
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a>
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a><span class="c1"># Modelo</span>
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#__codelineno-6-11"></a><span class="n">gb</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-6-12" name="__codelineno-6-12" href="#__codelineno-6-12"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-6-13" name="__codelineno-6-13" href="#__codelineno-6-13"></a>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
<a id="__codelineno-6-14" name="__codelineno-6-14" href="#__codelineno-6-14"></a>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<a id="__codelineno-6-15" name="__codelineno-6-15" href="#__codelineno-6-15"></a>    <span class="n">subsample</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
<a id="__codelineno-6-16" name="__codelineno-6-16" href="#__codelineno-6-16"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
<a id="__codelineno-6-17" name="__codelineno-6-17" href="#__codelineno-6-17"></a>    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span>  <span class="c1"># Mostrar progreso</span>
<a id="__codelineno-6-18" name="__codelineno-6-18" href="#__codelineno-6-18"></a><span class="p">)</span>
<a id="__codelineno-6-19" name="__codelineno-6-19" href="#__codelineno-6-19"></a>
<a id="__codelineno-6-20" name="__codelineno-6-20" href="#__codelineno-6-20"></a><span class="c1"># Entrenar</span>
<a id="__codelineno-6-21" name="__codelineno-6-21" href="#__codelineno-6-21"></a><span class="n">gb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-6-22" name="__codelineno-6-22" href="#__codelineno-6-22"></a>
<a id="__codelineno-6-23" name="__codelineno-6-23" href="#__codelineno-6-23"></a><span class="c1"># Evaluar</span>
<a id="__codelineno-6-24" name="__codelineno-6-24" href="#__codelineno-6-24"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Train score: </span><span class="si">{</span><span class="n">gb</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="w"> </span><span class="n">y_train</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-6-25" name="__codelineno-6-25" href="#__codelineno-6-25"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Test score: </span><span class="si">{</span><span class="n">gb</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-6-26" name="__codelineno-6-26" href="#__codelineno-6-26"></a>
<a id="__codelineno-6-27" name="__codelineno-6-27" href="#__codelineno-6-27"></a><span class="c1"># Feature importance</span>
<a id="__codelineno-6-28" name="__codelineno-6-28" href="#__codelineno-6-28"></a><span class="n">importances</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">feature_importances_</span>
<a id="__codelineno-6-29" name="__codelineno-6-29" href="#__codelineno-6-29"></a><span class="n">top_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="mi">5</span><span class="p">]</span>
<a id="__codelineno-6-30" name="__codelineno-6-30" href="#__codelineno-6-30"></a><span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Top 5 features:"</span><span class="p">)</span>
<a id="__codelineno-6-31" name="__codelineno-6-31" href="#__codelineno-6-31"></a><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">top_features</span><span class="p">:</span>
<a id="__codelineno-6-32" name="__codelineno-6-32" href="#__codelineno-6-32"></a>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Feature </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">importances</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-6-33" name="__codelineno-6-33" href="#__codelineno-6-33"></a>
<a id="__codelineno-6-34" name="__codelineno-6-34" href="#__codelineno-6-34"></a><span class="c1"># Learning curves</span>
<a id="__codelineno-6-35" name="__codelineno-6-35" href="#__codelineno-6-35"></a><span class="n">train_scores</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-6-36" name="__codelineno-6-36" href="#__codelineno-6-36"></a><span class="n">test_scores</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-6-37" name="__codelineno-6-37" href="#__codelineno-6-37"></a><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">train_pred</span><span class="p">,</span> <span class="n">test_pred</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span>
<a id="__codelineno-6-38" name="__codelineno-6-38" href="#__codelineno-6-38"></a>    <span class="n">gb</span><span class="o">.</span><span class="n">staged_predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span>
<a id="__codelineno-6-39" name="__codelineno-6-39" href="#__codelineno-6-39"></a>    <span class="n">gb</span><span class="o">.</span><span class="n">staged_predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<a id="__codelineno-6-40" name="__codelineno-6-40" href="#__codelineno-6-40"></a><span class="p">)):</span>
<a id="__codelineno-6-41" name="__codelineno-6-41" href="#__codelineno-6-41"></a>    <span class="n">train_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_pred</span> <span class="o">==</span> <span class="n">y_train</span><span class="p">))</span>
<a id="__codelineno-6-42" name="__codelineno-6-42" href="#__codelineno-6-42"></a>    <span class="n">test_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_pred</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">))</span>
<a id="__codelineno-6-43" name="__codelineno-6-43" href="#__codelineno-6-43"></a>
<a id="__codelineno-6-44" name="__codelineno-6-44" href="#__codelineno-6-44"></a><span class="c1"># El error converge gradualmente</span>
<a id="__codelineno-6-45" name="__codelineno-6-45" href="#__codelineno-6-45"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Scores cada 25 árboles:"</span><span class="p">)</span>
<a id="__codelineno-6-46" name="__codelineno-6-46" href="#__codelineno-6-46"></a><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span> <span class="mi">74</span><span class="p">,</span> <span class="mi">99</span><span class="p">]:</span>
<a id="__codelineno-6-47" name="__codelineno-6-47" href="#__codelineno-6-47"></a>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Árboles </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: Train=</span><span class="si">{</span><span class="n">train_scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Test=</span><span class="si">{</span><span class="n">test_scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<hr>
<h2 id="43-histgradientboosting-scikit-learn">4.3. HistGradientBoosting (scikit-learn)<a class="headerlink" href="#43-histgradientboosting-scikit-learn" title="Permanent link">¶</a></h2>
<p>HistGradientBoosting es la implementación moderna de Gradient Boosting en scikit-learn, inspirada en LightGBM y disponible desde la versión 0.21.</p>
<p><strong>Innovación principal:</strong> Usar <strong>histogramas</strong> para discretizar features continuas, acelerando enormemente la búsqueda de splits.</p>
<h3 id="diferencias-con-gradientboosting-tradicional">Diferencias con GradientBoosting tradicional<a class="headerlink" href="#diferencias-con-gradientboosting-tradicional" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Aspecto</th>
<th>GradientBoosting</th>
<th>HistGradientBoosting</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Búsqueda de splits</strong></td>
<td>Exacta (todos los valores)</td>
<td>Histogramas (valores discretizados)</td>
</tr>
<tr>
<td><strong>Velocidad</strong></td>
<td>Lenta</td>
<td><strong>10-100x más rápida</strong></td>
</tr>
<tr>
<td><strong>Escalabilidad</strong></td>
<td>Dataset mediano</td>
<td><strong>Dataset grande</strong></td>
</tr>
<tr>
<td><strong>Missing values</strong></td>
<td>No soportado</td>
<td><strong>Soporte nativo</strong></td>
</tr>
<tr>
<td><strong>Features categóricas</strong></td>
<td>Manual encoding</td>
<td><strong>Soporte experimental</strong></td>
</tr>
<tr>
<td><strong>API</strong></td>
<td>n_estimators</td>
<td><strong>max_iter</strong></td>
</tr>
</tbody>
</table>
<h3 id="algoritmo-basado-en-histogramas">Algoritmo basado en Histogramas<a class="headerlink" href="#algoritmo-basado-en-histogramas" title="Permanent link">¶</a></h3>
<p><strong>Idea:</strong> En lugar de considerar todos los posibles valores para splits, discretizar features en bins (histogramas).
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a>Feature continua: [0.1, 0.3, 0.5, 0.7, 0.9, 1.2, 1.5, ...]
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a>Tradicional: Considerar todos los valores como candidatos a split
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a>→ O(n * p) comparaciones por nivel
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a>
<a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a>Histogramas: Discretizar en 255 bins
<a id="__codelineno-7-7" name="__codelineno-7-7" href="#__codelineno-7-7"></a>→ O(255 * p) comparaciones por nivel (independiente de n!)
</code></pre></div><p></p>
<p><strong>Construcción de histogramas:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a>1. Para cada feature:
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>   - Calcular quantiles para crear bins
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a>   - Mapear valores continuos → índices de bins (0-254)
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a>
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a>2. Para encontrar mejor split:
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a>   - Solo considerar los 255 bins como candidatos
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#__codelineno-8-7"></a>   - Acumulación eficiente de gradientes por bin
</code></pre></div><p></p>
<p><strong>Ventaja masiva:</strong> Complejidad independiente del número de ejemplos.</p>
<h3 id="implementacion-en-scikit-learn">Implementación en scikit-learn<a class="headerlink" href="#implementacion-en-scikit-learn" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">,</span> <span class="n">HistGradientBoostingRegressor</span>
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a>
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a><span class="c1"># Clasificación</span>
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#__codelineno-9-4"></a><span class="n">hgb</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-9-5" name="__codelineno-9-5" href="#__codelineno-9-5"></a>    <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>              <span class="c1"># Número de árboles (equivalente a n_estimators)</span>
<a id="__codelineno-9-6" name="__codelineno-9-6" href="#__codelineno-9-6"></a>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>         <span class="c1"># Tasa de aprendizaje</span>
<a id="__codelineno-9-7" name="__codelineno-9-7" href="#__codelineno-9-7"></a>    <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>            <span class="c1"># Sin límite (usa max_leaf_nodes)</span>
<a id="__codelineno-9-8" name="__codelineno-9-8" href="#__codelineno-9-8"></a>    <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">31</span><span class="p">,</span>         <span class="c1"># Máximo número de hojas (default)</span>
<a id="__codelineno-9-9" name="__codelineno-9-9" href="#__codelineno-9-9"></a>    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>       <span class="c1"># Mínimo en hojas</span>
<a id="__codelineno-9-10" name="__codelineno-9-10" href="#__codelineno-9-10"></a>    <span class="n">l2_regularization</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>     <span class="c1"># Regularización L2</span>
<a id="__codelineno-9-11" name="__codelineno-9-11" href="#__codelineno-9-11"></a>    <span class="n">max_bins</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>              <span class="c1"># Número de bins (default)</span>
<a id="__codelineno-9-12" name="__codelineno-9-12" href="#__codelineno-9-12"></a>    <span class="n">categorical_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># Índices de features categóricas</span>
<a id="__codelineno-9-13" name="__codelineno-9-13" href="#__codelineno-9-13"></a>    <span class="n">early_stopping</span><span class="o">=</span><span class="s1">'auto'</span><span class="p">,</span>     <span class="c1"># Early stopping automático</span>
<a id="__codelineno-9-14" name="__codelineno-9-14" href="#__codelineno-9-14"></a>    <span class="n">scoring</span><span class="o">=</span><span class="s1">'loss'</span><span class="p">,</span>            <span class="c1"># Métrica para early stopping</span>
<a id="__codelineno-9-15" name="__codelineno-9-15" href="#__codelineno-9-15"></a>    <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>   <span class="c1"># Fracción para validación</span>
<a id="__codelineno-9-16" name="__codelineno-9-16" href="#__codelineno-9-16"></a>    <span class="n">n_iter_no_change</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>       <span class="c1"># Parar si no mejora</span>
<a id="__codelineno-9-17" name="__codelineno-9-17" href="#__codelineno-9-17"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-9-18" name="__codelineno-9-18" href="#__codelineno-9-18"></a><span class="p">)</span>
<a id="__codelineno-9-19" name="__codelineno-9-19" href="#__codelineno-9-19"></a>
<a id="__codelineno-9-20" name="__codelineno-9-20" href="#__codelineno-9-20"></a><span class="n">hgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<h3 id="diferencias-de-api-importantes">Diferencias de API importantes<a class="headerlink" href="#diferencias-de-api-importantes" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="c1"># GradientBoosting tradicional</span>
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a><span class="n">gb</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="c1"># ← Nombre del parámetro</span>
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span>
<a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a><span class="p">)</span>
<a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a>
<a id="__codelineno-10-8" name="__codelineno-10-8" href="#__codelineno-10-8"></a><span class="c1"># HistGradientBoosting</span>
<a id="__codelineno-10-9" name="__codelineno-10-9" href="#__codelineno-10-9"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingClassifier</span>
<a id="__codelineno-10-10" name="__codelineno-10-10" href="#__codelineno-10-10"></a><span class="n">hgb</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-10-11" name="__codelineno-10-11" href="#__codelineno-10-11"></a>    <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>      <span class="c1"># ← Diferente nombre!</span>
<a id="__codelineno-10-12" name="__codelineno-10-12" href="#__codelineno-10-12"></a>    <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">31</span>  <span class="c1"># ← Controla complejidad diferente</span>
<a id="__codelineno-10-13" name="__codelineno-10-13" href="#__codelineno-10-13"></a><span class="p">)</span>
</code></pre></div>
<h3 id="manejo-nativo-de-missing-values">Manejo Nativo de Missing Values<a class="headerlink" href="#manejo-nativo-de-missing-values" title="Permanent link">¶</a></h3>
<p>Una de las grandes ventajas:
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a>
<a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a><span class="c1"># Datos con valores faltantes</span>
<a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
<a id="__codelineno-11-5" name="__codelineno-11-5" href="#__codelineno-11-5"></a><span class="n">X_train</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
<a id="__codelineno-11-6" name="__codelineno-11-6" href="#__codelineno-11-6"></a>
<a id="__codelineno-11-7" name="__codelineno-11-7" href="#__codelineno-11-7"></a><span class="c1"># GradientBoosting: ERROR</span>
<a id="__codelineno-11-8" name="__codelineno-11-8" href="#__codelineno-11-8"></a><span class="c1"># gb = GradientBoostingClassifier()</span>
<a id="__codelineno-11-9" name="__codelineno-11-9" href="#__codelineno-11-9"></a><span class="c1"># gb.fit(X_train, y_train)  # ValueError: Input contains NaN</span>
<a id="__codelineno-11-10" name="__codelineno-11-10" href="#__codelineno-11-10"></a>
<a id="__codelineno-11-11" name="__codelineno-11-11" href="#__codelineno-11-11"></a><span class="c1"># HistGradientBoosting: Funciona!</span>
<a id="__codelineno-11-12" name="__codelineno-11-12" href="#__codelineno-11-12"></a><span class="n">hgb</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">()</span>
<a id="__codelineno-11-13" name="__codelineno-11-13" href="#__codelineno-11-13"></a><span class="n">hgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>  <span class="c1"># OK, aprende dirección óptima para NaN</span>
</code></pre></div><p></p>
<p><strong>Cómo maneja NaN:</strong>
- Trata NaN como un valor especial en el histograma
- Aprende la mejor dirección (izquierda o derecha) para NaN en cada split
- No requiere imputación manual</p>
<h3 id="soporte-experimental-para-features-categoricas">Soporte Experimental para Features Categóricas<a class="headerlink" href="#soporte-experimental-para-features-categoricas" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="c1"># Marcar features categóricas</span>
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a><span class="n">categorical_features</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>  <span class="c1"># Índices de columnas categóricas</span>
<a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a>
<a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a><span class="n">hgb</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-12-5" name="__codelineno-12-5" href="#__codelineno-12-5"></a>    <span class="n">categorical_features</span><span class="o">=</span><span class="n">categorical_features</span>
<a id="__codelineno-12-6" name="__codelineno-12-6" href="#__codelineno-12-6"></a><span class="p">)</span>
<a id="__codelineno-12-7" name="__codelineno-12-7" href="#__codelineno-12-7"></a>
<a id="__codelineno-12-8" name="__codelineno-12-8" href="#__codelineno-12-8"></a><span class="c1"># O pasar un boolean mask</span>
<a id="__codelineno-12-9" name="__codelineno-12-9" href="#__codelineno-12-9"></a><span class="n">categorical_mask</span> <span class="o">=</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
<a id="__codelineno-12-10" name="__codelineno-12-10" href="#__codelineno-12-10"></a><span class="n">hgb</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-12-11" name="__codelineno-12-11" href="#__codelineno-12-11"></a>    <span class="n">categorical_features</span><span class="o">=</span><span class="n">categorical_mask</span>
<a id="__codelineno-12-12" name="__codelineno-12-12" href="#__codelineno-12-12"></a><span class="p">)</span>
<a id="__codelineno-12-13" name="__codelineno-12-13" href="#__codelineno-12-13"></a>
<a id="__codelineno-12-14" name="__codelineno-12-14" href="#__codelineno-12-14"></a><span class="n">hgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<p><strong>Ventaja:</strong> No requiere one-hot encoding manual, maneja categorías de forma eficiente.</p>
<h3 id="early-stopping-automatico">Early Stopping Automático<a class="headerlink" href="#early-stopping-automatico" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="n">hgb</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a>    <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>            <span class="c1"># Número máximo de iteraciones</span>
<a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a>    <span class="n">early_stopping</span><span class="o">=</span><span class="s1">'auto'</span><span class="p">,</span>    <span class="c1"># Activar early stopping</span>
<a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a>    <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>  <span class="c1"># 10% para validación</span>
<a id="__codelineno-13-5" name="__codelineno-13-5" href="#__codelineno-13-5"></a>    <span class="n">n_iter_no_change</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>      <span class="c1"># Parar si no mejora en 10 iteraciones</span>
<a id="__codelineno-13-6" name="__codelineno-13-6" href="#__codelineno-13-6"></a>    <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span>                  <span class="c1"># Tolerancia de mejora</span>
<a id="__codelineno-13-7" name="__codelineno-13-7" href="#__codelineno-13-7"></a><span class="p">)</span>
<a id="__codelineno-13-8" name="__codelineno-13-8" href="#__codelineno-13-8"></a>
<a id="__codelineno-13-9" name="__codelineno-13-9" href="#__codelineno-13-9"></a><span class="n">hgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-13-10" name="__codelineno-13-10" href="#__codelineno-13-10"></a>
<a id="__codelineno-13-11" name="__codelineno-13-11" href="#__codelineno-13-11"></a><span class="c1"># Número real de iteraciones usadas</span>
<a id="__codelineno-13-12" name="__codelineno-13-12" href="#__codelineno-13-12"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Iteraciones usadas: </span><span class="si">{</span><span class="n">hgb</span><span class="o">.</span><span class="n">n_iter_</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<h3 id="comparacion-de-velocidad">Comparación de Velocidad<a class="headerlink" href="#comparacion-de-velocidad" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="kn">import</span> <span class="nn">time</span>
<a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a>
<a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a><span class="c1"># Dataset grande</span>
<a id="__codelineno-14-5" name="__codelineno-14-5" href="#__codelineno-14-5"></a><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-14-6" name="__codelineno-14-6" href="#__codelineno-14-6"></a><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<a id="__codelineno-14-7" name="__codelineno-14-7" href="#__codelineno-14-7"></a>
<a id="__codelineno-14-8" name="__codelineno-14-8" href="#__codelineno-14-8"></a><span class="c1"># GradientBoosting tradicional</span>
<a id="__codelineno-14-9" name="__codelineno-14-9" href="#__codelineno-14-9"></a><span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<a id="__codelineno-14-10" name="__codelineno-14-10" href="#__codelineno-14-10"></a><span class="n">gb</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<a id="__codelineno-14-11" name="__codelineno-14-11" href="#__codelineno-14-11"></a><span class="n">gb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-14-12" name="__codelineno-14-12" href="#__codelineno-14-12"></a><span class="n">gb_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
<a id="__codelineno-14-13" name="__codelineno-14-13" href="#__codelineno-14-13"></a>
<a id="__codelineno-14-14" name="__codelineno-14-14" href="#__codelineno-14-14"></a><span class="c1"># HistGradientBoosting</span>
<a id="__codelineno-14-15" name="__codelineno-14-15" href="#__codelineno-14-15"></a><span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<a id="__codelineno-14-16" name="__codelineno-14-16" href="#__codelineno-14-16"></a><span class="n">hgb</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<a id="__codelineno-14-17" name="__codelineno-14-17" href="#__codelineno-14-17"></a><span class="n">hgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-14-18" name="__codelineno-14-18" href="#__codelineno-14-18"></a><span class="n">hgb_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
<a id="__codelineno-14-19" name="__codelineno-14-19" href="#__codelineno-14-19"></a>
<a id="__codelineno-14-20" name="__codelineno-14-20" href="#__codelineno-14-20"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"GradientBoosting: </span><span class="si">{</span><span class="n">gb_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s, Score: </span><span class="si">{</span><span class="n">gb</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-14-21" name="__codelineno-14-21" href="#__codelineno-14-21"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"HistGradientBoosting: </span><span class="si">{</span><span class="n">hgb_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s, Score: </span><span class="si">{</span><span class="n">hgb</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-14-22" name="__codelineno-14-22" href="#__codelineno-14-22"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Speedup: </span><span class="si">{</span><span class="n">gb_time</span><span class="o">/</span><span class="n">hgb_time</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x"</span><span class="p">)</span>
</code></pre></div>
<p><strong>Resultado típico:</strong></p>
<p>GradientBoosting: 45.23s, Score: 0.8756
HistGradientBoosting: 2.14s, Score: 0.8812
Speedup: 21.1x</p>
<h3 id="hiperparametros-clave">Hiperparámetros Clave<a class="headerlink" href="#hiperparametros-clave" title="Permanent link">¶</a></h3>
<p><strong>Control de iteraciones:</strong>
- <code>max_iter</code>: Número de boosting iterations (100-1000)
- <code>learning_rate</code>: Shrinkage (0.01-0.1)</p>
<p><strong>Control de complejidad:</strong>
- <code>max_leaf_nodes</code>: Máximo número de hojas (31 default)
- <code>max_depth</code>: Profundidad máxima (None = sin límite)
- <code>min_samples_leaf</code>: Mínimo de muestras en hojas (20 default)Regularización:</p>
<p>l2_regularization: Regularización L2 en hojas (0.0 default)
Histogramas:</p>
<p>max_bins: Número de bins para histogramas (255 default, máximo)
Early stopping:</p>
<p>early_stopping: 'auto', True, False
validation_fraction: Fracción para validación (0.1 default)
n_iter_no_change: Paciencia para early stopping (10 default)
Ventajas de HistGradientBoosting
✅ Mucho más rápido que GradientBoosting tradicional (10-100x)
✅ Escala a datasets grandes (millones de ejemplos)
✅ Manejo nativo de missing values
✅ Soporte para categóricas (experimental)
✅ Early stopping incorporado
✅ Incluido en sklearn (sin instalación adicional)
✅ API similar a GradientBoosting tradicional
✅ Rendimiento comparable a XGBoost/LightGBM
Limitaciones
⚠️ Features categóricas aún experimental
⚠️ API ligeramente diferente (max_iter vs n_estimators)
⚠️ Menos features que XGBoost/LightGBM/CatBoost
⚠️ Sin soporte GPU
⚠️ Comunidad más pequeña que XGBoost
Cuándo usar HistGradientBoostingUsar HistGradientBoosting cuando:</p>
<p>✅ Quieres rapidez de LightGBM sin instalación extra
✅ Dataset grande (&gt;10k ejemplos, &gt;50 features)
✅ Tienes missing values (soporte nativo)
✅ Quieres quedarte dentro de sklearn (sin dependencias)
✅ No necesitas features avanzadas de XGBoost
Considerar alternativas cuando:</p>
<p>❌ Dataset pequeño → GradientBoosting tradicional está bien
❌ Necesitas features categóricas robustas → CatBoost
❌ Máximo rendimiento en competición → XGBoost/LightGBM
❌ Necesitas GPU → XGBoost/LightGBM/CatBoost</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:friedman2001greedy">
<p>Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. <em>Annals of Statistics</em>, <em>29</em>(5), 1189--1232.&nbsp;<a class="footnote-backref" href="#fnref:friedman2001greedy" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn:chen2016xgboost">
<p>Chen, T., &amp; Guestrin, C. (2016). XGBoost: A scalable tree boosting system. <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 785--794.&nbsp;<a class="footnote-backref" href="#fnref:chen2016xgboost" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
<li id="fn:ke2017lightgbm">
<p>Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., &amp; Liu, T.-Y. (2017). LightGBM: A highly efficient gradient boosting decision tree. <em>Advances in Neural Information Processing Systems</em>, <em>30</em>, 3146--3154.&nbsp;<a class="footnote-backref" href="#fnref:ke2017lightgbm" title="Jump back to footnote 3 in the text">↩</a></p>
</li>
<li id="fn:prokhorenkova2018catboost">
<p>Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., &amp; Gulin, A. (2018). CatBoost: Unbiased boosting with categorical features. <em>Advances in Neural Information Processing Systems</em>, <em>31</em>, 6638--6648.&nbsp;<a class="footnote-backref" href="#fnref:prokhorenkova2018catboost" title="Jump back to footnote 4 in the text">↩</a></p>
</li>
<li id="fn:friedman2002stochastic">
<p>Friedman, J. H. (2002). Stochastic gradient boosting. <em>Computational Statistics\ &amp; Data Analysis</em>, <em>38</em>(4), 367--378. <a href="https://doi.org/10.1016/S0167-9473(01)00065-2">https://doi.org/10.1016/S0167-9473(01)00065-2</a>&nbsp;<a class="footnote-backref" href="#fnref:friedman2002stochastic" title="Jump back to footnote 5 in the text">↩</a></p>
</li>
</ol>
</div>





                
              </article>
            </div>
          
          
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"></path></svg>
            Volver al principio
          </a>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.sections", "navigation.expand", "navigation.top", "navigation.indexes", "toc.follow", "toc.integrate", "search.suggest", "search.highlight", "content.code.copy", "content.code.annotate"], "search": "../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copiado al portapapeles", "clipboard.copy": "Copiar al portapapeles", "search.result.more.one": "1 m\u00e1s en esta p\u00e1gina", "search.result.more.other": "# m\u00e1s en esta p\u00e1gina", "search.result.none": "No se encontraron documentos", "search.result.one": "1 documento encontrado", "search.result.other": "# documentos encontrados", "search.result.placeholder": "Teclee para comenzar b\u00fasqueda", "search.result.term.missing": "Falta", "select.version": "Seleccionar versi\u00f3n"}}</script>
    
    
      <script src="../assets/javascripts/bundle.b78d2936.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>