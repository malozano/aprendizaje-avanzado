<!DOCTYPE html><html lang="es" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Apuntes de la asignatura Aprendizaje Avanzado del Grado en Ingeniería en Inteligencia Artificial de la Universidad de Alicante">
      
      
        <meta name="author" content="Miguel Angel Lozano">
      
      
        <link rel="canonical" href="https://malozano.github.io/aprendizaje-avanzado/04-random-forest/">
      
      
        <link rel="prev" href="../03-arboles-decision/">
      
      
        <link rel="next" href="../practicas/00-datos-y-visualizacion/">
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.1.1">
    
    
      
        <title>4. Métodos de ensemble. Random Forest - Aprendizaje Avanzado</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.402914a4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.a0c5b2b5.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#metodos-de-ensemble" class="md-skip">
          Saltar a contenido
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Cabecera">
    <a href=".." title="Aprendizaje Avanzado" class="md-header__button md-logo" aria-label="Aprendizaje Avanzado" data-md-component="logo">
      
  <img src="../images/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Aprendizaje Avanzado
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              4. Métodos de ensemble. Random Forest
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo" aria-label="Modo oscuro" type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Modo oscuro" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
            </label>
          
        
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="white" data-md-color-accent="blue" aria-label="Modo claro" type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Modo claro" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Búsqueda" placeholder="Búsqueda" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Buscar">
        
        <button type="reset" class="md-search__icon md-icon" title="Limpiar" aria-label="Limpiar" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Inicializando búsqueda
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navegación" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Aprendizaje Avanzado" class="md-nav__button md-logo" aria-label="Aprendizaje Avanzado" data-md-component="logo">
      
  <img src="../images/logo.png" alt="logo">

    </a>
    Aprendizaje Avanzado
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Introducción
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Bloque I. Aprendizaje supervisado
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Bloque I. Aprendizaje supervisado
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01-modelos-no-param/" class="md-nav__link">
        1. Modelos paramétricos y no paramétricos. Logistic Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02-svm/" class="md-nav__link">
        2. SVM
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03-arboles-decision/" class="md-nav__link">
        3. Árboles de decisión
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          4. Métodos de ensemble. Random Forest
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        4. Métodos de ensemble. Random Forest
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Tabla de contenidos">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Tabla de contenidos
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#voting" class="md-nav__link">
    Voting
  </a>
  
    <nav class="md-nav" aria-label="Voting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hard-voting" class="md-nav__link">
    Hard Voting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#soft-voting" class="md-nav__link">
    Soft Voting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#soft-voting-ponderado" class="md-nav__link">
    Soft Voting ponderado
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#promediado" class="md-nav__link">
    Promediado
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementacion" class="md-nav__link">
    Implementación
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#consideraciones-finales-sobre-voting" class="md-nav__link">
    Consideraciones finales sobre Voting
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stacking" class="md-nav__link">
    Stacking
  </a>
  
    <nav class="md-nav" aria-label="Stacking">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#algoritmo-de-entrenamiento" class="md-nav__link">
    Algoritmo de entrenamiento
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementacion_1" class="md-nav__link">
    Implementación
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variantes-de-stacking" class="md-nav__link">
    Variantes de Stacking
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#consideraciones-finales-sobre-stacking" class="md-nav__link">
    Consideraciones finales sobre Stacking
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bagging-bootstrap-aggregating" class="md-nav__link">
    Bagging (Bootstrap Aggregating)
  </a>
  
    <nav class="md-nav" aria-label="Bagging (Bootstrap Aggregating)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bootstrap-sampling" class="md-nav__link">
    Bootstrap Sampling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#algoritmo-de-bagging" class="md-nav__link">
    Algoritmo de Bagging
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#out-of-bag-oob-error" class="md-nav__link">
    Out-of-Bag (OOB) Error
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#analisis-del-metodo" class="md-nav__link">
    Análisis del método
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementacion-de-bagging-generico" class="md-nav__link">
    Implementación de Bagging Genérico
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#consideraciones-finales-sobre-bagging" class="md-nav__link">
    Consideraciones finales sobre Bagging
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-forest" class="md-nav__link">
    Random Forest
  </a>
  
    <nav class="md-nav" aria-label="Random Forest">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#algoritmo-de-random-forest" class="md-nav__link">
    Algoritmo de Random Forest
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hiperparametros" class="md-nav__link">
    Hiperparámetros
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#feature-importance" class="md-nav__link">
    Feature Importance
  </a>
  
    <nav class="md-nav" aria-label="Feature Importance">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gini-importance" class="md-nav__link">
    Gini Importance
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#permutation-importance" class="md-nav__link">
    Permutation Importance
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#out-of-bag-oob-error-en-random-forest" class="md-nav__link">
    Out-of-Bag (OOB) Error en Random Forest
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#extra-trees-extremely-randomized-trees" class="md-nav__link">
    Extra Trees (Extremely Randomized Trees)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#consideraciones-finales" class="md-nav__link">
    Consideraciones finales
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3">
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          Prácticas
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Prácticas
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../practicas/00-datos-y-visualizacion/" class="md-nav__link">
        0. Datos y visualización
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="metodos-de-ensemble">Métodos de ensemble<a class="headerlink" href="#metodos-de-ensemble" title="Permanent link">¶</a></h1>
<p>La idea tras los modelos de <em>ensemble</em> (Sagi &amp; Rokach, 2018)<sup id="fnref:sagi2018ensemble"><a class="footnote-ref" href="#fn:sagi2018ensemble">1</a></sup> consiste en combinar diferentes modelos base sencillos para construir un modelo más robusto y preciso que cualquiera de los modelos individuales. </p>
<p>Hemos visto que modelos diferentes cometen errores diferentes. Buscamos combinarlos de forma que podamos eliminar o reducir esos errores individuales. Para ello necesitaremos combinar un conjunto diverso de modelos, y cada uno de estos modelos individuales debería proporcionar por si mismo una precisión que sea superior al azar.</p>
<p>El aprendizaje con métodos de <em>ensemble</em> puede descomponerse en dos tareas principales:</p>
<ul>
<li>Aprender un conjunto de modelos base a partir de los datos de entrenamiento.</li>
<li>Combinarlos para construir el predictor conjunto.</li>
</ul>
<p>Una de las principales ventajas de los <em>ensembles</em> es que pueden mejorar el compromiso entre <strong>sesgo y varianza</strong>. Recordemos que el error esperado se puede descomponer como: </p>
<div class="arithmatex">\[E[(y - \hat{y})^2] = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}\]</div>
<p>Existen cuatro principales tipos de enfoques para abordar la combinación de modelos: <em>Voting</em>, <em>Stacking</em>, <em>Bagging</em> y <em>Boosting</em>. Diferentes enfoques abordarán el problema de forma distinta. Algunos tipos de métodos de <em>ensemble</em> se centran en reducir principalmente la varianza, como por ejemplo los métodos de <em>Bagging</em>, mientras que otros se centran fundamentalmente en el sesgo, como sería el caso del <em>Boosting</em>. </p>
<p>Cada categoría de métodos de <em>ensemble</em> tiene un enfoque diferente para generar y combinar modelos, tal como se resume en la siguiente tabla:</p>
<table>
<thead>
<tr>
<th>Categoría</th>
<th>Modelos</th>
<th>Datos de entrenamiento</th>
<th>Combinación</th>
<th>Entrenamiento</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Voting</strong></td>
<td>Heterogéneos</td>
<td>Iguales</td>
<td>Fija (votos/promedio)</td>
<td>Independiente</td>
</tr>
<tr>
<td><strong>Stacking</strong></td>
<td>Heterogéneos</td>
<td>Iguales</td>
<td>Aprendida (meta-modelo)</td>
<td>Dos niveles</td>
</tr>
<tr>
<td><strong>Bagging</strong></td>
<td>Homogéneos</td>
<td>Bootstrap (diferentes)</td>
<td>Fija (promedio)</td>
<td>Independiente</td>
</tr>
<tr>
<td><strong>Boosting</strong></td>
<td>Homogéneos</td>
<td>Ponderados/residuos</td>
<td>Ponderada (aprendida)</td>
<td>Secuencial</td>
</tr>
</tbody>
</table>
<p>A continuación estudiaremos en detalle cada una de estas categorías, y los principales métodos que existen dentro de cada una de ellas.</p>
<h2 id="voting">Voting<a class="headerlink" href="#voting" title="Permanent link">¶</a></h2>
<p>La idea tras los modelos de <em>Voting</em> (Kittler et al., 1998)<sup id="fnref:kittler1998combining"><a class="footnote-ref" href="#fn:kittler1998combining">2</a></sup> es la de entrenar múltiples modelos independientes y combinar sus predicciones mediante votación (en el caso de clasifiación) o mediante promediado (en el caso de regresión). </p>
<p>En este caso los modelos se entrenan de forma independiente con el <strong>mismo conjunto de datos</strong>, y no hay dependencia entre modelos, por lo que pueden entrenarse en paralelo. Además, podemos <strong>combinar diferentes tipos de modelos</strong>. </p>
<p>Por ejemplo, podríamos combinar un modelo de Regresión Logística, con KNN y SVM, obtener la predicción que devuelve cada uno de ellos, y devolver aquella que obtenga más votos. </p>
<p>Encontramos diferentes formas de abordar la votación, que podremos aplicar según se trate de un problema de clasificación o de regresión.</p>
<p>Vamos a considerar que combinamos <span class="arithmatex">\(M\)</span> clasificadores <span class="arithmatex">\(\{h_1(\mathbf{x}), h_2(\mathbf{x}), \ldots, h_M(\mathbf{x}) \}\)</span>. A continuación veremos cómo combinar sus predicciones con cada enfoque de votación.</p>
<h3 id="hard-voting">Hard Voting<a class="headerlink" href="#hard-voting" title="Permanent link">¶</a></h3>
<p>Se trata de un enfoque dirigido al problema de clasificación. Con el enfoque <em>Hard Voting</em>, la clase predicha por el clasificador será la que reciba más votos por parte de los clasificadores individuales (es decir, la moda del conjunto de predicciones). </p>
<div class="arithmatex">\[\hat{y} = \text{mode}(h_1(\mathbf{x}), h_2(\mathbf{x}), ..., h_M(\mathbf{x}))\]</div>
<p><strong>Ejemplo:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>Problema: Clasificar un email como spam o no-spam
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>Modelo 1 (Logistic Regression): spam
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>Modelo 2 (Decision Tree):       spam  
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>Modelo 3 (SVM):                 no-spam
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>Modelo 4 (KNN):                 spam
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>Resultado final: spam (3 votos contra 1)
</code></pre></div><p></p>
<h3 id="soft-voting">Soft Voting<a class="headerlink" href="#soft-voting" title="Permanent link">¶</a></h3>
<p>A diferencia del caso anterior, con el enfoque <em>Soft Voting</em> lo que tendremos en cuenta es la suma de probabilidades de predicción de cada clasificador individual. Aquella clase <span class="arithmatex">\(k\)</span> cuya suma de probabilidades de predicción sea mayor, será la seleccionada como predicción del modelo combinado:</p>
<div class="arithmatex">\[\hat{y} = \arg\max_k \frac{1}{M} \sum_{i=1}^{M} P_i(y = k | \mathbf{x})\]</div>
<p><strong>Ejemplo:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>Modelo 1: P(spam) = 0.9, P(no-spam) = 0.1
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>Modelo 2: P(spam) = 0.6, P(no-spam) = 0.4
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>Modelo 3: P(spam) = 0.4, P(no-spam) = 0.6
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>Promedio: P(spam) = 0.633, P(no-spam) = 0.367
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>Resultado: spam
</code></pre></div><p></p>
<p>Es importante destacar que para poder utilizar este enfoque, los modelos individuales deben poder proporcionarnos la probabilidad de la predicción. Este enfoque tiene la ventaja de que considera la confianza de cada modelo en la predicción realizada.</p>
<h3 id="soft-voting-ponderado">Soft Voting ponderado<a class="headerlink" href="#soft-voting-ponderado" title="Permanent link">¶</a></h3>
<p>Se trata de un caso similar al anterior, pero dando un peso diferente a cada predictor individual en la suma:</p>
<div class="arithmatex">\[\hat{y} = \arg\max_k \sum_{i=1}^{M} w_i \cdot P_i(y = k | \mathbf{x})\]</div>
<p>Donde <span class="arithmatex">\(\sum w_i = 1\)</span> y <span class="arithmatex">\(w_i\)</span> nos permite dar mayor peso a los modelos en los que tengamos mayor confianza. Estos valores se pueden determinar a partir de la precisión de los modelos individuales en la validación, de métricas como F1-score o el inverso del error cometido.</p>
<p><strong>Ejemplo:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>Modelo 1 (accuracy=0.85): w1 = 0.4
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>Modelo 2 (accuracy=0.80): w2 = 0.35  
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>Modelo 3 (accuracy=0.75): w3 = 0.25
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>La votación ponderada da más peso al mejor modelo
</code></pre></div><p></p>
<h3 id="promediado">Promediado<a class="headerlink" href="#promediado" title="Permanent link">¶</a></h3>
<p>Este enfoque, a diferencia de los anteriores, está dirigido al problema de regresión. En caso de regresión las predicciones de promedian de la siguiente forma:</p>
<div class="arithmatex">\[\hat{y} = \frac{1}{M} \sum_{i=1}^{M} h_i(\mathbf{x})\]</div>
<p>También es posible asignar un peso a cada predictor, igual que en el caso del enfoque anterior:</p>
<div class="arithmatex">\[\hat{y} = \sum_{i=1}^{M} w_i  h_i(\mathbf{x})\]</div>
<h3 id="implementacion">Implementación<a class="headerlink" href="#implementacion" title="Permanent link">¶</a></h3>
<p>En sklearn tenemos las clases <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier">VotingClassifier</a> y <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html#sklearn.ensemble.VotingRegressor">VotingRegressor</a>, para problemas de clasificación y regresión respectivamente.</p>
<p>Para utilizar estas clases deberemos pasar como parámetro <code>estimator</code> la lista de predictores individuales que queramos combinar, y nos permitirán aplicar cualquiera de los enfoques anteriores.</p>
<p>En el caso de la clasificación, podemos elegir el tipo de votación con el parámetro <code>voting</code>, que podrá tomar como valores <code>hard</code> o <code>soft</code>, y en el caso del segundo tipo con <code>weights</code> podemos especificar pesos específicos para cada predictor.</p>
<p>En el caso de la regresión, también contamos con un parámetro <code>weights</code> para especificar los pesos de cada predicción.</p>
<h3 id="consideraciones-finales-sobre-voting">Consideraciones finales sobre <em>Voting</em><a class="headerlink" href="#consideraciones-finales-sobre-voting" title="Permanent link">¶</a></h3>
<p>Se trata de un método muy fácil de entender y sencillo de implementar, que se basa en modelos existentes que no es necesario modificar. Todos los modelos base se entrenan independientemente y podría hacerse en paralelo. </p>
<p>Debemos tener en cuenta que para que sea efectivo los diferentes modelos base deben ser diversos, ya que si todos son similares no obtendremos apenas ganancia combinándolos. Funcionará mejor cuando los modelos base capturen aspectos distintos de los datos y cometan errores diferentes. </p>
<p>Este método también ser verá beneficiado cuando los modelos tengan un rendimiento similar. Si uno de los modelos base fuera muy superior, sería recomendable utilizar únicamente dicho modelo, mientras que si tenemos un modelo peor que el azar, ese modelo perjudicará al <em>ensemble</em> y convendría eliminarlo.</p>
<p>Voting es la forma más básica de modelo de <em>ensemble</em>, en la que se utiliza una combinación fija de modelos (votos o promedios con pesos predefinidos). Esto nos lleva a preguntarnos si podríamos aprender la mejor forma de combinar los modelos. Esto lo abordaremos con los métodos de <em>Stacking</em> que veremos a continuación.</p>
<h2 id="stacking">Stacking<a class="headerlink" href="#stacking" title="Permanent link">¶</a></h2>
<p>En el caso de <em>Stacking</em> (Wolpert, 1992)<sup id="fnref:wolpert1992stacked"><a class="footnote-ref" href="#fn:wolpert1992stacked">3</a></sup>, al igual que en <em>Voting</em> tenemos una serie de modelos base heterogéneos que entrenamos de forma independiente, pero a diferencia del caso anterior, no tendremos una combinación fija, sino que utilizaremos un <strong>meta-modelo</strong> para aprender la forma de combinar los diferentes modelos base. </p>
<p>De esta forma, se podrán capturar relaciones complejas entre las diferentes predicciones. Tendremos una arquitectura en dos niveles, en la que en el nivel inferior tendremos los <span class="arithmatex">\(M\)</span> diferentes modelos base, cada uno de los cuales producirá una predicción <span class="arithmatex">\(p_i\)</span> y en el nivel superior tendremos el meta-modelo, que recibirá como entrada las diferentes predicciones <span class="arithmatex">\(\{ p_1, p_2, \ldots, p_M \}\)</span> y producirá como salida la predicción del <em>ensemble</em>.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>NIVEL 0 (Modelos base):
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>├─ Modelo 1 (ej: DT)               → predicción p1
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>├─ Modelo 2 (ej: SVM)              → predicción p2
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>├─ Modelo 3 (ej: Logistic Reg)     → predicción p3
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>└─ Modelo 4 (ej: KNN)              → predicción p4
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>NIVEL 1 (Meta-modelo):
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a>└─ Recibe [p1, p2, p3, p4] como features
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a>   → aprende la mejor combinación
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a>   → predicción final
</code></pre></div>
<h3 id="algoritmo-de-entrenamiento">Algoritmo de entrenamiento<a class="headerlink" href="#algoritmo-de-entrenamiento" title="Permanent link">¶</a></h3>
<p>El entrenamiento de un <em>ensemble</em> de tipo <em>Stacking</em> se hará en varios pasos, ya que en primer lugar deberemos obtener una serie de predicciones de los modelos base para entrenar con ellas el meta-modelo. Para reducir el <em>overfitting</em>, utilizaremos <em>cross-validation</em> a la hora de generar estas predicciones, siguiendo el siguiente proceso:</p>
<p><strong>Paso 1: Obtención de predicciones para el meta-modelo (Nivel 0)</strong></p>
<p>Supongamos que contamos con <span class="arithmatex">\(M\)</span> modelos base <span class="arithmatex">\(m_j\)</span>, con <span class="arithmatex">\(j=1, 2, \ldots, M\)</span>. Entrenaremos cada uno de estos modelos utilizando <em>cross-validation</em>, y para cada <em>fold</em>:</p>
<ul>
<li>Dividimos el <em>dataset</em> en datos de entrenamiento y datos de validación. </li>
<li>Cada modelo base se entrena con el conjunto de entrenamiento del <em>fold</em></li>
<li>Se generan predicciones para los datos del conjunto de validación. Estas son predicciones <strong>out-of-fold</strong> (OOF), ya que han sido generadas para cada observación utilizando un modelo que no ha sido entrenado con dicha observación.</li>
<li>Guardamos las predicciones OOF generadas.</li>
</ul>
<p>De esta forma, obtendremos un nuevo conjunto de datos compuesto por las predicciones OOF generadas por cada modelo base para cada ejemplo de entrada. </p>
<p>Considerando que tenemos <span class="arithmatex">\(N\)</span> ejemplos de entrada en nuestro <em>dataset</em> y <span class="arithmatex">\(M\)</span> modelos, tendremos una matriz como la siguiente:</p>
<div class="arithmatex">\[
Z = 
\begin{bmatrix}
\hat{y}_1^{m_1} &amp; \hat{y}_1^{m_2} &amp; \ldots &amp; \hat{y}_1^{m_M} \\
\hat{y}_2^{m_1} &amp; \hat{y}_2^{m_2} &amp; \ldots &amp; \hat{y}_2^{m_M} \\
\vdots &amp; \vdots &amp;   &amp; \vdots \\
\hat{y}_N^{m_1} &amp; \hat{y}_N^{m_2} &amp; \ldots &amp; \hat{y}_N^{m_M} \\
\end{bmatrix} 
\]</div>
<p>Esta matriz <span class="arithmatex">\(Z\)</span> constituirá los datos de entrada para el entrenamiento del meta-modelo. Es importante haber utilizado las predicciones OOF para construirla, ya que si hubiéramos obtenido predicciones de los modelos base obtenidas a partir de datos vistos durante el entrenamiento, habríamos tenido un caso demasiado optimista y habríamos favorecido el <em>overfitting</em>.</p>
<p><strong>Paso 2: Entrenar el meta-modelo (Nivel 1)</strong></p>
<p>En este punto utilizamos la matriz <span class="arithmatex">\(Z\)</span> de predicciones OOF generada en el paso anterior como entrada del meta-modelo. Considerando que <span class="arithmatex">\(\mathbf{z}_i\)</span> (fila <span class="arithmatex">\(i\)</span>-ésima de la matriz <span class="arithmatex">\(Z\)</span>) es una tupla con las predicciones OOF generadas por cada uno de los <span class="arithmatex">\(M\)</span> modelos para el ejemplo de entrada <span class="arithmatex">\(\mathbf{x}_i\)</span>, al entrenar el meta-modelo la salida esperada será la etiqueta original <span class="arithmatex">\(y_i\)</span>. </p>
<p>Es decir, utilizaremos para entrenar el meta-modelo un conjunto de pares <span class="arithmatex">\((\mathbf{z}_i, y_i)\)</span>, con <span class="arithmatex">\(i=1, 2, \ldots, N\)</span>. </p>
<p><strong>Paso 3: Entrenamiento definitivo de los modelos base (Nivel 0)</strong></p>
<p>Dado que queremos tener el mejor modelo posible, entrenaremos ahora de nuevo todos los modelos base pero utilizando el <em>dataset</em> completo. De esta forma, guardaremos estos modelos base reentrenados junto con el meta-modelo entrenado en el paso anterior, y con esto tendremos el modelo completo.</p>
<p><strong>Predicción</strong></p>
<p>Una vez entrenados de forma definitiva modelos base y meta-modelo, a la hora de obtener una predicción con un nuevo dato <span class="arithmatex">\(\mathbf{x}\)</span> seguiremos el siguiente proceso</p>
<ol>
<li>Cada modelo base <span class="arithmatex">\(m_j\)</span>, con <span class="arithmatex">\(j=1,2,\ldots,M\)</span>, produce una predicción <span class="arithmatex">\(z_j\)</span> para <span class="arithmatex">\(\mathbf{x}\)</span>.</li>
<li>El meta-modelo recibe como entrada la tupla <span class="arithmatex">\((z_1, z_2, \ldots, z_M)\)</span> de predicciones realizadas por los modelos base.</li>
<li>El meta-modelo genera predicción final <span class="arithmatex">\(y\)</span>.</li>
</ol>
<h3 id="implementacion_1">Implementación<a class="headerlink" href="#implementacion_1" title="Permanent link">¶</a></h3>
<p>En la librería <em>sklearn</em> contamos con las clases <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html">StackingClassifier</a> y <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html">StackingRegressor</a> para implementar <em>Stacking</em> tanto en problemas de clasificación como de regresión. </p>
<p>A la hora de construir este <em>ensemble</em>, deberemos proporcionar tanto un conjunto de modelos base, en el parámetro <code>estimators</code>, como un meta-modelo, en <code>final_estimator</code>. A continuación, mostramos un ejemplo de código con 3 modelos base, y con regresión logística como meta-modelo:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="c1"># Definir modelos base (nivel 0)</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a><span class="n">base_models</span> <span class="o">=</span> <span class="p">[</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a>    <span class="p">(</span><span class="s1">'rf'</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)),</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a>    <span class="p">(</span><span class="s1">'gb'</span><span class="p">,</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)),</span>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a>    <span class="p">(</span><span class="s1">'svm'</span><span class="p">,</span> <span class="n">SVC</span><span class="p">(</span><span class="n">probability</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a><span class="p">]</span>
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a>
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a><span class="c1"># Definir meta-modelo (nivel 1)</span>
<a id="__codelineno-4-9" name="__codelineno-4-9" href="#__codelineno-4-9"></a><span class="n">meta_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<a id="__codelineno-4-10" name="__codelineno-4-10" href="#__codelineno-4-10"></a>
<a id="__codelineno-4-11" name="__codelineno-4-11" href="#__codelineno-4-11"></a><span class="c1"># Crear stacking ensemble</span>
<a id="__codelineno-4-12" name="__codelineno-4-12" href="#__codelineno-4-12"></a><span class="n">stacking</span> <span class="o">=</span> <span class="n">StackingClassifier</span><span class="p">(</span>
<a id="__codelineno-4-13" name="__codelineno-4-13" href="#__codelineno-4-13"></a>    <span class="n">estimators</span><span class="o">=</span><span class="n">base_models</span><span class="p">,</span>
<a id="__codelineno-4-14" name="__codelineno-4-14" href="#__codelineno-4-14"></a>    <span class="n">final_estimator</span><span class="o">=</span><span class="n">meta_model</span>
<a id="__codelineno-4-15" name="__codelineno-4-15" href="#__codelineno-4-15"></a><span class="p">)</span>
<a id="__codelineno-4-16" name="__codelineno-4-16" href="#__codelineno-4-16"></a>
<a id="__codelineno-4-17" name="__codelineno-4-17" href="#__codelineno-4-17"></a><span class="c1"># Entrenar</span>
<a id="__codelineno-4-18" name="__codelineno-4-18" href="#__codelineno-4-18"></a><span class="n">stacking</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-4-19" name="__codelineno-4-19" href="#__codelineno-4-19"></a>
<a id="__codelineno-4-20" name="__codelineno-4-20" href="#__codelineno-4-20"></a><span class="c1"># Predecir</span>
<a id="__codelineno-4-21" name="__codelineno-4-21" href="#__codelineno-4-21"></a><span class="n">predictions</span> <span class="o">=</span> <span class="n">stacking</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div>
<h3 id="variantes-de-stacking">Variantes de <em>Stacking</em><a class="headerlink" href="#variantes-de-stacking" title="Permanent link">¶</a></h3>
<p>Podemos encontrar diferentes variantes de <em>Stacking</em>. Vamos a continuación a describir algunas de ellas.</p>
<p>Una de las variantes es la conocida como <strong>Blending</strong>. Se diferencia de <em>Stacking</em> básicamente en que en lugar de utilizar un <em>K-Fold</em> para generar predicciones OOF para el entrenamiento del meta-modelo, utiliza un único particionamiento en conjunto de entrenamiento y conjunto de validación. Por ejemplo, podemos particionar con un 80% de los datos para entrenamiento y 20% para validación, con lo cual, sólo se estarían generando predicciones con el 20% de los datos. <em>Blending</em> resulta más sencillo y rápido, pero estaremos entrenando el meta-modelo con menos datos. Con un <em>dataset</em> pequeño podemos perder rendimiento y tendremos mayor varianza, pero en caso de tener un <em>dataset</em> grande y buscar reducir el coste computacional <em>blending</em> puede ser una opción adecuada.</p>
<p>Otra variante a considerar es el <strong>Stacking multi-nivel</strong>. Este método consiste en apilar múltiples niveles de meta-modelos:
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>NIVEL 0: Modelos base 
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>    ↓ (predicciones)
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a>NIVEL 1: Primer grupo de meta-modelos
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>    ↓ (predicciones)
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a>NIVEL 2: Meta-meta-modelo final
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a>    ↓
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a>Predicción final
</code></pre></div><p></p>
<p>Esta técnica tiene como ventaja que puede ser capaz de capturar relaciones muy complejas, pero tiene mayor riesgo de <em>overfitting</em> y resulta difícil de interpretar. </p>
<p>Según el <strong>tipo de meta-modelo</strong>, podríamos agrupar los tipos de <em>Stacking</em> en tres grandes categorías:</p>
<ul>
<li><strong>Linear Stacking:</strong> El meta-modelo es un modelo lineal, típicamente Regresión Lineal o Regresión Logística.</li>
<li><strong>Tree Stacking:</strong> El meta-modelo es un modelo basado en árboles, como Árboles de Decisión o Random Forest.</li>
<li><strong>Neural Stacking:</strong> El meta-modelo es una Red Neuronal.</li>
</ul>
<p>También podemos considerar varias variantes según el <strong>tipo de meta-<em>features</em></strong> (<em>features</em> generadas para el meta-modelo):</p>
<ul>
<li><strong>Predicciones solo:</strong> Tenemos como meta-<em>features</em> únicamente las predicciones de los modelos base. Por ejemplo, tendríamos únicamente <span class="arithmatex">\(0\)</span> o <span class="arithmatex">\(1\)</span> en caso de clasificación binaria. Si queremos que <em>sklearn</em> utilice este tipo de predicciones, podemos proporcionar el parámetro <code>stack_method='predict'</code>. </li>
<li><strong>Probabilidad:</strong> Como meta-<em>features</em> tendríamos las probabilidades de pertenencia a cada clase. Para que <em>sklearn</em> utilice este tipo de características, todos los modelos base deben contar con el método <code>predict_proba</code>. Como por defecto tenemos <code>stack_method='auto'</code>, si todos los modelos base cuentan con <code>predict_proba</code> entonces utilizará de forma preferente estas probabilidades.</li>
<li><strong>Predicciones y <em>features</em> originales:</strong> Concatenemos las <em>features</em> originales del ejemplo de entrada con las predicciones realizadas por los modelos base. En <em>sklearn</em> esto lo conseguiremos proporcionando el parámetro <code>passthrough=True</code>.</li>
</ul>
<h3 id="consideraciones-finales-sobre-stacking">Consideraciones finales sobre <em>Stacking</em><a class="headerlink" href="#consideraciones-finales-sobre-stacking" title="Permanent link">¶</a></h3>
<p><em>Stacking</em> lleva la idea de ensemble un paso más allá que <em>Voting</em>, ya que en lugar de usar combinaciones fijas, <strong>aprende la mejor forma de combinar modelos</strong>. Resulta muy <strong>flexible</strong>, pudiendo utilizar cualquier tipo de modelo en cualquier nivel y normalmente ofrecerá <strong>mejor rendimiento</strong> que <em>Voting</em>.</p>
<p>Sin embargo, es más <strong>complejo</strong>, ya que debemos ajustar un gran número de hiper-parámetros en sus diferentes niveles. Tiene un <strong>alto coste computacional</strong>, ya que tenemos que entrenar tanto modelos base como meta-modelo, y es <strong>difícil de interpretar</strong>.</p>
<p>Además, <em>Stacking</em> es propenso al <strong><em>overfitting</em></strong> y para mitigarlo es importante, tal como hemos comentado, utilizar <em>cross-validation</em> para generar las meta-<em>features</em>. Otras estrategias que pueden ayudar a mitigar este problema es utilizar un meta-modelo sencillo, como puede ser Regresión Logística, y utilizar siempre regularización en el meta-modelo. </p>
<p><em>Stacking</em> será un método adecuado cuando contemos con <strong>modelos buenos y diversos</strong>, y un <strong>conjunto de datos suficientemente grande</strong> como para evitar el <em>overfitting</em>. Sin embargo, con conjuntos de datos pequeños tendremos un alto riesgo de <em>overfitting</em>. </p>
<p>Hasta ahora hemos visto métodos que combinan modelos <strong>heterogéneos</strong> entrenados en los <strong>mismos datos</strong>, pero, ¿y si en lugar de esto buscamos la diversidad entrenando un mismo modelo con diferentes muestras de datos?. Esta pregunta nos lleva a explorar el siguiente tipo de métodos de <em>ensemble</em>, el conocido como <strong>Bagging</strong>.</p>
<h2 id="bagging-bootstrap-aggregating">Bagging (Bootstrap Aggregating)<a class="headerlink" href="#bagging-bootstrap-aggregating" title="Permanent link">¶</a></h2>
<p>A diferencia de <em>Voting</em> y <em>Stacking</em>, los métodos de <em>Bagging</em> (Breiman, 1996)<sup id="fnref:breiman1996bagging"><a class="footnote-ref" href="#fn:breiman1996bagging">4</a></sup> se basan en entrenar múltiples modelos <strong>homogéneos</strong> en <strong>diferentes muestras</strong> <em>bootstrap</em> (con reemplazo) del conjunto de entrenamiento, promediando sus predicciones.</p>
<p>Con esta técnica se busca principalmente <strong>reducir la varianza</strong>, con lo que va a ser efectiva principalmente cuando se aplique a modelos con alta varianza, como es el caso de los árboles de decisión. Los modelos pueden entrenarse en paralelo, cada uno de ellos con una muestra diferente de datos. Utiliza <strong>muestreo con reemplazo</strong> (<em>bootstrap sampling</em>) para obtener la muestra con la que se entrenará cada modelo. Es también importante destacar que <strong>se le da la misma importancia a todas las predicciones</strong>. Estas se combinarán mediante promedio en caso de regresión, y mediante votación en caso de clasificación.</p>
<h3 id="bootstrap-sampling">Bootstrap Sampling<a class="headerlink" href="#bootstrap-sampling" title="Permanent link">¶</a></h3>
<p>El término <em>Bootstrap</em> se refiere a una técnica estadística de remuestreo con reemplazo. A partir de un <em>dataset</em> original <span class="arithmatex">\(\mathcal{D}\)</span> con <span class="arithmatex">\(N\)</span> ejemplos obtenemos <span class="arithmatex">\(B\)</span> <em>bootstrap samples</em> <span class="arithmatex">\(\mathcal{D}_b\)</span>, con <span class="arithmatex">\(b=1, 2, \ldots, B\)</span>. Cada <em>bootstrap sample</em> tendrá también <span class="arithmatex">\(N\)</span> ejemplos, pero podrá haber ejemplos del conjunto original repetidos o ausentes (<em>out-of-bag</em>). </p>
<p>Por ejemplo, considerando <span class="arithmatex">\(N=10\)</span>, podríamos tener:</p>
<div class="arithmatex">\[
\begin{align*}
\mathcal{D} &amp;= \{ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\} \\
\Downarrow \\
\mathcal{D}_1 &amp;= [1, 3, 3, 5, 7, 8, 8, 9, 10, 10] \\ 
\mathcal{D}_2 &amp;= [1, 2, 2, 4, 4, 5, 6, 8, 9, 9] \\
 &amp; \ldots \\
\mathcal{D}_B &amp;= [2, 3, 4, 5, 5, 6, 7, 7, 8, 10]
\end{align*}
\]</div>
<p>Podemos calcular la probabilidad de que un ejemplo sea <em>out-of-bag</em> como <span class="arithmatex">\((1-\frac{1}{N})^N\)</span>, y con un valor alto de <span class="arithmatex">\(N\)</span> está probabilidad converge a <span class="arithmatex">\(0.368\)</span>. Por lo tanto, aproximadamente el <span class="arithmatex">\(37 \%\)</span> de los ejemplos quedarán fuera de cada muestra.</p>
<h3 id="algoritmo-de-bagging">Algoritmo de Bagging<a class="headerlink" href="#algoritmo-de-bagging" title="Permanent link">¶</a></h3>
<p>A continuación se muestra el algoritmo de entrenamiento de <em>Bagging</em>:</p>
<div class="arithmatex">\[
\begin{align*}
&amp; \text{Entrada: } \mathcal{D} \\
&amp; \text{Para } b=1 \text{ hasta } B:\\
&amp; \quad \mathcal{D}_b \leftarrow \text{Seleccionar, con reemplazo } N \text{ ejemplos del conjunto } \mathcal{D} \\
&amp; \quad h_b \leftarrow \text{Entrenar un modelo con }\mathcal{D}_b \\
&amp; \text{Devuelve: } H = \{ h_1, h_2, \ldots, h_B \}
\end{align*}
\]</div>
<p>Una vez entrenado el <em>ensemble</em> <span class="arithmatex">\(H\)</span>, la <strong>predicción en caso de regresión</strong> se calculará como el promedio de las predicciones de cada modelo del <em>ensemble</em>:</p>
<div class="arithmatex">\[
\hat{y} = \frac{1}{B} \sum_{b=1}^B h_b(\mathbf{x})
\]</div>
<p>La <strong>predicción en caso de clasificación</strong> se obtendrá mediante votación (moda del conjunto de predicciones):</p>
<div class="arithmatex">\[
\hat{y} = \text{moda}(h_1(\mathbf{x}), h_2(\mathbf{x}), \ldots, h_B(\mathbf{x}))
\]</div>
<p>Si cada modelo individual <span class="arithmatex">\(h_b\)</span> proporciona una probabilidad <span class="arithmatex">\(P_b(y=k|\mathbf{x})\)</span> de que el ejemplo de entrada <span class="arithmatex">\(\mathbf{x}\)</span> pertenezca a la clase <span class="arithmatex">\(k\)</span>, entonces también podríamos calcular la probabilidad global del <em>ensemble</em> mediante promediado:</p>
<div class="arithmatex">\[
P(y=k|\mathbf{x}) = \frac{1}{B} \sum_{b=1}^B P_b(y=k|\mathbf{x})
\]</div>
<h3 id="out-of-bag-oob-error">Out-of-Bag (OOB) Error<a class="headerlink" href="#out-of-bag-oob-error" title="Permanent link">¶</a></h3>
<p>Como hemos comentado anteriormente, aproximadamente el 37% de las muestras no aparecen en cada <em>bootstrap sample</em>. Estas muestras <strong>out-of-bag</strong> se pueden usar para validación sin necesidad de un conjunto separado.</p>
<p>A continuación se muestra el algoritmo para el <strong>cálculo del OOB error:</strong></p>
<p>$$
\begin{align*}
&amp; \text{Entrada: } \mathcal{D} \
&amp; \text{Para cada } (\mathbf{x}<em _in="\in" _mathcal_b="\mathcal{B" b="b">i, y_i) \in \mathcal{D}\
&amp; \quad \mathcal{B}_i \leftarrow {b: (\mathbf{x}_i, y_i) \notin \mathcal{D}_b  } \quad \text{(Modelos que no usaron } \mathbf({x}_i, y_i) \text{ en el entrenamiento)} \
&amp; \quad \text{Si } |\mathcal{B}_i| &gt; 0: \quad \text{(Calcula predicciones para estos modelos) }\
&amp; \quad\quad  \hat{y}_i^{\text{OOB}} =
\begin{cases}
    \text{moda}{h_b(\mathbf{x}_i) : b \in \mathcal{B}_i} \quad \text{(Clasificación)} \
    \frac{1}{|\mathcal{B}_i|} \sum</em><em oob="OOB">i} h_b(\mathbf{x}_i) \quad \text{(Regresión)}
\end{cases}
 \
&amp; \mathcal{I}</em> \leftarrow { i: |\mathcal{B}<em oob="OOB">i| &gt; 0 } \quad \text{(Índices de las muestras que fueron OOB para al menos un modelo)} \ 
&amp;OOBError \leftarrow \begin{cases}
    \frac{1}{|\mathcal{I}</em>|} \sum_{i \in \mathcal{I}<em oob="OOB">{OOB}} \mathbb{1}(\hat{y}_i^{\text{OOB}} \neq y_i)  \quad \text{(Clasificación)} \
    \frac{1}{|\mathcal{I}</em>|} \sum_{i \in \mathcal{I}_{OOB}} (y_i - \hat{y}_i^{\text{OOB}})^2 \quad \text{(Regresión)}
\end{cases}</p>
<p>\end{align*}
$$</p>
<p>De esta forma podemos estimar el error de generalización sin necesitar un conjunto de validación separado y sin suponer un coste computacional adicional</p>
<h3 id="analisis-del-metodo">Análisis del método<a class="headerlink" href="#analisis-del-metodo" title="Permanent link">¶</a></h3>
<p>Vamos a analizar a continuación los motivos por los que la técnica de <em>Bagging</em> ayuda a reducir la varianza. Vamos a empezar haciendo un estudio a nivel teórico.</p>
<p>Supongamos que tenemos B modelos independientes (sin correlación entre ellos) con varianza <span class="arithmatex">\(\sigma^2\)</span> cada uno. Si los promediamos:</p>
<div class="arithmatex">\[\text{Var}(\text{promedio}) = \text{Var}\left(\frac{1}{B}\sum_{i=1}^B h_i\right) = \frac{1}{B^2} \sum_{i=1}^B \text{Var}(h_i) = \frac{\sigma^2}{B}\]</div>
<p>La varianza se reduce por un factor de B. Sin embargo, en la práctica los modelos no son completamente independientes, ya que están entrenados con muestras correlacionadas, pero aún así la reducción de varianza es significativa. Si consideramos que tenemos una correlación <span class="arithmatex">\(\rho\)</span> entre modelos, entonces tendríamos:</p>
<div class="arithmatex">\[\text{Var}(\text{ensemble}) = \rho\sigma^2 + \frac{1-\rho}{B}\sigma^2\]</div>
<p>Si los modelos predicen exactamente lo mismo entonces tendremos correlación <span class="arithmatex">\(\rho=1\)</span>, y no obtendremos ninguna ganancia con el <em>ensemble</em>. Sin embargo, conforme consigamos reducir la correlación podremos reducir la varianza del modelo. Vemos que el primer término de la función anterior solo se puede reducir reduciendo la correlación, mientras que el segundo término se podría reducir aumentando el número de modelos. El caso ideal teórico sería conseguir <span class="arithmatex">\(\rho=0\)</span>, pero es difícil conseguirlo en la práctica. Lo principal a tener en cuenta es que a mayor correlación, la ganancia será menor, y por ello es importante la <strong>diversidad</strong> de los modelos.</p>
<h3 id="implementacion-de-bagging-generico">Implementación de Bagging Genérico<a class="headerlink" href="#implementacion-de-bagging-generico" title="Permanent link">¶</a></h3>
<p>La librería <em>sklearn</em> incluye las clase <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html"><code>BaggingClassifier</code></a> y <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html"><code>BaggingRegressor</code></a> que nos permiten utilizar de forma genérica está técnica, tanto para problemas de clasificación como de regresión.</p>
<p>Deberemos proporcionar el modelo base que queremos entrenar en el parámetro <code>estimator</code>. Por defecto utilizará como clasificador base Árboles de Decisión. Podemos también indicar la cantidad <span class="arithmatex">\(B\)</span> de clasificadores a entrenar en el parámetro <code>num_estimators</code>. </p>
<p>Es interesante observar que esta implementación nos da gran flexibilidad para la implementación del método. Por ejemplo, con <code>max_samples</code> podemos indicar el número de ejemplos que tendrá cada muestra generada (por defecto tendrá <span class="arithmatex">\(N\)</span> ejemplos, tantos como el conjunto de entrada original), y también podemos indicar con el parámetro <code>bootstrap</code> si queremos que muestree con reemplazo o sin reemplazo. Es recomendable que este parámetro tenga siempre valor <code>True</code>, que es la opción por defecto. También tenemos el parámetro <code>oob_score</code> que nos permite indicar si queremos utilizar los ejemplos <em>out-of-bag</em> para estimar el error de generalización (esto solo es posible si se utiliza muestre con reemplazo).</p>
<p>Además, no solo nos permite muestrear con reemplazo los ejemplos de entrada, sino que también nos permite hacer lo mismo con las <em>features</em> (esto como veremos a continuación es algo que incorporan los <em>Randon Forest</em>). Con <code>max_features</code> y <code>bootstrap_feature</code> podemos indicar el número de <em>features</em> que queremos seleccionar en cada muestra y si queremos que se puedan seleccionar con reemplazo, respectivamente. En este caso por defecto está establecido que no se realice muestreo con reemplazo de las <em>features</em>. </p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="c1"># Bagging con árboles de decisión profundos</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span class="n">bagging_tree</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a>    <span class="n">estimator</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>  <span class="c1"># Árbol sin restricciones</span>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>           <span class="c1"># Número de modelos</span>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a>    <span class="n">max_samples</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>            <span class="c1"># Usar 100% de muestras (por defecto)</span>
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a>    <span class="n">max_features</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>           <span class="c1"># Usar 100% de features (por defecto)</span>
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a>    <span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>             <span class="c1"># Con reemplazo (por defecto)</span>
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a>    <span class="n">oob_score</span><span class="o">=</span><span class="kc">True</span>              <span class="c1"># Calcular OOB error</span>
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a>
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a><span class="p">)</span>
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#__codelineno-6-11"></a>
<a id="__codelineno-6-12" name="__codelineno-6-12" href="#__codelineno-6-12"></a><span class="n">bagging_tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-6-13" name="__codelineno-6-13" href="#__codelineno-6-13"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"OOB Score: </span><span class="si">{</span><span class="n">bagging_tree</span><span class="o">.</span><span class="n">oob_score_</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<h3 id="consideraciones-finales-sobre-bagging">Consideraciones finales sobre <em>Bagging</em><a class="headerlink" href="#consideraciones-finales-sobre-bagging" title="Permanent link">¶</a></h3>
<p>Las técnicas de <em>Bagging</em> están enfocadas a la <strong>reducción de la varianza</strong>, y por lo tanto son muy efectivas con modelos con alta varianza, como es el caso de los Árboles de Decisión profundos. El <em>ensemble</em> es más <strong>robusto</strong> frente a <em>outliers</em> que un modelo individual.</p>
<p>Es <strong>paralelizable</strong>, ya que todos los modelos se pueden entrenar independientemente, y resulta <strong>sencillo</strong> de implementar y entender. Además, nos permite validar sin coste adicional mediante la evaluación <strong>OOB</strong>. </p>
<p>Sin embargo, entre sus limitaciones encontramos que <strong>no reduce sesgo</strong>. Además, es <strong>menos interpretable</strong> que un modelo invidual y computacionalmente más cosoto. </p>
<p>Será interesante utilizar <em>Bagging</em> con modelos con <strong>alta varianza</strong>, si queremos <strong>reducir el <em>overfitting</em></strong> y si el modelo base es rápido de entrenar. Por estos motivos, podemos encontrar un mayor beneficio al aplicar <em>Bagging</em> con modelos como Árboles de Decisión sin poda o KNN con <span class="arithmatex">\(K\)</span> pequeño, o en general cualquier modelo con alta varianza y bajo sesgo. Sin embargo, otros modelos como Regresión Logística que ya tienen baja varianza obtendrán normalmente poco beneficio.</p>
<p>Centrándonos en el caso de <em>Bagging</em> con Árboles de Decisión, como hemos comentado, <em>Bagging</em> reduce efectivamente la varianza al promediar múltiples árboles entrenados con diferentes muestras <em>bootstrap</em>. Sin embargo, los árboles resultantes tienden a ser similares entre sí. Dado que todos consideran las mismas características en cada división, suelen seleccionar las variables más informativas en los nodos superiores, generando estructuras correlacionadas. Esta correlación limita la reducción de varianza que puede conseguir el <em>ensemble</em>. Random Forest introduce una modificación sencilla pero efectiva: en cada división del árbol, en lugar de considerar todas las características disponibles, selecciona un subconjunto aleatorio de ellas. Esta aleatoriedad adicional reduce sustancialmente la correlación entre árboles, permitiendo una mayor reducción de varianza y mejorando significativamente el rendimiento del <em>ensemble</em>.</p>
<h2 id="random-forest">Random Forest<a class="headerlink" href="#random-forest" title="Permanent link">¶</a></h2>
<p>Random Forest (Breiman, 2001)<sup id="fnref:breiman2001random"><a class="footnote-ref" href="#fn:breiman2001random">5</a></sup> es el método de <em>Bagging</em> más popular, específicamente diseñado y optimizado para árboles de decisión. La innovación fundamental que introducen es combinar <em>Bagging</em> con selección aleatoria de características. </p>
<p>Para aumentar la diversidad entre los árboles, Random Forest introduce <strong>dos fuentes de aletoriedad</strong>:</p>
<ol>
<li>
<p><strong>Bootstrap sampling (como Bagging estándar):</strong>: Cada árbol se entrena en una muestra <em>bootstrap</em> diferente, con lo que aproximadamente el 37% de ejemplos quedan <em>out-of-bag</em> en cada árbol.</p>
</li>
<li>
<p><strong>Random feature selection:</strong>: En el <em>split</em> de cada nodo solo se considera un subconjunto aleatorio de <span class="arithmatex">\(m\)</span> <em>features</em>, ayudando a reducir la correlación entre árboles.</p>
</li>
</ol>
<h3 id="algoritmo-de-random-forest">Algoritmo de Random Forest<a class="headerlink" href="#algoritmo-de-random-forest" title="Permanent link">¶</a></h3>
<p>Detallamos a continuación el algoritmo para el <strong>entrenamiento</strong> de un modelo de tipo <em>Random Forest</em>:</p>
<div class="arithmatex">\[
\begin{align*}
&amp; \text{Entrada: Dataset } \mathcal{D} \text{, número de árboles } B \text{, número de feature } m \\
&amp; \text{Para } b=1 \text{ hasta } B:\\
&amp; \quad \mathcal{D}_b \leftarrow \text{Seleccionar, con reemplazo } N \text{ ejemplos del conjunto } \mathcal{D} \\
&amp; \quad T_b \leftarrow \text{Crear árbol} \\
&amp; \quad \text{Para cada nodo en } T_b: \\
&amp; \quad \quad \text{Seleccionar } m \text{ features al azar del total } d  \\
&amp; \quad \quad \text{Encontrar el mejor split utilizando solo esas features} \\
&amp; \quad \quad \text{Dividir el nodo con el mejor split} \\
&amp; \quad \quad \text{Hacer crecer el árbol hasta profundidad máxima (sin poda)} \\
&amp; \text{Devuelve: } \text{Random Forest} \{ T_1, T_2, \ldots, T_B \}
\end{align*}
\]</div>
<p>Podemos observar que la principal diferencia con <em>Bagging</em> puro de árboles es que con <em>Bagging</em> en cada <em>split</em> se estarían considerando siempre todas las <span class="arithmatex">\(d\)</span> <em>features</em>,  mientras que con <em>Random Forest</em> en cada <em>split</em> se considera un conjunto aleatorio de <em>features</em>, siendo <span class="arithmatex">\(m \ll d\)</span>.</p>
<h3 id="hiperparametros">Hiperparámetros<a class="headerlink" href="#hiperparametros" title="Permanent link">¶</a></h3>
<p>En sklearn contamos con las implementaciones <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">RandomForestClassifier</a> y <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html">RandomForestRegressor</a> de este modelo. A continuación enumeramos los principales hiperparámetros a tener en cuenta:</p>
<ul>
<li>
<p><strong><code>n_estimators</code></strong> (número de árboles <span class="arithmatex">\(B\)</span>): Obtendremos mayor rendimiento cuantos más árboles utilicemos (hasta la convergencia), sin llegar a causar <em>overfitting</em>, pero cuanto mayor sea este número mayor coste computacional tendremos. Típicamente toma valores entre <span class="arithmatex">\(100\)</span> (por defecto) y <span class="arithmatex">\(500\)</span>.</p>
</li>
<li>
<p><strong><code>max_features</code></strong> (<em>features</em> por split <span class="arithmatex">\(m\)</span>): El valor por defecto cambia según estemos en un problema de clasificación (<span class="arithmatex">\(m = \sqrt{d}\)</span>) o de regresión (<span class="arithmatex">\(m=d\)</span>). Valores menores de este parámetro reducirán la correlación, pero pueden aumentar el sesgo.</p>
</li>
<li>
<p><strong><code>bootstrap</code></strong> (usar <em>bootstrap sampling</em>): Por defecto toma valor <code>True</code>. En caso de cambiar a <code>False</code> utilizaría siempre el <em>dataset</em> completo.</p>
</li>
<li>
<p><strong><code>oob_score</code></strong> (calcular error OOB): Por defecto es <code>False</code>. El cálculo del error OOB solo está disponible si <code>bootstrap=True</code>. </p>
</li>
</ul>
<p>Los siguientes parámetros nos permiten ajustar cómo se construyen los árboles individuales:</p>
<ul>
<li>
<p><strong><code>max_depth</code></strong> (profundidad máxima de cada árbol). Por defecto es <code>None</code> (sin límite), con lo que los árboles crecen completamente. Sin introducimos valores menores tendremos menos <em>overfitting</em>, pero mayor sesgo.</p>
</li>
<li>
<p><strong><code>min_samples_split</code></strong> (mínimo de muestras para dividir un nodo): Por defecto toma el valor <span class="arithmatex">\(2\)</span>. Con valores mayores tendremos árboles más simples. </p>
</li>
<li>
<p><strong><code>min_samples_leaf</code></strong> (mínimo de muestras en una hoja). Por defecto toma el valor <span class="arithmatex">\(1\)</span>. Solo se dividirá un nodo si en cada una de las hojas resultantes hay al menos este número de muestras. Con valores más altos tendremos una regularización más fuerte que producirá modelos más suaves, especialmente en el caso de regresión.</p>
</li>
<li>
<p><strong><code>max_leaf_nodes</code></strong> (máximo número de hojas). Por defecto toma valor <code>None</code> (sin límite). Nos permite controlar la complejidad del árbol. </p>
</li>
</ul>
<h3 id="feature-importance"><em>Feature Importance</em><a class="headerlink" href="#feature-importance" title="Permanent link">¶</a></h3>
<p>Una vez entrenado el modelo de <em>Random Forest</em>, podemos calcular la importancia de cada característica. Esta información nos será de utilidad para:</p>
<ul>
<li>
<p><strong>Interpretabilidad del modelo</strong>: Nos permite identificar qué características son más relevantes para las predicciones. Esta información nos ayudará a explicar el comportamiento del modelo y validar que se estén utilizando características con sentido. </p>
</li>
<li>
<p><strong>Selección de características</strong>: Nos permite simplificar el modelo eliminando características no relevantes. Esto reducirá el coste computacional, puede mejorar la generalización, eliminando ruido, y facilitará el despliegue del modelo al requerir menos datos de entrada. </p>
</li>
</ul>
<p>Podemos calcular la importancia de cada <em>feature</em> de dos formas diferentes.</p>
<h4 id="gini-importance">Gini Importance<a class="headerlink" href="#gini-importance" title="Permanent link">¶</a></h4>
<p>Esta primera forma de medir la importancia, también conocida como <em>Mean Decrease in Impurity</em> (MDI) está basada en la mejora promedio de la impureza cuando se usa esa feature:</p>
<div class="arithmatex">\[\text{Importance}(x_j) = \frac{1}{B} \sum_{b=1}^{B} \sum_{t \in T_b} \Delta I_t \cdot \mathbb{1}(v_t = j)\]</div>
<p>Donde <span class="arithmatex">\(\Delta I_t\)</span> es la reducción en impureza en el nodo <span class="arithmatex">\(t\)</span> y <span class="arithmatex">\(v_t\)</span> es la <em>feature</em> usada en el <em>split</em> del nodo <span class="arithmatex">\(t\)</span>. Esta medida suma toda la reducción de impureza de los nodos en los que se utiliza <span class="arithmatex">\(j\)</span> como <em>feature</em> para dividir, y promedia sobre todos los árboles <span class="arithmatex">\(B\)</span>. </p>
<p>A continuación podemos ver cómo obtener este valor de impureza con sklearn:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="c1"># Calcular importancia</span>
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a><span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a><span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a>
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a><span class="n">importances</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">feature_importances_</span>
<a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a>
<a id="__codelineno-7-7" name="__codelineno-7-7" href="#__codelineno-7-7"></a><span class="c1"># Ordena por importancia y muestra un ranking de características</span>
<a id="__codelineno-7-8" name="__codelineno-7-8" href="#__codelineno-7-8"></a><span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<a id="__codelineno-7-9" name="__codelineno-7-9" href="#__codelineno-7-9"></a>
<a id="__codelineno-7-10" name="__codelineno-7-10" href="#__codelineno-7-10"></a><span class="nb">print</span><span class="p">(</span><span class="s2">"Ranking de caracteristicas:"</span><span class="p">)</span>
<a id="__codelineno-7-11" name="__codelineno-7-11" href="#__codelineno-7-11"></a><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
<a id="__codelineno-7-12" name="__codelineno-7-12" href="#__codelineno-7-12"></a>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">. Feature </span><span class="si">{</span><span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">importances</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<p>Esta forma de obtener la importancia es muy rápida de calcular, ya que se obtiene a partir de las medidas de impureza obtenidas a partir del entrenamiento. </p>
<p>Sin embargo, tiene un sesgo hacia características de alta cardinalidad, es decir, características con muchos valores únicos, como por ejemplo un ID que es único para cada ejemplo de entrada, ya que dividen artificialmente el espacio, pero no generalizan. </p>
<p>También es problemático con características correlacionadas, ya que la importancia se diluye entre ellas, y no se sabe cuál es realmente importante.</p>
<h4 id="permutation-importance">Permutation Importance<a class="headerlink" href="#permutation-importance" title="Permanent link">¶</a></h4>
<p>Esta segunda forma mide la degradación del rendimiento cuando se permutan los valores de una <em>feature</em>. </p>
<p>Para ello, primer evaluaremos en modelo con un <em>dataset</em> <span class="arithmatex">\(\mathcal{D}\)</span>, obteniendo el error <em>baseline</em>. Una vez hecho esto, en el conjunto <span class="arithmatex">\(\mathcal{D}\)</span> permutamos los valores de la <em>feature</em> (columna) <span class="arithmatex">\(j\)</span>, corrompiendo así los datos para dicha <em>feature</em>, y calculamos el error del modelo con la <em>feature</em> corrupta. Repetiremos la permutación varias veces y promediamos el error obtenido al corromper <span class="arithmatex">\(j\)</span>. Con esto, podemos calcular la importancia de permutación de <span class="arithmatex">\(j\)</span> calculando la diferencia entre el error <em>baseline</em> y el error promedio obtenido al corromper <span class="arithmatex">\(j\)</span>.</p>
<p>A continuación podemos ver la implementación en sklearn:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">permutation_importance</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a><span class="c1"># Calcular permutation importance</span>
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a><span class="n">result</span> <span class="o">=</span> <span class="n">permutation_importance</span><span class="p">(</span><span class="n">rf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a>
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a><span class="n">importances</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">importances_mean</span>
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#__codelineno-8-7"></a>
<a id="__codelineno-8-8" name="__codelineno-8-8" href="#__codelineno-8-8"></a><span class="c1"># Mostramos ranking de caracteristicas</span>
<a id="__codelineno-8-9" name="__codelineno-8-9" href="#__codelineno-8-9"></a><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
<a id="__codelineno-8-10" name="__codelineno-8-10" href="#__codelineno-8-10"></a>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Feature </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">importances</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> +/- </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">importances_std</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<p>Este tipo de cálculo de la importancia es más fiable cuando contamos con características correlacionadas, y no existe sesgo por la cardinalidad, pero tiene un mayor coste computacional y requiere un conjunto de <em>test</em> o validación.</p>
<h3 id="out-of-bag-oob-error-en-random-forest">Out-of-Bag (OOB) Error en Random Forest<a class="headerlink" href="#out-of-bag-oob-error-en-random-forest" title="Permanent link">¶</a></h3>
<p>Como Random Forest usa <em>bootstrap</em>, hereda la estimación del error OOB. A continuación podemos ver cómo obtener este valor con sklearn:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span>
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a>    <span class="n">oob_score</span><span class="o">=</span><span class="kc">True</span>  <span class="c1"># Activar OOB</span>
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#__codelineno-9-4"></a><span class="p">)</span>
<a id="__codelineno-9-5" name="__codelineno-9-5" href="#__codelineno-9-5"></a>
<a id="__codelineno-9-6" name="__codelineno-9-6" href="#__codelineno-9-6"></a><span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-9-7" name="__codelineno-9-7" href="#__codelineno-9-7"></a>
<a id="__codelineno-9-8" name="__codelineno-9-8" href="#__codelineno-9-8"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"OOB Score: </span><span class="si">{</span><span class="n">rf</span><span class="o">.</span><span class="n">oob_score_</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-9-9" name="__codelineno-9-9" href="#__codelineno-9-9"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Test Score: </span><span class="si">{</span><span class="n">rf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<p>El <em>OOB score</em> es típicamente una buena aproximación del error de generalización.</p>
<h3 id="extra-trees-extremely-randomized-trees">Extra Trees (Extremely Randomized Trees)<a class="headerlink" href="#extra-trees-extremely-randomized-trees" title="Permanent link">¶</a></h3>
<p>Los <em>Extra Trees</em> son una variante de <em>Random Forest</em> en la que se introduce aún más aleatoriedad. Si bien en <em>Random Forest</em> en cada nodo de los árboles se elige el mejor <em>split</em> entre las <em>features</em> seleccionadas, en <em>Extra Trees</em> se elige un <em>split</em> de forma aleatoria para cada <em>feature</em>, y nos quedamos con aquella que proporciona una mayor ganancia.   </p>
<p>En la siguiente tabla se resumen las principales diferencias estre estos modelos:</p>
<table>
<thead>
<tr>
<th>Aspecto</th>
<th>Random Forest</th>
<th>Extra Trees</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Sampling</strong></td>
<td><em>Bootstrap</em> (por defecto con reemplazo)</td>
<td>Todo el <em>dataset</em> (por defecto sin <em>bootstrap</em>)</td>
</tr>
<tr>
<td><strong>Splits</strong></td>
<td>Mejor <em>split</em> entre <span class="arithmatex">\(m\)</span> features</td>
<td><em>Split</em> <strong>aleatorio</strong> entre <span class="arithmatex">\(m\)</span> features</td>
</tr>
<tr>
<td><strong>Varianza</strong></td>
<td>Bajo</td>
<td>Aún menor</td>
</tr>
<tr>
<td><strong>Sesgo</strong></td>
<td>Bajo</td>
<td>Ligeramente mayor</td>
</tr>
<tr>
<td><strong>Velocidad</strong></td>
<td>Más lento</td>
<td>Más rápido (<em>splits</em> aleatorios)</td>
</tr>
</tbody>
</table>
<p>En sklearn tenemos las clase <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html">ExtraTreesClassifier</a> y <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html">ExtraTreesRegressor</a>. A continuación vemos un ejemplo de implementación:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesClassifier</span>
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a>
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a><span class="c1"># Extra Trees</span>
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a><span class="n">et</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">(</span>
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a>    <span class="n">max_features</span><span class="o">=</span><span class="s1">'sqrt'</span><span class="p">,</span>
<a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a>    <span class="n">bootstrap</span><span class="o">=</span><span class="kc">False</span>  <span class="c1"># No usa bootstrap (default)</span>
<a id="__codelineno-10-8" name="__codelineno-10-8" href="#__codelineno-10-8"></a><span class="p">)</span>
<a id="__codelineno-10-9" name="__codelineno-10-9" href="#__codelineno-10-9"></a>
<a id="__codelineno-10-10" name="__codelineno-10-10" href="#__codelineno-10-10"></a><span class="n">et</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<p>Será conveniente utilizar <em>Extra Trees</em> cuando busquemos reducir aún más la varianza, aunque sea con un ligero aumento del sesgo, y cuando necesitemos una mayor velocidad de entrenamiento. </p>
<h3 id="consideraciones-finales">Consideraciones finales<a class="headerlink" href="#consideraciones-finales" title="Permanent link">¶</a></h3>
<p>Random Forest es <strong>uno de los mejores algoritmos <em>out-of-the-box</em></strong>. Es decir, es capaz de funcionar correctamente sin necesidad de configuración o ajustes adicionales, funcionando bien con los hiperparámetros por defecto.</p>
<p>Es un método <strong>robusto</strong>, que puede manejar bien datos con ruido, <em>outliers</em> y <em>features</em> irrelevantes. Puede manejar además valores faltantes (<em>missing values</em>). Mediante la evaluación OOB podemos validar el modelo sin requerir un conjunto de validación por separado. Una limitación que encontramos es que el modelo no extrapola, es decir, no puede predecir fuera del rango de entrenamiento.</p>
<p>Es también <strong>versátil</strong>, dando buenos resultados tanto en problemas de clasificación como de regresión. Permite también tener <em>features</em> mixtas, numéricas y categóricas, y permite capturar relaciones complejas (no lineales) de los datos de entrada. También es invariante a la escala de las <em>features</em>, por lo que no necesita normalización. Sin embargo, con <em>datasets</em> pequeños puede haber algo de <em>overfitting</em>, y puede tener un peor rendimiento cuando tenemos <em>features</em> muy correlacionadas.</p>
<p>En cuanto a la <strong>interpretabilidad</strong>, tenemos la posibilidad de obtener una medida de la importancia de características, pudiendo destacar las más relevantes. Sin embargo, es menos interpretable que un árbol individual.</p>
<p>Respecto al <strong>coste</strong>, el entrenamiento es muy rápido y es paralelizable, pero cuando tenemos muchos árboles la predicción puede ser más lenta. Además, tiene un alto coste espacial en memoria, ya que debe almacenar muchos árboles profundos. El tamaño de los modelos puede ser grande. </p>
<p>En resumen, <em>Random Forest</em> es robusto y versátil, siendo el método de <em>Bagging</em> más popular y uno de los algoritmos más importantes en <em>Machine Learning</em>. Combina:
- <em>Bootstrap sampling</em> (como <em>Bagging</em>)
- <em>Random feature selection</em>
- Árboles profundos sin poda
- Promediado/votación</p>
<p>Normalmente <em>Random Forest</em> funcionará mejor que <em>Bagging</em> porque existe menos correlación entre los árboles y esta mayor diversidad produce una reducción de la varianza. Con <em>Random Forest</em> podemos obtener de forma rápida un <em>baseline</em> robusto para tratar datos tabulares. </p>
<p>Sin embargo, en algunos casos deberíamos considerar otros modelos. Si necesitamos mayor rendimiento podríamos considerar métodos como XGBoost o LightGBM. Si buscamos una alta interpretabilidad será más adecuado utilizar árboles individuales o modelos lineales. Si tenemos datos de tipo imagen o texto, será más adecuado utilizar redes neuronales profundas. En caso de tener <em>datasets</em> pequeños, con solo decenas o unos pocos cientos de ejemplos, deberíamos considerar modelos más simples. </p>
<p>Hasta ahora hemos visto métodos que combinan modelos entrenados <strong>independientemente</strong> (en paralelo), ya sea en los mismos datos (<em>Voting</em>, <em>Stacking</em>) o en diferentes muestras (<em>Bagging</em>). La siguiente pregunta es: <strong>¿y si los modelos se entrenan secuencialmente, aprendiendo cada uno de los errores del anterior?</strong>. En esto se basarán los métodos de <strong>Boosting</strong>.</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:sagi2018ensemble">
<p>Sagi, O., &amp; Rokach, L. (2018). Ensemble learning: A survey. <em>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</em>, <em>8</em>(4), e1249.&nbsp;<a class="footnote-backref" href="#fnref:sagi2018ensemble" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn:kittler1998combining">
<p>Kittler, J., Hatef, M., Duin, R. P., &amp; Matas, J. (1998). On combining classifiers. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <em>20</em>(3), 226--239.&nbsp;<a class="footnote-backref" href="#fnref:kittler1998combining" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
<li id="fn:wolpert1992stacked">
<p>Wolpert, D. H. (1992). Stacked generalization. <em>Neural Networks</em>, <em>5</em>(2), 241--259.&nbsp;<a class="footnote-backref" href="#fnref:wolpert1992stacked" title="Jump back to footnote 3 in the text">↩</a></p>
</li>
<li id="fn:breiman1996bagging">
<p>Breiman, L. (1996). Bagging predictors. <em>Machine Learning</em>, <em>24</em>(2), 123--140.&nbsp;<a class="footnote-backref" href="#fnref:breiman1996bagging" title="Jump back to footnote 4 in the text">↩</a></p>
</li>
<li id="fn:breiman2001random">
<p>Breiman, L. (2001). Random forests. <em>Machine Learning</em>, <em>45</em>(1), 5--32.&nbsp;<a class="footnote-backref" href="#fnref:breiman2001random" title="Jump back to footnote 5 in the text">↩</a></p>
</li>
</ol>
</div>





                
              </article>
            </div>
          
          
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"></path></svg>
            Volver al principio
          </a>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.sections", "navigation.expand", "navigation.top", "navigation.indexes", "toc.follow", "toc.integrate", "search.suggest", "search.highlight", "content.code.copy", "content.code.annotate"], "search": "../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copiado al portapapeles", "clipboard.copy": "Copiar al portapapeles", "search.result.more.one": "1 m\u00e1s en esta p\u00e1gina", "search.result.more.other": "# m\u00e1s en esta p\u00e1gina", "search.result.none": "No se encontraron documentos", "search.result.one": "1 documento encontrado", "search.result.other": "# documentos encontrados", "search.result.placeholder": "Teclee para comenzar b\u00fasqueda", "search.result.term.missing": "Falta", "select.version": "Seleccionar versi\u00f3n"}}</script>
    
    
      <script src="../assets/javascripts/bundle.b78d2936.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>