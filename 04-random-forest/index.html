<!DOCTYPE html><html lang="es" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Apuntes de la asignatura Aprendizaje Avanzado del Grado en Ingeniería en Inteligencia Artificial de la Universidad de Alicante">
      
      
        <meta name="author" content="Miguel Angel Lozano">
      
      
        <link rel="canonical" href="https://malozano.github.io/aprendizaje-avanzado/04-random-forest/">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.1.1">
    
    
      
        <title>Métodos de ensemble - Aprendizaje Avanzado</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.402914a4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.a0c5b2b5.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#metodos-de-ensemble" class="md-skip">
          Saltar a contenido
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Cabecera">
    <a href=".." title="Aprendizaje Avanzado" class="md-header__button md-logo" aria-label="Aprendizaje Avanzado" data-md-component="logo">
      
  <img src="../images/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Aprendizaje Avanzado
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Métodos de ensemble
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo" aria-label="Modo oscuro" type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Modo oscuro" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
            </label>
          
        
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="white" data-md-color-accent="blue" aria-label="Modo claro" type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Modo claro" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Búsqueda" placeholder="Búsqueda" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Buscar">
        
        <button type="reset" class="md-search__icon md-icon" title="Limpiar" aria-label="Limpiar" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Inicializando búsqueda
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navegación" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Aprendizaje Avanzado" class="md-nav__button md-logo" aria-label="Aprendizaje Avanzado" data-md-component="logo">
      
  <img src="../images/logo.png" alt="logo">

    </a>
    Aprendizaje Avanzado
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Introducción
      </a>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2">
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Bloque I. Aprendizaje supervisado
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Bloque I. Aprendizaje supervisado
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01-modelos-no-param/" class="md-nav__link">
        1. Modelos paramétricos y no paramétricos
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02-svm/" class="md-nav__link">
        2. SVM
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03-arboles-decision/" class="md-nav__link">
        3. Árboles de decisión
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3">
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          Prácticas
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Prácticas
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../practicas/00-datos-y-visualizacion/" class="md-nav__link">
        0. Datos y visualización
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="metodos-de-ensemble">Métodos de ensemble<a class="headerlink" href="#metodos-de-ensemble" title="Permanent link">¶</a></h1>
<p>La idea tras los modelos de <em>ensemble</em> consiste en combinar diferentes modelos base sencillos para construir un modelo más robusto y preciso que cualquier de los modelos individuales. </p>
<p>Hemos visto que modelos diferentes cometen errores diferentes. Buscamos combinarlos de forma que podamos eliminar o reducir esos errores individuales. Para ello necesitaremos combinar un conjunto diverso de modelos, y cada uno de estos modelos individuales debería proporcionar por si mismo una precisión que sea superior al azar.</p>
<p>El aprendizaje con métodos de <em>ensemble</em> puede descomponerse en dos tareas principales:</p>
<ul>
<li>Aprender un conjunto de modelos base a partir de los datos de entrenamiento.</li>
<li>Combinarlos para construir el predictor conjunto.</li>
</ul>
<p>Una de las principales ventajas de los <em>ensembles</em> es que pueden mejorar el compromiso entre <strong>sesgo y varianza</strong>. Recordemos que el error esperado se puede descomponer como: </p>
<div class="arithmatex">\[E[(y - \hat{y})^2] = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}\]</div>
<p>Existen cuatro principales tipos de enfoques para abordar la combinación de modelos: <em>Voting</em>, <em>Stacking</em>, <em>Bagging</em> y <em>Boosting</em>. Diferentes enfoques abordarán el problema de forma distinta. Algunos tipos de métodos de <em>ensemble</em> se centran en reducir principalmente la varianza, como por ejemplo los métodos de <em>Bagging</em>, mientras que otros se centran fundamentalmente en el sesgo, como sería el caso del <em>Boosting</em>. </p>
<p>Cada categoría de métodos de <em>ensemble</em> tiene un enfoque diferente para generar y combinar modelos, tal como se resume en la siguiente tabla:</p>
<table>
<thead>
<tr>
<th>Categoría</th>
<th>Modelos</th>
<th>Datos de entrenamiento</th>
<th>Combinación</th>
<th>Entrenamiento</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Voting</strong></td>
<td>Heterogéneos</td>
<td>Iguales</td>
<td>Fija (votos/promedio)</td>
<td>Independiente</td>
</tr>
<tr>
<td><strong>Stacking</strong></td>
<td>Heterogéneos</td>
<td>Iguales</td>
<td>Aprendida (meta-modelo)</td>
<td>Dos niveles</td>
</tr>
<tr>
<td><strong>Bagging</strong></td>
<td>Homogéneos</td>
<td>Bootstrap (diferentes)</td>
<td>Fija (promedio)</td>
<td>Independiente</td>
</tr>
<tr>
<td><strong>Boosting</strong></td>
<td>Homogéneos</td>
<td>Ponderados/residuos</td>
<td>Ponderada (aprendida)</td>
<td>Secuencial</td>
</tr>
</tbody>
</table>
<p>A continuación estudiaremos en detalle cada una de estas categorías, y los principales métodos que existen dentro de cada una de ellas.</p>
<h2 id="voting">Voting<a class="headerlink" href="#voting" title="Permanent link">¶</a></h2>
<p>La idea tras los modelos de clasificación es la de entrenar múltiples modelos independientes y combinar sus predicciones mediante votación (en el caso de clasifiación) o mediante promedidado (en el caso de regresión). </p>
<p>En este caso los modelos se entrenan de forma independiente con el <strong>mismo conjunto de datos</strong>, y no hay dependencia entre modelos, por lo que pueden entrenarse en paralelo. Además, podemos <strong>combinar diferentes tipos de modelos</strong>. </p>
<p>Por ejemplo, podríamos combinar un modelo de Regresión Logística, con KNN y SVM, obtener la predicción que devuelve cada uno de ellos, y devolver aquella que obtenga más votos. </p>
<p>Encontramos diferentes formas de abordar la votación, que podremos aplicar según se trate de un problema de clasificación o de regresión.</p>
<p>Vamos a considerar que combinamos <span class="arithmatex">\(M\)</span> clasificadores <span class="arithmatex">\(\{h_1(\mathbf{x}), h_2(\mathbf{x}), \ldots, h_M(\mathbf{x}) \}\)</span>. A continuación veremos cómo combinar sus predicciones con cada enfoque de votación.</p>
<h3 id="hard-voting">Hard Voting<a class="headerlink" href="#hard-voting" title="Permanent link">¶</a></h3>
<p>Se trata de un enfoque dirigido al problema de clasificación. Con el enfoque <em>Hard Voting</em>, la clase predicha por el clasificador será la que reciba más votos por parte de los clasificadores individuales (es decir, la moda del conjunto de predicciones). </p>
<div class="arithmatex">\[\hat{y} = \text{mode}(h_1(\mathbf{x}), h_2(\mathbf{x}), ..., h_M(\mathbf{x}))\]</div>
<p><strong>Ejemplo:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>Problema: Clasificar un email como spam o no-spam
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>Modelo 1 (Logistic Regression): spam
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>Modelo 2 (Decision Tree):       spam  
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>Modelo 3 (SVM):                 no-spam
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>Modelo 4 (KNN):                 spam
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>Resultado final: spam (3 votos contra 1)
</code></pre></div><p></p>
<h3 id="soft-voting">Soft Voting<a class="headerlink" href="#soft-voting" title="Permanent link">¶</a></h3>
<p>A diferencia del caso anterior, con el enfoque <em>Soft Voting</em> lo que tendremos en cuenta es la suma de probabilidades de predicción de cada clasificador individual. Aquella clase <span class="arithmatex">\(k\)</span> cuya suma de probabilidades de predicción sea mayor, será la seleccionada como predicción del modelo combinado:</p>
<div class="arithmatex">\[\hat{y} = \arg\max_k \frac{1}{M} \sum_{i=1}^{M} P_i(y = k | \mathbf{x})\]</div>
<p><strong>Ejemplo:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>Modelo 1: P(spam) = 0.9, P(no-spam) = 0.1
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>Modelo 2: P(spam) = 0.6, P(no-spam) = 0.4
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>Modelo 3: P(spam) = 0.4, P(no-spam) = 0.6
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>Promedio: P(spam) = 0.633, P(no-spam) = 0.367
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>Resultado: spam
</code></pre></div><p></p>
<p>Es importante destacar que para poder utilizar este enfoque, los modelos individuales deben poder proporcionarnos la probabilidad de la predicción. Este enfoque tiene la ventaja de que considera la confianza de cada modelo en la predicción realizada.</p>
<h3 id="soft-voting-ponderado">Soft Voting ponderado<a class="headerlink" href="#soft-voting-ponderado" title="Permanent link">¶</a></h3>
<p>Se trata de un caso similar al anterior, pero dando un peso diferente a cada predictor individual en la suma:</p>
<div class="arithmatex">\[\hat{y} = \arg\max_k \sum_{i=1}^{M} w_i \cdot P_i(y = k | \mathbf{x})\]</div>
<p>Donde <span class="arithmatex">\(\sum w_i = 1\)</span> y <span class="arithmatex">\(w_i\)</span> nos permite dar mayor peso a los modelos en los que tengamos mayor confianza. Estos valores se pueden determinar a partir de la precisión de los modelos individuales en la validación, de métricas como F1-score o el inverso del error cometido.</p>
<p><strong>Ejemplo:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>Modelo 1 (accuracy=0.85): w1 = 0.4
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>Modelo 2 (accuracy=0.80): w2 = 0.35  
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>Modelo 3 (accuracy=0.75): w3 = 0.25
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>La votación ponderada da más peso al mejor modelo
</code></pre></div><p></p>
<h3 id="promediado">Promediado<a class="headerlink" href="#promediado" title="Permanent link">¶</a></h3>
<p>Este enfoque, a diferencia de los anteriores, está dirigido al problema de regresión. En caso de regresión las predicciones de promedian de la siguiente forma:</p>
<div class="arithmatex">\[\hat{y} = \frac{1}{M} \sum_{i=1}^{M} h_i(\mathbf{x})\]</div>
<p>También es posible asignar un peso a cada predictor, igual que en el caso del enfoque anterior:</p>
<div class="arithmatex">\[\hat{y} = \sum_{i=1}^{M} w_i  h_i(\mathbf{x})\]</div>
<h3 id="implementacion">Implementación<a class="headerlink" href="#implementacion" title="Permanent link">¶</a></h3>
<p>En sklearn tenemos las clases <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier">VotingClassifier</a> y <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html#sklearn.ensemble.VotingRegressor">VotingRegressor</a>, para problemas de clasificación y regresión respectivamente.</p>
<p>Para utilizar estas clases deberemos pasar como parámetro <code>estimator</code> la lista de predictores individuales que queramos combinar, y nos permitirán aplicar cualquiera de los enfoques anteriores.</p>
<p>En el caso de la clasificación, podemos elegir el tipo de votación con el parámetro <code>voting</code>, que podrá tomar como valores <code>hard</code> o <code>soft</code>, y en el caso del segundo tipo con <code>weights</code> podemos especificar pesos específicos para cada predictor.</p>
<p>En el caso de la regresión, también contamos con un parámetro <code>weights</code> para especificar los pesos de cada predicción.</p>
<h3 id="ventajas-y-limitaciones-de-voting">Ventajas y limitaciones de <em>Voting</em><a class="headerlink" href="#ventajas-y-limitaciones-de-voting" title="Permanent link">¶</a></h3>
<p>Se trata de un método muy fácil de entender y sencillo de implementar, que se basa en modelos existentes que no es necesario modificar. Todos los modelos base se entrenan independientemente y podría hacerse en paralelo. </p>
<p>Debemos tener en cuenta que para que sea efectivo los diferentes modelos base deben ser diversos, ya que si todos son similares no obtendremos apenas ganancia combinándolos. Funcionará mejor cuando los modelos base capturen aspectos distintos de los datos y cometan errores diferentes. </p>
<p>Este método también ser verá beneficiado cuando los modelos tengan un rendimiento similar. Si uno de los modelos base fuera muy superior, sería recomendable utilizar únicamente dicho modelo, mientras que si tenemos un modelo muy malo, ese modelo perjudicará al <em>ensemble</em> y convendría eliminarlo.</p>
<p>Voting es la forma más básica de modelo de <em>ensemble</em>, en la que se utiliza una combinación fija de modelos (votos o promedios con pesos predefinidos). Esto nos lleva a preguntarnos si podríamos aprender la mejor forma de combinar los modelos. Esto lo abordaremos con los métodos de <em>Stacking</em> que veremos a continuación.</p>
<h2 id="stacking">Stacking<a class="headerlink" href="#stacking" title="Permanent link">¶</a></h2>
<p>En el caso de <em>Stacking</em>, al igual que en <em>Voting</em> tenemos una serie de modelos base heterogéneos que entrenamos de forma independiente, pero a diferencia del caso anterior, no tendremos una combinación fija, sino que utilizaremos un <strong>meta-modelo</strong> para aprender la forma de combinar los diferentes modelos base. </p>
<p>De esta forma, se podrán capturar relaciones complejas entre las diferentes predicciones. Tendremos una arquitectura en dos niveles, en la que en el nivel inferior tendremos los <span class="arithmatex">\(M\)</span> diferentes modelos base, cada uno de los cuales producirá una predicción <span class="arithmatex">\(p_i\)</span> y en el nivel superior tendremos el meta-modelo, que recibirá como entrada las diferentes predicciones <span class="arithmatex">\(\{ p_1, p_2, \ldots, p_M \}\)</span> y producirá como salida la predicción del <em>ensemble</em>.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>NIVEL 0 (Modelos base):
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>├─ Modelo 1 (ej: DT)               → predicción p1
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>├─ Modelo 2 (ej: SVM)              → predicción p2
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>├─ Modelo 3 (ej: Logistic Reg)     → predicción p3
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>└─ Modelo 4 (ej: KNN)              → predicción p4
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>NIVEL 1 (Meta-modelo):
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a>└─ Recibe [p1, p2, p3, p4] como features
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a>   → aprende la mejor combinación
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a>   → predicción final
</code></pre></div>
<p>Para reducir el <em>overfitting</em>, el meta-modelo será entrenado mediante <em>cross-validation</em>. </p>
<h2 id="bagging">Bagging<a class="headerlink" href="#bagging" title="Permanent link">¶</a></h2>
<h2 id="boosting">Boosting<a class="headerlink" href="#boosting" title="Permanent link">¶</a></h2>
<h2 id="random-forests">Random Forests<a class="headerlink" href="#random-forests" title="Permanent link">¶</a></h2>
<h3 id="algoritmo-basico">Algoritmo Básico<a class="headerlink" href="#algoritmo-basico" title="Permanent link">¶</a></h3>
<p><strong>Paso 1: Entrenar modelos base</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>Para cada modelo base mⱼ (j = 1, ..., M):
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>    Entrenar mⱼ en el conjunto de entrenamiento D
</code></pre></div><p></p>
<p><strong>Paso 2: Generar predicciones para el meta-modelo</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>Para evitar overfitting, usar cross-validation:
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>Para cada fold k:
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a>    1. Entrenar modelos base en train_k
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>    2. Predecir en validation_k
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a>    3. Almacenar predicciones
</code></pre></div><p></p>
<p><strong>Paso 3: Entrenar meta-modelo</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a>Usar las predicciones de nivel 0 como features:
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a>X_meta = [predicciones de modelo 1, modelo 2, ..., modelo M]
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a>y_meta = etiquetas originales
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a>Entrenar meta-modelo en (X_meta, y_meta)
</code></pre></div><p></p>
<p><strong>Predicción en nuevos datos:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a>1. Cada modelo base predice en x_nuevo
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a>2. Meta-modelo recibe estas predicciones
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a>3. Meta-modelo genera predicción final
</code></pre></div><p></p>
<h3 id="stacking-vs-blending">Stacking vs Blending<a class="headerlink" href="#stacking-vs-blending" title="Permanent link">¶</a></h3>
<p><strong>Stacking (con K-fold CV):</strong>
- Usa cross-validation para generar predicciones de nivel 0
- Aprovecha todos los datos de entrenamiento
- Más robusto pero más costoso computacionalmente
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_predict</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a><span class="c1"># Generar predicciones out-of-fold</span>
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a><span class="n">meta_features</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a><span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">base_models</span><span class="p">:</span>
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a>    <span class="n">predictions</span> <span class="o">=</span> <span class="n">cross_val_predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#__codelineno-8-7"></a>    <span class="n">meta_features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</code></pre></div><p></p>
<p><strong>Blending (con hold-out):</strong>
- Divide datos en train/validation de una sola vez
- Más simple y rápido
- Usa menos datos para entrenar modelos base
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="c1"># Split simple</span>
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a><span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a>
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#__codelineno-9-4"></a><span class="c1"># Entrenar en train, predecir en validation</span>
<a id="__codelineno-9-5" name="__codelineno-9-5" href="#__codelineno-9-5"></a><span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">base_models</span><span class="p">:</span>
<a id="__codelineno-9-6" name="__codelineno-9-6" href="#__codelineno-9-6"></a>    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-9-7" name="__codelineno-9-7" href="#__codelineno-9-7"></a>    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<a id="__codelineno-9-8" name="__codelineno-9-8" href="#__codelineno-9-8"></a>    <span class="n">meta_features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</code></pre></div><p></p>
<h3 id="implementacion-en-scikit-learn">Implementación en scikit-learn<a class="headerlink" href="#implementacion-en-scikit-learn" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">StackingClassifier</span><span class="p">,</span> <span class="n">StackingRegressor</span>
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">GradientBoostingClassifier</span>
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a>
<a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a><span class="c1"># Definir modelos base (nivel 0)</span>
<a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a><span class="n">base_models</span> <span class="o">=</span> <span class="p">[</span>
<a id="__codelineno-10-8" name="__codelineno-10-8" href="#__codelineno-10-8"></a>    <span class="p">(</span><span class="s1">'rf'</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)),</span>
<a id="__codelineno-10-9" name="__codelineno-10-9" href="#__codelineno-10-9"></a>    <span class="p">(</span><span class="s1">'gb'</span><span class="p">,</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)),</span>
<a id="__codelineno-10-10" name="__codelineno-10-10" href="#__codelineno-10-10"></a>    <span class="p">(</span><span class="s1">'svm'</span><span class="p">,</span> <span class="n">SVC</span><span class="p">(</span><span class="n">probability</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">))</span>
<a id="__codelineno-10-11" name="__codelineno-10-11" href="#__codelineno-10-11"></a><span class="p">]</span>
<a id="__codelineno-10-12" name="__codelineno-10-12" href="#__codelineno-10-12"></a>
<a id="__codelineno-10-13" name="__codelineno-10-13" href="#__codelineno-10-13"></a><span class="c1"># Definir meta-modelo (nivel 1)</span>
<a id="__codelineno-10-14" name="__codelineno-10-14" href="#__codelineno-10-14"></a><span class="n">meta_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<a id="__codelineno-10-15" name="__codelineno-10-15" href="#__codelineno-10-15"></a>
<a id="__codelineno-10-16" name="__codelineno-10-16" href="#__codelineno-10-16"></a><span class="c1"># Crear stacking ensemble</span>
<a id="__codelineno-10-17" name="__codelineno-10-17" href="#__codelineno-10-17"></a><span class="n">stacking</span> <span class="o">=</span> <span class="n">StackingClassifier</span><span class="p">(</span>
<a id="__codelineno-10-18" name="__codelineno-10-18" href="#__codelineno-10-18"></a>    <span class="n">estimators</span><span class="o">=</span><span class="n">base_models</span><span class="p">,</span>
<a id="__codelineno-10-19" name="__codelineno-10-19" href="#__codelineno-10-19"></a>    <span class="n">final_estimator</span><span class="o">=</span><span class="n">meta_model</span><span class="p">,</span>
<a id="__codelineno-10-20" name="__codelineno-10-20" href="#__codelineno-10-20"></a>    <span class="n">cv</span><span class="o">=</span><span class="mi">5</span>  <span class="c1"># Cross-validation para generar predicciones nivel 0</span>
<a id="__codelineno-10-21" name="__codelineno-10-21" href="#__codelineno-10-21"></a><span class="p">)</span>
<a id="__codelineno-10-22" name="__codelineno-10-22" href="#__codelineno-10-22"></a>
<a id="__codelineno-10-23" name="__codelineno-10-23" href="#__codelineno-10-23"></a><span class="c1"># Entrenar</span>
<a id="__codelineno-10-24" name="__codelineno-10-24" href="#__codelineno-10-24"></a><span class="n">stacking</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-10-25" name="__codelineno-10-25" href="#__codelineno-10-25"></a>
<a id="__codelineno-10-26" name="__codelineno-10-26" href="#__codelineno-10-26"></a><span class="c1"># Predecir</span>
<a id="__codelineno-10-27" name="__codelineno-10-27" href="#__codelineno-10-27"></a><span class="n">predictions</span> <span class="o">=</span> <span class="n">stacking</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div>
<h3 id="variantes-de-stacking">Variantes de Stacking<a class="headerlink" href="#variantes-de-stacking" title="Permanent link">¶</a></h3>
<h4 id="multi-level-stacking">Multi-level Stacking<a class="headerlink" href="#multi-level-stacking" title="Permanent link">¶</a></h4>
<p>Apilar múltiples niveles de meta-modelos:
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a>NIVEL 0: Modelos base diversos
<a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a>    ↓ (predicciones)
<a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a>NIVEL 1: Primer grupo de meta-modelos
<a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a>    ↓ (predicciones)
<a id="__codelineno-11-5" name="__codelineno-11-5" href="#__codelineno-11-5"></a>NIVEL 2: Meta-meta-modelo final
<a id="__codelineno-11-6" name="__codelineno-11-6" href="#__codelineno-11-6"></a>    ↓
<a id="__codelineno-11-7" name="__codelineno-11-7" href="#__codelineno-11-7"></a>Predicción final
</code></pre></div><p></p>
<p><strong>Ventajas:</strong>
- Puede capturar relaciones muy complejas
- Común en competiciones (ej: Kaggle)</p>
<p><strong>Desventajas:</strong>
- Alto riesgo de overfitting
- Muy complejo de implementar y mantener
- Difícil de interpretar</p>
<h4 id="stacking-con-diferentes-estrategias">Stacking con diferentes estrategias<a class="headerlink" href="#stacking-con-diferentes-estrategias" title="Permanent link">¶</a></h4>
<p><strong>Por tipo de meta-modelo:</strong>
- <strong>Linear Stacking:</strong> Meta-modelo = Regresión Lineal/Logística simple
- <strong>Tree Stacking:</strong> Meta-modelo = Árbol de decisión o Random Forest
- <strong>Neural Stacking:</strong> Meta-modelo = Red neuronal</p>
<p><strong>Por features del meta-modelo:</strong>
- <strong>Predicciones solo:</strong> Meta-features = solo predicciones de modelos base
- <strong>Predicciones + features originales:</strong> Meta-features = predicciones + X original
- <strong>Predicciones + probabilidades:</strong> Para clasificación, usar todas las probabilidades</p>
<h3 id="prevencion-de-overfitting-en-stacking">Prevención de Overfitting en Stacking<a class="headerlink" href="#prevencion-de-overfitting-en-stacking" title="Permanent link">¶</a></h3>
<p>Stacking es propenso al overfitting. Estrategias para mitigarlo:</p>
<ol>
<li>
<p><strong>Cross-validation para nivel 0:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a>   <span class="c1"># SIEMPRE usar CV para generar meta-features</span>
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a>   <span class="n">stacking</span> <span class="o">=</span> <span class="n">StackingClassifier</span><span class="p">(</span>
<a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a>       <span class="n">estimators</span><span class="o">=</span><span class="n">base_models</span><span class="p">,</span>
<a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a>       <span class="n">final_estimator</span><span class="o">=</span><span class="n">meta_model</span><span class="p">,</span>
<a id="__codelineno-12-5" name="__codelineno-12-5" href="#__codelineno-12-5"></a>       <span class="n">cv</span><span class="o">=</span><span class="mi">5</span>  <span class="c1"># CRÍTICO</span>
<a id="__codelineno-12-6" name="__codelineno-12-6" href="#__codelineno-12-6"></a>   <span class="p">)</span>
</code></pre></div><p></p>
</li>
<li>
<p><strong>Meta-modelo simple:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a>   <span class="c1"># Preferir modelos lineales simples</span>
<a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a>   <span class="n">meta_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>  <span class="c1"># Regularizado</span>
<a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a>
<a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a>   <span class="c1"># En lugar de modelos complejos</span>
<a id="__codelineno-13-5" name="__codelineno-13-5" href="#__codelineno-13-5"></a>   <span class="c1"># meta_model = RandomForestClassifier()  # Puede overfit</span>
</code></pre></div><p></p>
</li>
<li>
<p><strong>Regularización:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a>   <span class="c1"># Usar regularización en meta-modelo</span>
<a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a>   <span class="n">meta_model</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>  <span class="c1"># Para regresión</span>
<a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a>   <span class="n">meta_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">'l2'</span><span class="p">)</span>  <span class="c1"># Para clasificación</span>
</code></pre></div><p></p>
</li>
<li>
<p><strong>Validación rigurosa:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a>   <span class="c1"># Conjunto de validación SEPARADO</span>
<a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a>   <span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a>   <span class="n">stacking</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a>   <span class="n">score</span> <span class="o">=</span> <span class="n">stacking</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>
</code></pre></div><p></p>
</li>
</ol>
<h3 id="cuando-usar-stacking">¿Cuándo usar Stacking?<a class="headerlink" href="#cuando-usar-stacking" title="Permanent link">¶</a></h3>
<p><strong>Usar Stacking cuando:</strong>
- ✅ Tienes varios modelos buenos y diversos
- ✅ Voting no es suficiente (quieres exprimir cada 0.1% de accuracy)
- ✅ Tienes datos suficientes para evitar overfitting del meta-modelo
- ✅ El costo computacional no es un problema
- ✅ Estás en una competición y buscas máximo rendimiento</p>
<p><strong>No usar Stacking cuando:</strong>
- ❌ Dataset pequeño (riesgo de overfitting)
- ❌ Modelos base son muy similares (poca ganancia)
- ❌ Necesitas interpretabilidad
- ❌ Recursos computacionales limitados
- ❌ Necesitas rapidez en producción</p>
<h3 id="ventajas-de-stacking">Ventajas de Stacking<a class="headerlink" href="#ventajas-de-stacking" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Combinación óptima aprendida:</strong> No necesitas adivinar pesos</li>
<li><strong>Puede capturar interacciones:</strong> El meta-modelo puede aprender cuando confiar en cada modelo</li>
<li><strong>Muy flexible:</strong> Puedes usar cualquier modelo en cualquier nivel</li>
<li><strong>Mejor rendimiento:</strong> A menudo supera a voting simple</li>
<li><strong>Aprovecha fortalezas:</strong> Cada modelo base aporta su perspectiva única</li>
</ul>
<h3 id="limitaciones-de-stacking">Limitaciones de Stacking<a class="headerlink" href="#limitaciones-de-stacking" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Complejidad:</strong> Más difícil de implementar y mantener</li>
<li><strong>Riesgo de overfitting:</strong> Especialmente con meta-modelos complejos</li>
<li><strong>Computacionalmente costoso:</strong> Entrenar múltiples modelos + meta-modelo</li>
<li><strong>Requiere más datos:</strong> Para evitar overfitting en el meta-modelo</li>
<li><strong>Difícil de interpretar:</strong> Caja negra dentro de caja negra</li>
<li><strong>Tuning complejo:</strong> Muchos hiperparámetros en dos niveles</li>
</ul>
<h3 id="ejemplo-completo">Ejemplo Completo<a class="headerlink" href="#ejemplo-completo" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_val_score</span>
<a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">GradientBoostingClassifier</span><span class="p">,</span> <span class="n">StackingClassifier</span>
<a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<a id="__codelineno-16-7" name="__codelineno-16-7" href="#__codelineno-16-7"></a>
<a id="__codelineno-16-8" name="__codelineno-16-8" href="#__codelineno-16-8"></a><span class="c1"># Generar datos</span>
<a id="__codelineno-16-9" name="__codelineno-16-9" href="#__codelineno-16-9"></a><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> 
<a id="__codelineno-16-10" name="__codelineno-16-10" href="#__codelineno-16-10"></a>                          <span class="n">n_redundant</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-16-11" name="__codelineno-16-11" href="#__codelineno-16-11"></a><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-16-12" name="__codelineno-16-12" href="#__codelineno-16-12"></a>
<a id="__codelineno-16-13" name="__codelineno-16-13" href="#__codelineno-16-13"></a><span class="c1"># Modelos base</span>
<a id="__codelineno-16-14" name="__codelineno-16-14" href="#__codelineno-16-14"></a><span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-16-15" name="__codelineno-16-15" href="#__codelineno-16-15"></a><span class="n">gb</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-16-16" name="__codelineno-16-16" href="#__codelineno-16-16"></a><span class="n">svm</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">probability</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-16-17" name="__codelineno-16-17" href="#__codelineno-16-17"></a>
<a id="__codelineno-16-18" name="__codelineno-16-18" href="#__codelineno-16-18"></a><span class="c1"># Evaluar modelos individuales</span>
<a id="__codelineno-16-19" name="__codelineno-16-19" href="#__codelineno-16-19"></a><span class="nb">print</span><span class="p">(</span><span class="s2">"Modelos individuales:"</span><span class="p">)</span>
<a id="__codelineno-16-20" name="__codelineno-16-20" href="#__codelineno-16-20"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Random Forest: </span><span class="si">{</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">rf</span><span class="p">,</span><span class="w"> </span><span class="n">X_train</span><span class="p">,</span><span class="w"> </span><span class="n">y_train</span><span class="p">,</span><span class="w"> </span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-16-21" name="__codelineno-16-21" href="#__codelineno-16-21"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Gradient Boosting: </span><span class="si">{</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">gb</span><span class="p">,</span><span class="w"> </span><span class="n">X_train</span><span class="p">,</span><span class="w"> </span><span class="n">y_train</span><span class="p">,</span><span class="w"> </span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-16-22" name="__codelineno-16-22" href="#__codelineno-16-22"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"SVM: </span><span class="si">{</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">svm</span><span class="p">,</span><span class="w"> </span><span class="n">X_train</span><span class="p">,</span><span class="w"> </span><span class="n">y_train</span><span class="p">,</span><span class="w"> </span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-16-23" name="__codelineno-16-23" href="#__codelineno-16-23"></a>
<a id="__codelineno-16-24" name="__codelineno-16-24" href="#__codelineno-16-24"></a><span class="c1"># Stacking</span>
<a id="__codelineno-16-25" name="__codelineno-16-25" href="#__codelineno-16-25"></a><span class="n">stacking</span> <span class="o">=</span> <span class="n">StackingClassifier</span><span class="p">(</span>
<a id="__codelineno-16-26" name="__codelineno-16-26" href="#__codelineno-16-26"></a>    <span class="n">estimators</span><span class="o">=</span><span class="p">[</span>
<a id="__codelineno-16-27" name="__codelineno-16-27" href="#__codelineno-16-27"></a>        <span class="p">(</span><span class="s1">'rf'</span><span class="p">,</span> <span class="n">rf</span><span class="p">),</span>
<a id="__codelineno-16-28" name="__codelineno-16-28" href="#__codelineno-16-28"></a>        <span class="p">(</span><span class="s1">'gb'</span><span class="p">,</span> <span class="n">gb</span><span class="p">),</span>
<a id="__codelineno-16-29" name="__codelineno-16-29" href="#__codelineno-16-29"></a>        <span class="p">(</span><span class="s1">'svm'</span><span class="p">,</span> <span class="n">svm</span><span class="p">)</span>
<a id="__codelineno-16-30" name="__codelineno-16-30" href="#__codelineno-16-30"></a>    <span class="p">],</span>
<a id="__codelineno-16-31" name="__codelineno-16-31" href="#__codelineno-16-31"></a>    <span class="n">final_estimator</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">(),</span>
<a id="__codelineno-16-32" name="__codelineno-16-32" href="#__codelineno-16-32"></a>    <span class="n">cv</span><span class="o">=</span><span class="mi">5</span>
<a id="__codelineno-16-33" name="__codelineno-16-33" href="#__codelineno-16-33"></a><span class="p">)</span>
<a id="__codelineno-16-34" name="__codelineno-16-34" href="#__codelineno-16-34"></a>
<a id="__codelineno-16-35" name="__codelineno-16-35" href="#__codelineno-16-35"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Stacking: </span><span class="si">{</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">stacking</span><span class="p">,</span><span class="w"> </span><span class="n">X_train</span><span class="p">,</span><span class="w"> </span><span class="n">y_train</span><span class="p">,</span><span class="w"> </span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-16-36" name="__codelineno-16-36" href="#__codelineno-16-36"></a>
<a id="__codelineno-16-37" name="__codelineno-16-37" href="#__codelineno-16-37"></a><span class="c1"># Entrenar y evaluar en test</span>
<a id="__codelineno-16-38" name="__codelineno-16-38" href="#__codelineno-16-38"></a><span class="n">stacking</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-16-39" name="__codelineno-16-39" href="#__codelineno-16-39"></a><span class="n">test_score</span> <span class="o">=</span> <span class="n">stacking</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<a id="__codelineno-16-40" name="__codelineno-16-40" href="#__codelineno-16-40"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Stacking en test: </span><span class="si">{</span><span class="n">test_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<h3 id="resumen-stacking">Resumen: Stacking<a class="headerlink" href="#resumen-stacking" title="Permanent link">¶</a></h3>
<p>Stacking lleva la idea de ensemble un paso más allá: en lugar de usar combinaciones fijas (como en voting), <strong>aprende la mejor forma de combinar modelos</strong>. Es más sofisticado pero también más complejo y propenso al overfitting.</p>
<p>Hasta ahora hemos visto métodos que combinan modelos <strong>heterogéneos</strong> entrenados en los <strong>mismos datos</strong>. La siguiente pregunta natural es: <strong>¿y si la diversidad proviene de entrenar el mismo modelo en diferentes muestras de los datos?</strong> → <strong>Bagging</strong></p>
<hr>
<h2 id="3-bagging-bootstrap-aggregating">3. Bagging (Bootstrap Aggregating)<a class="headerlink" href="#3-bagging-bootstrap-aggregating" title="Permanent link">¶</a></h2>
<p>Bagging entrena múltiples modelos <strong>homogéneos</strong> en diferentes muestras bootstrap del conjunto de entrenamiento y promedia sus predicciones.</p>
<p><strong>Diferencias clave con Voting y Stacking:</strong>
- Voting/Stacking: Modelos <strong>heterogéneos</strong>, mismos datos
- Bagging: Modelos <strong>homogéneos</strong>, datos <strong>diferentes</strong> (bootstrap)</p>
<p><strong>Características principales:</strong>
- Reduce principalmente la <strong>varianza</strong>
- Los modelos se entrenan en <strong>paralelo</strong> (independientes)
- Usa muestreo con reemplazo (bootstrap sampling)
- Combina predicciones mediante promedio (regresión) o votación (clasificación)
- Especialmente efectivo con modelos de alta varianza</p>
<h3 id="bootstrap-sampling">Bootstrap Sampling<a class="headerlink" href="#bootstrap-sampling" title="Permanent link">¶</a></h3>
<p><strong>Bootstrap:</strong> Técnica estadística de remuestreo con reemplazo.
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a>Dataset original D con n ejemplos: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
<a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a>
<a id="__codelineno-17-3" name="__codelineno-17-3" href="#__codelineno-17-3"></a>Bootstrap sample 1: [1, 3, 3, 5, 7, 8, 8, 9, 10, 10]  (algunos repetidos, otros ausentes)
<a id="__codelineno-17-4" name="__codelineno-17-4" href="#__codelineno-17-4"></a>Bootstrap sample 2: [1, 2, 2, 4, 4, 5, 6, 8, 9, 9]
<a id="__codelineno-17-5" name="__codelineno-17-5" href="#__codelineno-17-5"></a>Bootstrap sample 3: [2, 3, 4, 5, 5, 6, 7, 7, 8, 10]
<a id="__codelineno-17-6" name="__codelineno-17-6" href="#__codelineno-17-6"></a>...
</code></pre></div><p></p>
<p><strong>Propiedades importantes:</strong>
- Cada muestra bootstrap tiene el mismo tamaño que D (n ejemplos)
- Algunos ejemplos aparecen múltiples veces
- Algunos ejemplos no aparecen (out-of-bag)
- Probabilidad de que un ejemplo NO esté en una muestra: <span class="arithmatex">\((1-\frac{1}{n})^n \approx 0.368\)</span> para n grande
- Aproximadamente el <strong>37% de ejemplos quedan fuera</strong> de cada muestra (out-of-bag)</p>
<h3 id="algoritmo-de-bagging">Algoritmo de Bagging<a class="headerlink" href="#algoritmo-de-bagging" title="Permanent link">¶</a></h3>
<p><strong>Entrenamiento:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a>Entrada: Dataset D, algoritmo base L, número de modelos B
<a id="__codelineno-18-2" name="__codelineno-18-2" href="#__codelineno-18-2"></a>
<a id="__codelineno-18-3" name="__codelineno-18-3" href="#__codelineno-18-3"></a>Para b = 1 hasta B:
<a id="__codelineno-18-4" name="__codelineno-18-4" href="#__codelineno-18-4"></a>    1. Generar muestra bootstrap Dₑ muestreando n ejemplos de D con reemplazo
<a id="__codelineno-18-5" name="__codelineno-18-5" href="#__codelineno-18-5"></a>    2. Entrenar modelo hₑ = L(Dₑ)
<a id="__codelineno-18-6" name="__codelineno-18-6" href="#__codelineno-18-6"></a>
<a id="__codelineno-18-7" name="__codelineno-18-7" href="#__codelineno-18-7"></a>Salida: Ensemble {h₁, h₂, ..., hₑ}
</code></pre></div><p></p>
<p><strong>Predicción:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a>Para regresión:
<a id="__codelineno-19-2" name="__codelineno-19-2" href="#__codelineno-19-2"></a>    ŷ = (1/B) * Σ hₑ(x)  (promedio)
<a id="__codelineno-19-3" name="__codelineno-19-3" href="#__codelineno-19-3"></a>
<a id="__codelineno-19-4" name="__codelineno-19-4" href="#__codelineno-19-4"></a>Para clasificación:
<a id="__codelineno-19-5" name="__codelineno-19-5" href="#__codelineno-19-5"></a>    ŷ = mode(h₁(x), h₂(x), ..., hₑ(x))  (votación por mayoría)
<a id="__codelineno-19-6" name="__codelineno-19-6" href="#__codelineno-19-6"></a>
<a id="__codelineno-19-7" name="__codelineno-19-7" href="#__codelineno-19-7"></a>    O con probabilidades:
<a id="__codelineno-19-8" name="__codelineno-19-8" href="#__codelineno-19-8"></a>    P(y=c|x) = (1/B) * Σ P_b(y=c|x)
</code></pre></div><p></p>
<h3 id="out-of-bag-oob-error">Out-of-Bag (OOB) Error<a class="headerlink" href="#out-of-bag-oob-error" title="Permanent link">¶</a></h3>
<p>Aproximadamente el 37% de las muestras no aparecen en cada bootstrap sample. Estas muestras <strong>out-of-bag</strong> se pueden usar para validación sin necesidad de un conjunto separado.</p>
<p><strong>Cálculo del OOB error:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a>Para cada ejemplo xᵢ en el dataset de entrenamiento:
<a id="__codelineno-20-2" name="__codelineno-20-2" href="#__codelineno-20-2"></a>    1. Encontrar todos los modelos que NO usaron xᵢ en entrenamiento
<a id="__codelineno-20-3" name="__codelineno-20-3" href="#__codelineno-20-3"></a>    2. Usar esos modelos para predecir xᵢ
<a id="__codelineno-20-4" name="__codelineno-20-4" href="#__codelineno-20-4"></a>    3. Comparar con la etiqueta verdadera yᵢ
<a id="__codelineno-20-5" name="__codelineno-20-5" href="#__codelineno-20-5"></a>
<a id="__codelineno-20-6" name="__codelineno-20-6" href="#__codelineno-20-6"></a>OOB Error = (1/n) * Σ L(yᵢ, ŷᵢ^OOB)
</code></pre></div><p></p>
<p><strong>Ventajas del OOB error:</strong>
- ✅ Estimación no sesgada del error de generalización
- ✅ No requiere conjunto de validación separado
- ✅ Usa todos los datos de entrenamiento
- ✅ Gratis (no costo computacional adicional)</p>
<h3 id="implementacion-de-bagging-generico">Implementación de Bagging Genérico<a class="headerlink" href="#implementacion-de-bagging-generico" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span><span class="p">,</span> <span class="n">BaggingRegressor</span>
<a id="__codelineno-21-2" name="__codelineno-21-2" href="#__codelineno-21-2"></a><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<a id="__codelineno-21-3" name="__codelineno-21-3" href="#__codelineno-21-3"></a><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<a id="__codelineno-21-4" name="__codelineno-21-4" href="#__codelineno-21-4"></a>
<a id="__codelineno-21-5" name="__codelineno-21-5" href="#__codelineno-21-5"></a><span class="c1"># Bagging con árboles de decisión profundos</span>
<a id="__codelineno-21-6" name="__codelineno-21-6" href="#__codelineno-21-6"></a><span class="n">bagging_tree</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span>
<a id="__codelineno-21-7" name="__codelineno-21-7" href="#__codelineno-21-7"></a>    <span class="n">estimator</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>  <span class="c1"># Árbol sin restricciones</span>
<a id="__codelineno-21-8" name="__codelineno-21-8" href="#__codelineno-21-8"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>           <span class="c1"># Número de modelos</span>
<a id="__codelineno-21-9" name="__codelineno-21-9" href="#__codelineno-21-9"></a>    <span class="n">max_samples</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>            <span class="c1"># Usar 100% de muestras (default)</span>
<a id="__codelineno-21-10" name="__codelineno-21-10" href="#__codelineno-21-10"></a>    <span class="n">max_features</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>           <span class="c1"># Usar 100% de features (default)</span>
<a id="__codelineno-21-11" name="__codelineno-21-11" href="#__codelineno-21-11"></a>    <span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>             <span class="c1"># Con reemplazo (default)</span>
<a id="__codelineno-21-12" name="__codelineno-21-12" href="#__codelineno-21-12"></a>    <span class="n">oob_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>             <span class="c1"># Calcular OOB error</span>
<a id="__codelineno-21-13" name="__codelineno-21-13" href="#__codelineno-21-13"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-21-14" name="__codelineno-21-14" href="#__codelineno-21-14"></a><span class="p">)</span>
<a id="__codelineno-21-15" name="__codelineno-21-15" href="#__codelineno-21-15"></a>
<a id="__codelineno-21-16" name="__codelineno-21-16" href="#__codelineno-21-16"></a><span class="n">bagging_tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-21-17" name="__codelineno-21-17" href="#__codelineno-21-17"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"OOB Score: </span><span class="si">{</span><span class="n">bagging_tree</span><span class="o">.</span><span class="n">oob_score_</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-21-18" name="__codelineno-21-18" href="#__codelineno-21-18"></a>
<a id="__codelineno-21-19" name="__codelineno-21-19" href="#__codelineno-21-19"></a><span class="c1"># Bagging con KNN</span>
<a id="__codelineno-21-20" name="__codelineno-21-20" href="#__codelineno-21-20"></a><span class="n">bagging_knn</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span>
<a id="__codelineno-21-21" name="__codelineno-21-21" href="#__codelineno-21-21"></a>    <span class="n">estimator</span><span class="o">=</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
<a id="__codelineno-21-22" name="__codelineno-21-22" href="#__codelineno-21-22"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
<a id="__codelineno-21-23" name="__codelineno-21-23" href="#__codelineno-21-23"></a>    <span class="n">oob_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-21-24" name="__codelineno-21-24" href="#__codelineno-21-24"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-21-25" name="__codelineno-21-25" href="#__codelineno-21-25"></a><span class="p">)</span>
<a id="__codelineno-21-26" name="__codelineno-21-26" href="#__codelineno-21-26"></a>
<a id="__codelineno-21-27" name="__codelineno-21-27" href="#__codelineno-21-27"></a><span class="n">bagging_knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<h3 id="por-que-bagging-reduce-la-varianza">¿Por qué Bagging reduce la varianza?<a class="headerlink" href="#por-que-bagging-reduce-la-varianza" title="Permanent link">¶</a></h3>
<p><strong>Análisis teórico:</strong></p>
<p>Supongamos que tenemos B modelos independientes con varianza <span class="arithmatex">\(\sigma^2\)</span> cada uno. Si los promediamos:</p>
<div class="arithmatex">\[\text{Var}(\text{promedio}) = \text{Var}\left(\frac{1}{B}\sum_{i=1}^B h_i\right) = \frac{1}{B^2} \sum_{i=1}^B \text{Var}(h_i) = \frac{\sigma^2}{B}\]</div>
<p>La varianza se reduce por un factor de B.</p>
<p><strong>En la práctica:</strong>
Los modelos no son completamente independientes (están entrenados en muestras correlacionadas), pero aún así la reducción de varianza es significativa.</p>
<p><strong>Con correlación ρ entre modelos:</strong></p>
<div class="arithmatex">\[\text{Var}(\text{ensemble}) = \rho\sigma^2 + \frac{1-\rho}{B}\sigma^2\]</div>
<ul>
<li>A mayor correlación → menor reducción de varianza</li>
<li>Por eso la <strong>diversidad</strong> es importante</li>
</ul>
<h3 id="ventajas-de-bagging">Ventajas de Bagging<a class="headerlink" href="#ventajas-de-bagging" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Reduce varianza:</strong> Muy efectivo con modelos de alta varianza</li>
<li><strong>Previene overfitting:</strong> Especialmente con árboles profundos</li>
<li><strong>Paralelizable:</strong> Todos los modelos se entrenan independientemente</li>
<li><strong>Simple:</strong> Fácil de implementar y entender</li>
<li><strong>OOB evaluation:</strong> Validación gratuita</li>
<li><strong>Robusto:</strong> Menos sensible a outliers que un modelo individual</li>
<li><strong>No aumenta sesgo:</strong> El sesgo se mantiene aproximadamente igual</li>
</ul>
<h3 id="limitaciones-de-bagging">Limitaciones de Bagging<a class="headerlink" href="#limitaciones-de-bagging" title="Permanent link">¶</a></h3>
<ul>
<li><strong>No reduce sesgo:</strong> Si el modelo base tiene alto sesgo, bagging no lo arregla</li>
<li><strong>Menos interpretable:</strong> Perder la simplicidad del modelo individual</li>
<li><strong>Computacionalmente costoso:</strong> Entrenar múltiples modelos</li>
<li><strong>Memoria:</strong> Necesita almacenar múltiples modelos</li>
<li><strong>Mejora limitada con modelos estables:</strong> No aporta mucho con modelos de baja varianza (ej: regresión lineal)</li>
</ul>
<h3 id="cuando-usar-bagging">Cuándo usar Bagging<a class="headerlink" href="#cuando-usar-bagging" title="Permanent link">¶</a></h3>
<p><strong>Usar Bagging cuando:</strong>
- ✅ Tu modelo base tiene <strong>alta varianza</strong> (ej: árboles profundos, KNN con k pequeño)
- ✅ El modelo es <strong>inestable</strong> (pequeños cambios en datos → grandes cambios en modelo)
- ✅ Quieres <strong>reducir overfitting</strong>
- ✅ Puedes <strong>paralelizar</strong> el entrenamiento
- ✅ El modelo base es <strong>rápido</strong> de entrenar</p>
<p><strong>Modelos que se benefician más de Bagging:</strong>
- Árboles de decisión sin poda (alta varianza)
- KNN con k pequeño
- Redes neuronales sin regularización
- Cualquier modelo con alta varianza y bajo sesgo</p>
<p><strong>Modelos que NO se benefician mucho:</strong>
- Regresión lineal (ya tiene baja varianza)
- Naive Bayes (modelo simple y estable)
- Modelos fuertemente regularizados</p>
<h3 id="ejemplo-comparativo">Ejemplo Comparativo<a class="headerlink" href="#ejemplo-comparativo" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>
<a id="__codelineno-22-2" name="__codelineno-22-2" href="#__codelineno-22-2"></a><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<a id="__codelineno-22-3" name="__codelineno-22-3" href="#__codelineno-22-3"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>
<a id="__codelineno-22-4" name="__codelineno-22-4" href="#__codelineno-22-4"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-22-5" name="__codelineno-22-5" href="#__codelineno-22-5"></a>
<a id="__codelineno-22-6" name="__codelineno-22-6" href="#__codelineno-22-6"></a><span class="c1"># Datos con ruido</span>
<a id="__codelineno-22-7" name="__codelineno-22-7" href="#__codelineno-22-7"></a><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-22-8" name="__codelineno-22-8" href="#__codelineno-22-8"></a><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<a id="__codelineno-22-9" name="__codelineno-22-9" href="#__codelineno-22-9"></a>
<a id="__codelineno-22-10" name="__codelineno-22-10" href="#__codelineno-22-10"></a><span class="c1"># Árbol profundo individual (alta varianza)</span>
<a id="__codelineno-22-11" name="__codelineno-22-11" href="#__codelineno-22-11"></a><span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-22-12" name="__codelineno-22-12" href="#__codelineno-22-12"></a><span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-22-13" name="__codelineno-22-13" href="#__codelineno-22-13"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Árbol individual: </span><span class="si">{</span><span class="n">tree</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-22-14" name="__codelineno-22-14" href="#__codelineno-22-14"></a>
<a id="__codelineno-22-15" name="__codelineno-22-15" href="#__codelineno-22-15"></a><span class="c1"># Bagging de árboles profundos</span>
<a id="__codelineno-22-16" name="__codelineno-22-16" href="#__codelineno-22-16"></a><span class="n">bagging</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span>
<a id="__codelineno-22-17" name="__codelineno-22-17" href="#__codelineno-22-17"></a>    <span class="n">estimator</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
<a id="__codelineno-22-18" name="__codelineno-22-18" href="#__codelineno-22-18"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-22-19" name="__codelineno-22-19" href="#__codelineno-22-19"></a>    <span class="n">oob_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-22-20" name="__codelineno-22-20" href="#__codelineno-22-20"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-22-21" name="__codelineno-22-21" href="#__codelineno-22-21"></a><span class="p">)</span>
<a id="__codelineno-22-22" name="__codelineno-22-22" href="#__codelineno-22-22"></a><span class="n">bagging</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-22-23" name="__codelineno-22-23" href="#__codelineno-22-23"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Bagging (100 árboles): </span><span class="si">{</span><span class="n">bagging</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-22-24" name="__codelineno-22-24" href="#__codelineno-22-24"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"OOB Score: </span><span class="si">{</span><span class="n">bagging</span><span class="o">.</span><span class="n">oob_score_</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<p><strong>Resultado típico:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-23-1" name="__codelineno-23-1" href="#__codelineno-23-1"></a>Árbol individual: 0.82
<a id="__codelineno-23-2" name="__codelineno-23-2" href="#__codelineno-23-2"></a>Bagging (100 árboles): 0.92
<a id="__codelineno-23-3" name="__codelineno-23-3" href="#__codelineno-23-3"></a>OOB Score: 0.91
</code></pre></div><p></p>
<hr>
<h2 id="31-random-forest">3.1. Random Forest<a class="headerlink" href="#31-random-forest" title="Permanent link">¶</a></h2>
<p>Random Forest es el método de bagging más popular, específicamente diseñado y optimizado para árboles de decisión.</p>
<p><strong>Innovación clave:</strong> Random Forest = Bagging + <strong>Random Feature Selection</strong></p>
<h3 id="dos-fuentes-de-aleatoriedad">Dos fuentes de aleatoriedad<a class="headerlink" href="#dos-fuentes-de-aleatoriedad" title="Permanent link">¶</a></h3>
<p>Random Forest introduce <strong>dos niveles de randomización</strong> para aumentar la diversidad entre árboles:</p>
<ol>
<li><strong>Bootstrap sampling (como Bagging estándar):</strong></li>
<li>Cada árbol se entrena en una muestra bootstrap diferente</li>
<li>
<p>~37% de ejemplos quedan out-of-bag en cada árbol</p>
</li>
<li>
<p><strong>Random feature selection (nuevo):</strong></p>
</li>
<li>En cada split de cada nodo, solo se considera un subconjunto aleatorio de m features</li>
<li>Esto <strong>decorrelaciona</strong> los árboles aún más</li>
</ol>
<h3 id="algoritmo-de-random-forest">Algoritmo de Random Forest<a class="headerlink" href="#algoritmo-de-random-forest" title="Permanent link">¶</a></h3>
<p><strong>Entrenamiento:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-24-1" name="__codelineno-24-1" href="#__codelineno-24-1"></a>Entrada: Dataset D, número de árboles B, número de features m
<a id="__codelineno-24-2" name="__codelineno-24-2" href="#__codelineno-24-2"></a>
<a id="__codelineno-24-3" name="__codelineno-24-3" href="#__codelineno-24-3"></a>Para b = 1 hasta B:
<a id="__codelineno-24-4" name="__codelineno-24-4" href="#__codelineno-24-4"></a>    1. Generar muestra bootstrap Dₑ
<a id="__codelineno-24-5" name="__codelineno-24-5" href="#__codelineno-24-5"></a>    2. Crear árbol Tₑ:
<a id="__codelineno-24-6" name="__codelineno-24-6" href="#__codelineno-24-6"></a>       Para cada nodo en Tₑ:
<a id="__codelineno-24-7" name="__codelineno-24-7" href="#__codelineno-24-7"></a>           a. Seleccionar m features al azar del total p
<a id="__codelineno-24-8" name="__codelineno-24-8" href="#__codelineno-24-8"></a>           b. Encontrar el mejor split usando solo esas m features
<a id="__codelineno-24-9" name="__codelineno-24-9" href="#__codelineno-24-9"></a>           c. Dividir el nodo
<a id="__codelineno-24-10" name="__codelineno-24-10" href="#__codelineno-24-10"></a>    3. Crecer el árbol hasta profundidad máxima (sin poda)
<a id="__codelineno-24-11" name="__codelineno-24-11" href="#__codelineno-24-11"></a>
<a id="__codelineno-24-12" name="__codelineno-24-12" href="#__codelineno-24-12"></a>Salida: Random Forest {T₁, T₂, ..., Tₑ}
</code></pre></div><p></p>
<p><strong>Diferencia con Bagging puro:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-25-1" name="__codelineno-25-1" href="#__codelineno-25-1"></a>Bagging de árboles:
<a id="__codelineno-25-2" name="__codelineno-25-2" href="#__codelineno-25-2"></a>└─ En cada split: considerar TODAS las p features
<a id="__codelineno-25-3" name="__codelineno-25-3" href="#__codelineno-25-3"></a>
<a id="__codelineno-25-4" name="__codelineno-25-4" href="#__codelineno-25-4"></a>Random Forest:
<a id="__codelineno-25-5" name="__codelineno-25-5" href="#__codelineno-25-5"></a>└─ En cada split: considerar SOLO m features aleatorias (m &lt;&lt; p)
</code></pre></div><p></p>
<h3 id="hiperparametros-importantes">Hiperparámetros importantes<a class="headerlink" href="#hiperparametros-importantes" title="Permanent link">¶</a></h3>
<h4 id="parametros-de-ensemble">Parámetros de ensemble:<a class="headerlink" href="#parametros-de-ensemble" title="Permanent link">¶</a></h4>
<p><strong><code>n_estimators</code></strong> (número de árboles):
- Más árboles → mejor rendimiento (hasta convergencia)
- Típicamente: 100-500 árboles
- No causa overfitting (solo aumenta costo computacional)</p>
<p><strong><code>max_features</code></strong> (m, features por split):
- <strong>Clasificación:</strong> <span class="arithmatex">\(m = \sqrt{p}\)</span> (default)
- <strong>Regresión:</strong> <span class="arithmatex">\(m = p/3\)</span> (default)
- Valores menores → más decorrelación, pero puede aumentar sesgo
- Valores mayores → menos decorrelación, árboles más correlacionados</p>
<p><strong><code>bootstrap</code></strong>:
- True: usa bootstrap sampling (default)
- False: usa todo el dataset (se convierte en Extra Trees)</p>
<p><strong><code>oob_score</code></strong>:
- True: calcula error OOB
- Útil para validación sin conjunto separado</p>
<h4 id="parametros-de-arboles-individuales">Parámetros de árboles individuales:<a class="headerlink" href="#parametros-de-arboles-individuales" title="Permanent link">¶</a></h4>
<p><strong><code>max_depth</code></strong>:
- Profundidad máxima de cada árbol
- None: sin límite (árboles crecen completamente)
- Valores menores → menos overfitting, más sesgo</p>
<p><strong><code>min_samples_split</code></strong>:
- Mínimo de muestras para dividir un nodo
- Default: 2
- Valores mayores → árboles más simples</p>
<p><strong><code>min_samples_leaf</code></strong>:
- Mínimo de muestras en una hoja
- Default: 1
- Valores mayores → regularización más fuerte</p>
<p><strong><code>max_leaf_nodes</code></strong>:
- Máximo número de hojas
- None: sin límite
- Útil para controlar complejidad</p>
<h3 id="feature-importance">Feature Importance<a class="headerlink" href="#feature-importance" title="Permanent link">¶</a></h3>
<p>Random Forest calcula la importancia de cada feature de dos formas:</p>
<h4 id="1-gini-importance-o-impurity-based">1. Gini Importance (o Impurity-based)<a class="headerlink" href="#1-gini-importance-o-impurity-based" title="Permanent link">¶</a></h4>
<p>Basada en la mejora promedio en impureza cuando se usa esa feature:</p>
<div class="arithmatex">\[\text{Importance}(X_j) = \frac{1}{B} \sum_{b=1}^{B} \sum_{t \in T_b} \Delta i_t \cdot \mathbb{1}(v_t = j)\]</div>
<p>Donde:
- <span class="arithmatex">\(\Delta i_t\)</span> es la reducción en impureza en el nodo t
- <span class="arithmatex">\(v_t\)</span> es la feature usada en el split del nodo t
- Se promedia sobre todos los árboles B
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-26-1" name="__codelineno-26-1" href="#__codelineno-26-1"></a><span class="c1"># Calcular importancia</span>
<a id="__codelineno-26-2" name="__codelineno-26-2" href="#__codelineno-26-2"></a><span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<a id="__codelineno-26-3" name="__codelineno-26-3" href="#__codelineno-26-3"></a><span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-26-4" name="__codelineno-26-4" href="#__codelineno-26-4"></a>
<a id="__codelineno-26-5" name="__codelineno-26-5" href="#__codelineno-26-5"></a><span class="n">importances</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">feature_importances_</span>
<a id="__codelineno-26-6" name="__codelineno-26-6" href="#__codelineno-26-6"></a><span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<a id="__codelineno-26-7" name="__codelineno-26-7" href="#__codelineno-26-7"></a>
<a id="__codelineno-26-8" name="__codelineno-26-8" href="#__codelineno-26-8"></a><span class="nb">print</span><span class="p">(</span><span class="s2">"Feature ranking:"</span><span class="p">)</span>
<a id="__codelineno-26-9" name="__codelineno-26-9" href="#__codelineno-26-9"></a><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
<a id="__codelineno-26-10" name="__codelineno-26-10" href="#__codelineno-26-10"></a>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">. Feature </span><span class="si">{</span><span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">importances</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div><p></p>
<p><strong>Características:</strong>
- ✅ Rápida de calcular (viene gratis con el entrenamiento)
- ✅ Considera interacciones entre features
- ⚠️ Sesgada hacia features de alta cardinalidad
- ⚠️ No confiable con features correlacionadas</p>
<h4 id="2-permutation-importance">2. Permutation Importance<a class="headerlink" href="#2-permutation-importance" title="Permanent link">¶</a></h4>
<p>Mide la degradación del rendimiento cuando se permutan los valores de una feature:
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-27-1" name="__codelineno-27-1" href="#__codelineno-27-1"></a><span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">permutation_importance</span>
<a id="__codelineno-27-2" name="__codelineno-27-2" href="#__codelineno-27-2"></a>
<a id="__codelineno-27-3" name="__codelineno-27-3" href="#__codelineno-27-3"></a><span class="c1"># Calcular permutation importance</span>
<a id="__codelineno-27-4" name="__codelineno-27-4" href="#__codelineno-27-4"></a><span class="n">result</span> <span class="o">=</span> <span class="n">permutation_importance</span><span class="p">(</span><span class="n">rf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-27-5" name="__codelineno-27-5" href="#__codelineno-27-5"></a>
<a id="__codelineno-27-6" name="__codelineno-27-6" href="#__codelineno-27-6"></a><span class="n">importances</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">importances_mean</span>
<a id="__codelineno-27-7" name="__codelineno-27-7" href="#__codelineno-27-7"></a><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
<a id="__codelineno-27-8" name="__codelineno-27-8" href="#__codelineno-27-8"></a>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Feature </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">importances</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> +/- </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">importances_std</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div><p></p>
<p><strong>Características:</strong>
- ✅ Más confiable con features correlacionadas
- ✅ No sesgada por cardinalidad
- ⚠️ Más costosa computacionalmente
- ⚠️ Requiere datos de test/validación</p>
<h3 id="out-of-bag-oob-error-en-random-forest">Out-of-Bag (OOB) Error en Random Forest<a class="headerlink" href="#out-of-bag-oob-error-en-random-forest" title="Permanent link">¶</a></h3>
<p>Como Random Forest usa bootstrap, hereda el OOB error estimation:
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-28-1" name="__codelineno-28-1" href="#__codelineno-28-1"></a><span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span>
<a id="__codelineno-28-2" name="__codelineno-28-2" href="#__codelineno-28-2"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-28-3" name="__codelineno-28-3" href="#__codelineno-28-3"></a>    <span class="n">oob_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Activar OOB</span>
<a id="__codelineno-28-4" name="__codelineno-28-4" href="#__codelineno-28-4"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-28-5" name="__codelineno-28-5" href="#__codelineno-28-5"></a><span class="p">)</span>
<a id="__codelineno-28-6" name="__codelineno-28-6" href="#__codelineno-28-6"></a>
<a id="__codelineno-28-7" name="__codelineno-28-7" href="#__codelineno-28-7"></a><span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-28-8" name="__codelineno-28-8" href="#__codelineno-28-8"></a>
<a id="__codelineno-28-9" name="__codelineno-28-9" href="#__codelineno-28-9"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"OOB Score: </span><span class="si">{</span><span class="n">rf</span><span class="o">.</span><span class="n">oob_score_</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-28-10" name="__codelineno-28-10" href="#__codelineno-28-10"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Test Score: </span><span class="si">{</span><span class="n">rf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div><p></p>
<p>El OOB score es típicamente una buena aproximación del error de generalización.</p>
<h3 id="implementacion-completa">Implementación completa<a class="headerlink" href="#implementacion-completa" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-29-1" name="__codelineno-29-1" href="#__codelineno-29-1"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">RandomForestRegressor</span>
<a id="__codelineno-29-2" name="__codelineno-29-2" href="#__codelineno-29-2"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<a id="__codelineno-29-3" name="__codelineno-29-3" href="#__codelineno-29-3"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-29-4" name="__codelineno-29-4" href="#__codelineno-29-4"></a>
<a id="__codelineno-29-5" name="__codelineno-29-5" href="#__codelineno-29-5"></a><span class="c1"># Clasificación</span>
<a id="__codelineno-29-6" name="__codelineno-29-6" href="#__codelineno-29-6"></a><span class="n">rf_clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span>
<a id="__codelineno-29-7" name="__codelineno-29-7" href="#__codelineno-29-7"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>           <span class="c1"># Número de árboles</span>
<a id="__codelineno-29-8" name="__codelineno-29-8" href="#__codelineno-29-8"></a>    <span class="n">max_features</span><span class="o">=</span><span class="s1">'sqrt'</span><span class="p">,</span>        <span class="c1"># sqrt(p) features por split</span>
<a id="__codelineno-29-9" name="__codelineno-29-9" href="#__codelineno-29-9"></a>    <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>             <span class="c1"># Sin límite de profundidad</span>
<a id="__codelineno-29-10" name="__codelineno-29-10" href="#__codelineno-29-10"></a>    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>        <span class="c1"># Mínimo para dividir</span>
<a id="__codelineno-29-11" name="__codelineno-29-11" href="#__codelineno-29-11"></a>    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>         <span class="c1"># Mínimo en hojas</span>
<a id="__codelineno-29-12" name="__codelineno-29-12" href="#__codelineno-29-12"></a>    <span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>             <span class="c1"># Bootstrap sampling</span>
<a id="__codelineno-29-13" name="__codelineno-29-13" href="#__codelineno-29-13"></a>    <span class="n">oob_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>             <span class="c1"># Calcular OOB</span>
<a id="__codelineno-29-14" name="__codelineno-29-14" href="#__codelineno-29-14"></a>    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>                  <span class="c1"># Paralelizar</span>
<a id="__codelineno-29-15" name="__codelineno-29-15" href="#__codelineno-29-15"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-29-16" name="__codelineno-29-16" href="#__codelineno-29-16"></a><span class="p">)</span>
<a id="__codelineno-29-17" name="__codelineno-29-17" href="#__codelineno-29-17"></a>
<a id="__codelineno-29-18" name="__codelineno-29-18" href="#__codelineno-29-18"></a><span class="c1"># Entrenar</span>
<a id="__codelineno-29-19" name="__codelineno-29-19" href="#__codelineno-29-19"></a><span class="n">rf_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-29-20" name="__codelineno-29-20" href="#__codelineno-29-20"></a>
<a id="__codelineno-29-21" name="__codelineno-29-21" href="#__codelineno-29-21"></a><span class="c1"># Evaluar</span>
<a id="__codelineno-29-22" name="__codelineno-29-22" href="#__codelineno-29-22"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Train score: </span><span class="si">{</span><span class="n">rf_clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="w"> </span><span class="n">y_train</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-29-23" name="__codelineno-29-23" href="#__codelineno-29-23"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"OOB score: </span><span class="si">{</span><span class="n">rf_clf</span><span class="o">.</span><span class="n">oob_score_</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-29-24" name="__codelineno-29-24" href="#__codelineno-29-24"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Test score: </span><span class="si">{</span><span class="n">rf_clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-29-25" name="__codelineno-29-25" href="#__codelineno-29-25"></a>
<a id="__codelineno-29-26" name="__codelineno-29-26" href="#__codelineno-29-26"></a><span class="c1"># Cross-validation</span>
<a id="__codelineno-29-27" name="__codelineno-29-27" href="#__codelineno-29-27"></a><span class="n">cv_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">rf_clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<a id="__codelineno-29-28" name="__codelineno-29-28" href="#__codelineno-29-28"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"CV score: </span><span class="si">{</span><span class="n">cv_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (+/- </span><span class="si">{</span><span class="n">cv_scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)"</span><span class="p">)</span>
<a id="__codelineno-29-29" name="__codelineno-29-29" href="#__codelineno-29-29"></a>
<a id="__codelineno-29-30" name="__codelineno-29-30" href="#__codelineno-29-30"></a><span class="c1"># Feature importance</span>
<a id="__codelineno-29-31" name="__codelineno-29-31" href="#__codelineno-29-31"></a><span class="n">importances</span> <span class="o">=</span> <span class="n">rf_clf</span><span class="o">.</span><span class="n">feature_importances_</span>
<a id="__codelineno-29-32" name="__codelineno-29-32" href="#__codelineno-29-32"></a><span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Top 5 features:"</span><span class="p">)</span>
<a id="__codelineno-29-33" name="__codelineno-29-33" href="#__codelineno-29-33"></a><span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="mi">5</span><span class="p">]</span>
<a id="__codelineno-29-34" name="__codelineno-29-34" href="#__codelineno-29-34"></a><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">:</span>
<a id="__codelineno-29-35" name="__codelineno-29-35" href="#__codelineno-29-35"></a>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Feature </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">importances</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<h3 id="ventajas-de-random-forest">Ventajas de Random Forest<a class="headerlink" href="#ventajas-de-random-forest" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Excelente rendimiento:</strong> Uno de los mejores algoritmos out-of-the-box</li>
<li><strong>Robusto:</strong> Maneja bien datos con ruido, outliers, features irrelevantes</li>
<li><strong>Versátil:</strong> Funciona bien en clasificación y regresión</li>
<li><strong>Poco tuning:</strong> Funciona bien con hiperparámetros por defecto</li>
<li><strong>Feature importance:</strong> Identifica features relevantes</li>
<li><strong>OOB evaluation:</strong> Validación sin conjunto separado</li>
<li><strong>Maneja missing values:</strong> (implementaciones modernas)</li>
<li><strong>Maneja features mixtas:</strong> Numéricas y categóricas</li>
<li><strong>No lineal:</strong> Captura relaciones complejas</li>
<li><strong>Paralelizable:</strong> Entrenamiento muy rápido</li>
<li><strong>No requiere normalización:</strong> Invariante a escala de features</li>
</ul>
<h3 id="limitaciones-de-random-forest">Limitaciones de Random Forest<a class="headerlink" href="#limitaciones-de-random-forest" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Menos interpretable:</strong> Que un árbol individual</li>
<li><strong>Memoria:</strong> Almacenar muchos árboles profundos</li>
<li><strong>Predicción lenta:</strong> Con muchos árboles (no tanto el entrenamiento)</li>
<li><strong>No extrapola:</strong> No puede predecir fuera del rango de entrenamiento</li>
<li><strong>Modelos grandes:</strong> Archivos de modelo pueden ser grandes</li>
<li><strong>Overfitting posible:</strong> Con datasets muy pequeños</li>
<li><strong>Peor con features muy correlacionadas:</strong> Que otros métodos</li>
</ul>
<h3 id="comparacion-bagging-vs-random-forest">Comparación: Bagging vs Random Forest<a class="headerlink" href="#comparacion-bagging-vs-random-forest" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-30-1" name="__codelineno-30-1" href="#__codelineno-30-1"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span><span class="p">,</span> <span class="n">RandomForestClassifier</span>
<a id="__codelineno-30-2" name="__codelineno-30-2" href="#__codelineno-30-2"></a><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<a id="__codelineno-30-3" name="__codelineno-30-3" href="#__codelineno-30-3"></a>
<a id="__codelineno-30-4" name="__codelineno-30-4" href="#__codelineno-30-4"></a><span class="c1"># Bagging puro (considera todas las features en cada split)</span>
<a id="__codelineno-30-5" name="__codelineno-30-5" href="#__codelineno-30-5"></a><span class="n">bagging</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span>
<a id="__codelineno-30-6" name="__codelineno-30-6" href="#__codelineno-30-6"></a>    <span class="n">estimator</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
<a id="__codelineno-30-7" name="__codelineno-30-7" href="#__codelineno-30-7"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-30-8" name="__codelineno-30-8" href="#__codelineno-30-8"></a>    <span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-30-9" name="__codelineno-30-9" href="#__codelineno-30-9"></a>    <span class="n">oob_score</span><span class="o">=</span><span class="kc">True</span>
<a id="__codelineno-30-10" name="__codelineno-30-10" href="#__codelineno-30-10"></a><span class="p">)</span>
<a id="__codelineno-30-11" name="__codelineno-30-11" href="#__codelineno-30-11"></a>
<a id="__codelineno-30-12" name="__codelineno-30-12" href="#__codelineno-30-12"></a><span class="c1"># Random Forest (solo considera subset de features en cada split)</span>
<a id="__codelineno-30-13" name="__codelineno-30-13" href="#__codelineno-30-13"></a><span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span>
<a id="__codelineno-30-14" name="__codelineno-30-14" href="#__codelineno-30-14"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-30-15" name="__codelineno-30-15" href="#__codelineno-30-15"></a>    <span class="n">max_features</span><span class="o">=</span><span class="s1">'sqrt'</span><span class="p">,</span>  <span class="c1"># Esto es la diferencia clave</span>
<a id="__codelineno-30-16" name="__codelineno-30-16" href="#__codelineno-30-16"></a>    <span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-30-17" name="__codelineno-30-17" href="#__codelineno-30-17"></a>    <span class="n">oob_score</span><span class="o">=</span><span class="kc">True</span>
<a id="__codelineno-30-18" name="__codelineno-30-18" href="#__codelineno-30-18"></a><span class="p">)</span>
<a id="__codelineno-30-19" name="__codelineno-30-19" href="#__codelineno-30-19"></a>
<a id="__codelineno-30-20" name="__codelineno-30-20" href="#__codelineno-30-20"></a><span class="n">bagging</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-30-21" name="__codelineno-30-21" href="#__codelineno-30-21"></a><span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-30-22" name="__codelineno-30-22" href="#__codelineno-30-22"></a>
<a id="__codelineno-30-23" name="__codelineno-30-23" href="#__codelineno-30-23"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Bagging OOB: </span><span class="si">{</span><span class="n">bagging</span><span class="o">.</span><span class="n">oob_score_</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-30-24" name="__codelineno-30-24" href="#__codelineno-30-24"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Random Forest OOB: </span><span class="si">{</span><span class="n">rf</span><span class="o">.</span><span class="n">oob_score_</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<p><strong>Resultado típico:</strong> Random Forest suele superar a Bagging puro porque:
- Los árboles están más decorrelacionados
- Mayor diversidad → mejor reducción de varianza</p>
<h3 id="tuning-de-random-forest">Tuning de Random Forest<a class="headerlink" href="#tuning-de-random-forest" title="Permanent link">¶</a></h3>
<p>Aunque funciona bien con defaults, se puede mejorar:
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-31-1" name="__codelineno-31-1" href="#__codelineno-31-1"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
<a id="__codelineno-31-2" name="__codelineno-31-2" href="#__codelineno-31-2"></a><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">randint</span>
<a id="__codelineno-31-3" name="__codelineno-31-3" href="#__codelineno-31-3"></a>
<a id="__codelineno-31-4" name="__codelineno-31-4" href="#__codelineno-31-4"></a><span class="c1"># Espacio de búsqueda</span>
<a id="__codelineno-31-5" name="__codelineno-31-5" href="#__codelineno-31-5"></a><span class="n">param_dist</span> <span class="o">=</span> <span class="p">{</span>
<a id="__codelineno-31-6" name="__codelineno-31-6" href="#__codelineno-31-6"></a>    <span class="s1">'n_estimators'</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">500</span><span class="p">],</span>
<a id="__codelineno-31-7" name="__codelineno-31-7" href="#__codelineno-31-7"></a>    <span class="s1">'max_features'</span><span class="p">:</span> <span class="p">[</span><span class="s1">'sqrt'</span><span class="p">,</span> <span class="s1">'log2'</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
<a id="__codelineno-31-8" name="__codelineno-31-8" href="#__codelineno-31-8"></a>    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>
<a id="__codelineno-31-9" name="__codelineno-31-9" href="#__codelineno-31-9"></a>    <span class="s1">'min_samples_split'</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
<a id="__codelineno-31-10" name="__codelineno-31-10" href="#__codelineno-31-10"></a>    <span class="s1">'min_samples_leaf'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
<a id="__codelineno-31-11" name="__codelineno-31-11" href="#__codelineno-31-11"></a>    <span class="s1">'bootstrap'</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]</span>
<a id="__codelineno-31-12" name="__codelineno-31-12" href="#__codelineno-31-12"></a><span class="p">}</span>
<a id="__codelineno-31-13" name="__codelineno-31-13" href="#__codelineno-31-13"></a>
<a id="__codelineno-31-14" name="__codelineno-31-14" href="#__codelineno-31-14"></a><span class="c1"># Random search</span>
<a id="__codelineno-31-15" name="__codelineno-31-15" href="#__codelineno-31-15"></a><span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-31-16" name="__codelineno-31-16" href="#__codelineno-31-16"></a><span class="n">search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span>
<a id="__codelineno-31-17" name="__codelineno-31-17" href="#__codelineno-31-17"></a>    <span class="n">rf</span><span class="p">,</span> 
<a id="__codelineno-31-18" name="__codelineno-31-18" href="#__codelineno-31-18"></a>    <span class="n">param_distributions</span><span class="o">=</span><span class="n">param_dist</span><span class="p">,</span>
<a id="__codelineno-31-19" name="__codelineno-31-19" href="#__codelineno-31-19"></a>    <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
<a id="__codelineno-31-20" name="__codelineno-31-20" href="#__codelineno-31-20"></a>    <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<a id="__codelineno-31-21" name="__codelineno-31-21" href="#__codelineno-31-21"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
<a id="__codelineno-31-22" name="__codelineno-31-22" href="#__codelineno-31-22"></a>    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<a id="__codelineno-31-23" name="__codelineno-31-23" href="#__codelineno-31-23"></a><span class="p">)</span>
<a id="__codelineno-31-24" name="__codelineno-31-24" href="#__codelineno-31-24"></a>
<a id="__codelineno-31-25" name="__codelineno-31-25" href="#__codelineno-31-25"></a><span class="n">search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-31-26" name="__codelineno-31-26" href="#__codelineno-31-26"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Best params: </span><span class="si">{</span><span class="n">search</span><span class="o">.</span><span class="n">best_params_</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-31-27" name="__codelineno-31-27" href="#__codelineno-31-27"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Best CV score: </span><span class="si">{</span><span class="n">search</span><span class="o">.</span><span class="n">best_score_</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div><p></p>
<h3 id="cuando-usar-random-forest">Cuándo usar Random Forest<a class="headerlink" href="#cuando-usar-random-forest" title="Permanent link">¶</a></h3>
<p><strong>Usar Random Forest cuando:</strong>
- ✅ Necesitas un <strong>baseline fuerte</strong> rápidamente
- ✅ Tienes <strong>datos tabulares</strong> (no imágenes/texto/secuencias)
- ✅ Quieres <strong>feature importance</strong>
- ✅ No tienes tiempo para mucho tuning
- ✅ Necesitas <strong>robustez</strong> (datos con ruido, outliers)
- ✅ Tienes <strong>features mixtas</strong> (categóricas y numéricas)</p>
<p><strong>Considerar alternativas cuando:</strong>
- ❌ Necesitas <strong>máximo rendimiento</strong> → XGBoost/LightGBM
- ❌ Necesitas <strong>interpretabilidad</strong> → árboles individuales, modelos lineales
- ❌ Datos son <strong>imágenes/texto</strong> → redes neuronales
- ❌ Dataset muy pequeño (n &lt; 100) → modelos más simples
- ❌ Predicción debe ser <strong>extremadamente rápida</strong> → modelos más simples</p>
<h3 id="extra-trees-extremely-randomized-trees">Extra Trees (Extremely Randomized Trees)<a class="headerlink" href="#extra-trees-extremely-randomized-trees" title="Permanent link">¶</a></h3>
<p>Una variante de Random Forest con aún más aleatoriedad:</p>
<p><strong>Diferencias con Random Forest:</strong></p>
<table>
<thead>
<tr>
<th>Aspecto</th>
<th>Random Forest</th>
<th>Extra Trees</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Sampling</strong></td>
<td>Bootstrap (con reemplazo)</td>
<td>Todo el dataset (sin bootstrap)</td>
</tr>
<tr>
<td><strong>Splits</strong></td>
<td>Mejor split entre m features</td>
<td>Split <strong>aleatorio</strong> entre m features</td>
</tr>
<tr>
<td><strong>Varianza</strong></td>
<td>Menor</td>
<td>Aún menor</td>
</tr>
<tr>
<td><strong>Sesgo</strong></td>
<td>Menor</td>
<td>Ligeramente mayor</td>
</tr>
<tr>
<td><strong>Velocidad</strong></td>
<td>Más lento</td>
<td>Más rápido (splits aleatorios)</td>
</tr>
<tr>
<td><div class="highlight"><pre><span></span><code><a id="__codelineno-32-1" name="__codelineno-32-1" href="#__codelineno-32-1"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesClassifier</span>
<a id="__codelineno-32-2" name="__codelineno-32-2" href="#__codelineno-32-2"></a>
<a id="__codelineno-32-3" name="__codelineno-32-3" href="#__codelineno-32-3"></a><span class="c1"># Extra Trees</span>
<a id="__codelineno-32-4" name="__codelineno-32-4" href="#__codelineno-32-4"></a><span class="n">et</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">(</span>
<a id="__codelineno-32-5" name="__codelineno-32-5" href="#__codelineno-32-5"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-32-6" name="__codelineno-32-6" href="#__codelineno-32-6"></a>    <span class="n">max_features</span><span class="o">=</span><span class="s1">'sqrt'</span><span class="p">,</span>
<a id="__codelineno-32-7" name="__codelineno-32-7" href="#__codelineno-32-7"></a>    <span class="n">bootstrap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># No usa bootstrap (default)</span>
<a id="__codelineno-32-8" name="__codelineno-32-8" href="#__codelineno-32-8"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-32-9" name="__codelineno-32-9" href="#__codelineno-32-9"></a><span class="p">)</span>
<a id="__codelineno-32-10" name="__codelineno-32-10" href="#__codelineno-32-10"></a>
<a id="__codelineno-32-11" name="__codelineno-32-11" href="#__codelineno-32-11"></a><span class="n">et</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Cuándo usar Extra Trees:</strong>
- Quieres aún más reducción de varianza
- Velocidad de entrenamiento es crítica
- Dispuesto a aceptar ligero aumento en sesgo</p>
<h3 id="resumen-random-forest">Resumen: Random Forest<a class="headerlink" href="#resumen-random-forest" title="Permanent link">¶</a></h3>
<p>Random Forest es el método de bagging más exitoso y uno de los algoritmos más importantes en machine learning. Combina:
- Bootstrap sampling (como Bagging)
- Random feature selection (innovación clave)
- Árboles profundos sin poda
- Promediado/votación</p>
<p>Es robusto, versátil, y funciona excelentemente como punto de partida en la mayoría de problemas con datos tabulares.</p>
<p>Hasta ahora hemos visto métodos que combinan modelos entrenados <strong>independientemente</strong> (en paralelo), ya sea en los mismos datos (Voting, Stacking) o en diferentes muestras (Bagging). La siguiente pregunta es: <strong>¿y si los modelos se entrenan secuencialmente, donde cada uno aprende de los errores del anterior?</strong> → <strong>Boosting</strong></p>
<hr>
<h2 id="4-boosting">4. Boosting<a class="headerlink" href="#4-boosting" title="Permanent link">¶</a></h2>
<p>Boosting entrena modelos secuencialmente, donde cada nuevo modelo se enfoca en corregir los errores cometidos por el ensemble actual.</p>
<p><strong>Diferencias clave con los métodos anteriores:</strong>
- Voting/Stacking/Bagging: Modelos <strong>independientes</strong> (paralelos)
- Boosting: Modelos <strong>dependientes</strong> (secuenciales)</p>
<p><strong>Características principales:</strong>
- Reduce principalmente el <strong>sesgo</strong> (y también algo la varianza)
- Los modelos se entrenan <strong>secuencialmente</strong>
- Cada modelo se enfoca en los ejemplos mal clasificados o en los residuos
- Los modelos se combinan mediante suma ponderada
- Usa típicamente <strong>clasificadores débiles</strong></p>
<h3 id="clasificadores-debiles-vs-clasificadores-fuertes">Clasificadores Débiles vs Clasificadores Fuertes<a class="headerlink" href="#clasificadores-debiles-vs-clasificadores-fuertes" title="Permanent link">¶</a></h3>
<p><strong>Contexto: Un concepto específico de Boosting</strong></p>
<p>El marco teórico de clasificadores débiles y fuertes es <strong>específico del paradigma de boosting</strong>. No se aplica de la misma manera a bagging, voting o stacking. Este concepto surge de la teoría de aprendizaje computacional (computational learning theory) y es fundamental para entender cómo y por qué funciona boosting.</p>
<h4 id="definiciones">Definiciones<a class="headerlink" href="#definiciones" title="Permanent link">¶</a></h4>
<p><strong>Clasificador débil (weak learner):</strong>
Un clasificador que es solo ligeramente mejor que el azar. Formalmente, para clasificación binaria, un clasificador débil es aquel cuya tasa de error es:</p>
<div class="arithmatex">\[\epsilon &lt; \frac{1}{2}\]</div>
<p>Es decir, su accuracy debe ser mayor que 50% (mejor que lanzar una moneda). No necesita ser muy preciso, solo consistentemente mejor que adivinar al azar.</p>
<p><strong>Clasificador fuerte (strong learner):</strong>
Un clasificador con alta precisión, capaz de aproximarse arbitrariamente bien a la función objetivo. Su tasa de error puede ser tan pequeña como se desee con suficientes datos y capacidad del modelo.</p>
<h4 id="ejemplos-practicos">Ejemplos Prácticos<a class="headerlink" href="#ejemplos-practicos" title="Permanent link">¶</a></h4>
<p><strong>Clasificadores débiles típicos:</strong>
- <strong>Decision stumps</strong>: Árboles de decisión con una única división (profundidad = 1)
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-33-1" name="__codelineno-33-1" href="#__codelineno-33-1"></a>  if x₁ &gt; 5:
<a id="__codelineno-33-2" name="__codelineno-33-2" href="#__codelineno-33-2"></a>      return +1
<a id="__codelineno-33-3" name="__codelineno-33-3" href="#__codelineno-33-3"></a>  else:
<a id="__codelineno-33-4" name="__codelineno-33-4" href="#__codelineno-33-4"></a>      return -1
</code></pre></div>
- <strong>Árboles poco profundos</strong>: Árboles con profundidad máxima 2-3
- <strong>Reglas simples</strong>: "Si feature &gt; threshold → clase positiva"
- <strong>Clasificadores lineales</strong> en problemas no lineales<p></p>
<p><strong>Clasificadores fuertes:</strong>
- Redes neuronales profundas
- Árboles de decisión muy profundos
- SVMs con kernels complejos
- Random Forests
- Gradient Boosting Machines</p>
<h4 id="el-teorema-fundamental-del-boosting">El Teorema Fundamental del Boosting<a class="headerlink" href="#el-teorema-fundamental-del-boosting" title="Permanent link">¶</a></h4>
<p>Uno de los resultados más importantes en teoría de aprendizaje automático es que <strong>un conjunto de clasificadores débiles puede combinarse para formar un clasificador fuerte</strong>.</p>
<p><strong>Teorema (Schapire, 1990):</strong>
Si existe un clasificador débil que puede lograr error menor que 1/2 - γ (donde γ &gt; 0), entonces existe un algoritmo de boosting que puede combinarlo para lograr un error arbitrariamente pequeño en el conjunto de entrenamiento.</p>
<p>Este teorema es revolucionario porque:
1. Demuestra que la "debilidad" es suficiente para el aprendizaje
2. Proporciona una garantía matemática sobre boosting
3. Es constructivo: muestra cómo construir el clasificador fuerte</p>
<h4 id="por-que-usar-clasificadores-debiles">¿Por qué usar clasificadores débiles?<a class="headerlink" href="#por-que-usar-clasificadores-debiles" title="Permanent link">¶</a></h4>
<p>Aunque pueda parecer contraintuitivo usar modelos "débiles", hay razones importantes:</p>
<ol>
<li><strong>Prevención de overfitting:</strong> </li>
<li>Modelos débiles tienen baja varianza</li>
<li>Menos propensos a memorizar ruido</li>
<li>
<p>La combinación reduce overfitting</p>
</li>
<li>
<p><strong>Eficiencia computacional:</strong></p>
</li>
<li>Decision stumps son extremadamente rápidos</li>
<li>
<p>Podemos entrenar cientos o miles rápidamente</p>
</li>
<li>
<p><strong>Interpretabilidad:</strong></p>
</li>
<li>Cada modelo débil es simple de entender</li>
<li>
<p>La combinación mantiene cierta trazabilidad</p>
</li>
<li>
<p><strong>Teoría sólida:</strong></p>
</li>
<li>Garantías matemáticas de convergencia</li>
<li>Bounds en el error de generalización</li>
</ol>
<h4 id="visualizacion-conceptual">Visualización Conceptual<a class="headerlink" href="#visualizacion-conceptual" title="Permanent link">¶</a></h4>
<p>Imaginemos un problema de clasificación binaria en 2D:
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-34-1" name="__codelineno-34-1" href="#__codelineno-34-1"></a>Clasificador débil 1 (división vertical):  
<a id="__codelineno-34-2" name="__codelineno-34-2" href="#__codelineno-34-2"></a>    |  +  -  |
<a id="__codelineno-34-3" name="__codelineno-34-3" href="#__codelineno-34-3"></a>    |  +  -  |
<a id="__codelineno-34-4" name="__codelineno-34-4" href="#__codelineno-34-4"></a>    |  -  +  |
<a id="__codelineno-34-5" name="__codelineno-34-5" href="#__codelineno-34-5"></a>
<a id="__codelineno-34-6" name="__codelineno-34-6" href="#__codelineno-34-6"></a>Clasificador débil 2 (división horizontal):
<a id="__codelineno-34-7" name="__codelineno-34-7" href="#__codelineno-34-7"></a>    --------
<a id="__codelineno-34-8" name="__codelineno-34-8" href="#__codelineno-34-8"></a>    +  +  +
<a id="__codelineno-34-9" name="__codelineno-34-9" href="#__codelineno-34-9"></a>    -  -  -
<a id="__codelineno-34-10" name="__codelineno-34-10" href="#__codelineno-34-10"></a>    --------
<a id="__codelineno-34-11" name="__codelineno-34-11" href="#__codelineno-34-11"></a>
<a id="__codelineno-34-12" name="__codelineno-34-12" href="#__codelineno-34-12"></a>Clasificador débil 3 (división diagonal):
<a id="__codelineno-34-13" name="__codelineno-34-13" href="#__codelineno-34-13"></a>      /  +  +
<a id="__codelineno-34-14" name="__codelineno-34-14" href="#__codelineno-34-14"></a>     /  +  -
<a id="__codelineno-34-15" name="__codelineno-34-15" href="#__codelineno-34-15"></a>    /  -  -
<a id="__codelineno-34-16" name="__codelineno-34-16" href="#__codelineno-34-16"></a>
<a id="__codelineno-34-17" name="__codelineno-34-17" href="#__codelineno-34-17"></a>Combinación (clasificador fuerte):
<a id="__codelineno-34-18" name="__codelineno-34-18" href="#__codelineno-34-18"></a>    Regiones complejas y precisas
<a id="__codelineno-34-19" name="__codelineno-34-19" href="#__codelineno-34-19"></a>    mediante la suma ponderada
</code></pre></div><p></p>
<p>Cada clasificador débil captura un patrón simple, pero su combinación puede representar fronteras de decisión arbitrariamente complejas.</p>
<h4 id="analogia-con-la-vida-real">Analogía con la Vida Real<a class="headerlink" href="#analogia-con-la-vida-real" title="Permanent link">¶</a></h4>
<p>Piensa en un panel de expertos versus un comité de conocedores:</p>
<p><strong>Clasificador fuerte = Experto individual:</strong>
- Una persona altamente capacitada
- Puede cometer errores por sesgos personales
- Costoso de formar/entrenar
- Si se equivoca, el error es grande</p>
<p><strong>Clasificadores débiles = Comité de personas:</strong>
- Cada persona sabe algo útil pero limitado
- Combinados, sus opiniones diversas capturan más información
- Más económico formar muchas personas con conocimiento básico
- Los errores individuales se compensan</p>
<h4 id="requerimientos-para-clasificadores-debiles">Requerimientos para Clasificadores Débiles<a class="headerlink" href="#requerimientos-para-clasificadores-debiles" title="Permanent link">¶</a></h4>
<p>Para que un clasificador débil sea útil en un ensemble debe cumplir:</p>
<ol>
<li><strong>Mejor que el azar:</strong> <span class="arithmatex">\(P(\text{correcto}) &gt; 0.5\)</span></li>
<li><strong>Diversidad:</strong> Cometer errores diferentes a otros clasificadores</li>
<li><strong>Eficiencia:</strong> Ser rápido de entrenar</li>
<li><strong>Estabilidad mínima:</strong> No ser extremadamente sensible a pequeños cambios</li>
</ol>
<h4 id="clasificadores-debiles-en-la-practica">Clasificadores Débiles en la Práctica<a class="headerlink" href="#clasificadores-debiles-en-la-practica" title="Permanent link">¶</a></h4>
<p><strong>AdaBoost típicamente usa:</strong>
- Decision stumps (árboles de profundidad 1)
- Accuracy individual: 55-70%
- Combinación de 50-500 stumps → accuracy &gt; 95%</p>
<p><strong>Gradient Boosting típicamente usa:</strong>
- Árboles de profundidad 3-8
- Cada árbol captura interacciones de bajo orden
- Combinación de 100-1000 árboles</p>
<h4 id="ejemplo-numerico">Ejemplo Numérico<a class="headerlink" href="#ejemplo-numerico" title="Permanent link">¶</a></h4>
<p>Consideremos un problema donde queremos clasificar si un correo es spam:
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-35-1" name="__codelineno-35-1" href="#__codelineno-35-1"></a><span class="c1"># Clasificador débil 1: "Si contiene 'gratis' → spam"</span>
<a id="__codelineno-35-2" name="__codelineno-35-2" href="#__codelineno-35-2"></a><span class="n">accuracy_1</span> <span class="o">=</span> <span class="mf">0.60</span>  <span class="c1"># 60% correcto</span>
<a id="__codelineno-35-3" name="__codelineno-35-3" href="#__codelineno-35-3"></a>
<a id="__codelineno-35-4" name="__codelineno-35-4" href="#__codelineno-35-4"></a><span class="c1"># Clasificador débil 2: "Si tiene &gt;3 signos $ → spam"  </span>
<a id="__codelineno-35-5" name="__codelineno-35-5" href="#__codelineno-35-5"></a><span class="n">accuracy_2</span> <span class="o">=</span> <span class="mf">0.58</span>  <span class="c1"># 58% correcto</span>
<a id="__codelineno-35-6" name="__codelineno-35-6" href="#__codelineno-35-6"></a>
<a id="__codelineno-35-7" name="__codelineno-35-7" href="#__codelineno-35-7"></a><span class="c1"># Clasificador débil 3: "Si todas mayúsculas → spam"</span>
<a id="__codelineno-35-8" name="__codelineno-35-8" href="#__codelineno-35-8"></a><span class="n">accuracy_3</span> <span class="o">=</span> <span class="mf">0.55</span>  <span class="c1"># 55% correcto</span>
<a id="__codelineno-35-9" name="__codelineno-35-9" href="#__codelineno-35-9"></a>
<a id="__codelineno-35-10" name="__codelineno-35-10" href="#__codelineno-35-10"></a><span class="c1"># Individualmente son débiles, pero...</span>
<a id="__codelineno-35-11" name="__codelineno-35-11" href="#__codelineno-35-11"></a><span class="c1"># Combinados con AdaBoost:</span>
<a id="__codelineno-35-12" name="__codelineno-35-12" href="#__codelineno-35-12"></a><span class="n">ensemble_accuracy</span> <span class="o">=</span> <span class="mf">0.94</span>  <span class="c1"># 94% correcto!</span>
</code></pre></div><p></p>
<p>Cada regla individual es imperfecta, pero la combinación inteligente de muchas reglas simples crea un clasificador robusto.</p>
<h4 id="conexion-con-sesgo-varianza">Conexión con Sesgo-Varianza<a class="headerlink" href="#conexion-con-sesgo-varianza" title="Permanent link">¶</a></h4>
<p>Los clasificadores débiles y fuertes se relacionan con el trade-off sesgo-varianza:</p>
<p><strong>Clasificadores débiles:</strong>
- Alto sesgo (no pueden capturar patrones complejos)
- Baja varianza (estables ante cambios en datos)
- Ejemplo: decision stump siempre divide igual</p>
<p><strong>Clasificadores fuertes:</strong>
- Bajo sesgo (pueden aprender patrones complejos)
- Alta varianza (sensibles a datos de entrenamiento)
- Ejemplo: árbol profundo memoriza detalles</p>
<p><strong>Ensemble de clasificadores débiles (Boosting):</strong>
- Reduce sesgo mediante combinación aditiva secuencial
- Mantiene baja varianza de modelos individuales
- <strong>Mejor de ambos mundos</strong></p>
<hr>
<h2 id="41-adaboost-adaptive-boosting">4.1. AdaBoost (Adaptive Boosting)<a class="headerlink" href="#41-adaboost-adaptive-boosting" title="Permanent link">¶</a></h2>
<p>AdaBoost es el primer algoritmo de boosting exitoso y responde directamente a la pregunta: <strong>¿Podemos convertir clasificadores débiles en un clasificador fuerte?</strong></p>
<p>Desarrollado por Freund y Schapire (1997), AdaBoost ganó el premio Gödel Prize por su importancia teórica y práctica.</p>
<h3 id="idea-central">Idea Central<a class="headerlink" href="#idea-central" title="Permanent link">¶</a></h3>
<p>AdaBoost funciona de forma adaptativa:
1. Entrena un clasificador débil
2. <strong>Aumenta el peso</strong> de los ejemplos mal clasificados
3. Entrena el siguiente clasificador en datos <strong>ponderados</strong>
4. Repite el proceso
5. Combina todos los clasificadores con pesos según su accuracy</p>
<p><strong>Metáfora:</strong> Es como un estudiante que identifica sus errores en un examen y se concentra más en estudiar esos temas para el siguiente examen.</p>
<h3 id="algoritmo-detallado">Algoritmo Detallado<a class="headerlink" href="#algoritmo-detallado" title="Permanent link">¶</a></h3>
<p><strong>Inicialización:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-36-1" name="__codelineno-36-1" href="#__codelineno-36-1"></a>Dataset: D = {(x₁,y₁), ..., (xₙ,yₙ)} donde yᵢ ∈ {-1, +1}
<a id="__codelineno-36-2" name="__codelineno-36-2" href="#__codelineno-36-2"></a>Inicializar pesos uniformes: w₁(i) = 1/n para i = 1, ..., n
</code></pre></div><p></p>
<p><strong>Para t = 1 hasta T:</strong></p>
<p><strong>Paso 1: Entrenar clasificador débil</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-37-1" name="__codelineno-37-1" href="#__codelineno-37-1"></a>Entrenar clasificador hₜ en D con distribución de pesos wₜ
<a id="__codelineno-37-2" name="__codelineno-37-2" href="#__codelineno-37-2"></a>(Los ejemplos con mayor peso tienen más influencia)
</code></pre></div><p></p>
<p><strong>Paso 2: Calcular error ponderado</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-38-1" name="__codelineno-38-1" href="#__codelineno-38-1"></a>εₜ = Σᵢ wₜ(i) · 𝟙(hₜ(xᵢ) ≠ yᵢ)
<a id="__codelineno-38-2" name="__codelineno-38-2" href="#__codelineno-38-2"></a>
<a id="__codelineno-38-3" name="__codelineno-38-3" href="#__codelineno-38-3"></a>Donde 𝟙 es la función indicadora (1 si error, 0 si correcto)
</code></pre></div><p></p>
<p><strong>Paso 3: Calcular peso del clasificador</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-39-1" name="__codelineno-39-1" href="#__codelineno-39-1"></a>αₜ = (1/2) · ln((1 - εₜ) / εₜ)
<a id="__codelineno-39-2" name="__codelineno-39-2" href="#__codelineno-39-2"></a>
<a id="__codelineno-39-3" name="__codelineno-39-3" href="#__codelineno-39-3"></a>Interpretación:
<a id="__codelineno-39-4" name="__codelineno-39-4" href="#__codelineno-39-4"></a>- Si εₜ pequeño (buen clasificador) → αₜ grande (mucho peso)
<a id="__codelineno-39-5" name="__codelineno-39-5" href="#__codelineno-39-5"></a>- Si εₜ = 0.5 (azar) → αₜ = 0 (sin peso)
<a id="__codelineno-39-6" name="__codelineno-39-6" href="#__codelineno-39-6"></a>- Si εₜ &gt; 0.5 (peor que azar) → αₜ negativo (invertir predicción)
</code></pre></div><p></p>
<p><strong>Paso 4: Actualizar pesos de ejemplos</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-40-1" name="__codelineno-40-1" href="#__codelineno-40-1"></a>wₜ₊₁(i) = wₜ(i) · exp(-αₜ · yᵢ · hₜ(xᵢ))
<a id="__codelineno-40-2" name="__codelineno-40-2" href="#__codelineno-40-2"></a>
<a id="__codelineno-40-3" name="__codelineno-40-3" href="#__codelineno-40-3"></a>Simplificado:
<a id="__codelineno-40-4" name="__codelineno-40-4" href="#__codelineno-40-4"></a>- Si hₜ clasifica correctamente xᵢ: wₜ₊₁(i) disminuye
<a id="__codelineno-40-5" name="__codelineno-40-5" href="#__codelineno-40-5"></a>- Si hₜ clasifica incorrectamente xᵢ: wₜ₊₁(i) aumenta
</code></pre></div><p></p>
<p><strong>Paso 5: Normalizar pesos</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-41-1" name="__codelineno-41-1" href="#__codelineno-41-1"></a>wₜ₊₁(i) = wₜ₊₁(i) / Σⱼ wₜ₊₁(j)
<a id="__codelineno-41-2" name="__codelineno-41-2" href="#__codelineno-41-2"></a>
<a id="__codelineno-41-3" name="__codelineno-41-3" href="#__codelineno-41-3"></a>(Para que los pesos sumen 1)
</code></pre></div><p></p>
<p><strong>Predicción final:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-42-1" name="__codelineno-42-1" href="#__codelineno-42-1"></a>H(x) = sign(Σₜ₌₁ᵀ αₜ · hₜ(x))
<a id="__codelineno-42-2" name="__codelineno-42-2" href="#__codelineno-42-2"></a>
<a id="__codelineno-42-3" name="__codelineno-42-3" href="#__codelineno-42-3"></a>Es decir, suma ponderada de todos los clasificadores
</code></pre></div><p></p>
<h3 id="ejemplo-paso-a-paso">Ejemplo Paso a Paso<a class="headerlink" href="#ejemplo-paso-a-paso" title="Permanent link">¶</a></h3>
<p>Consideremos un ejemplo simple con 10 puntos:
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-43-1" name="__codelineno-43-1" href="#__codelineno-43-1"></a><span class="c1"># Dataset</span>
<a id="__codelineno-43-2" name="__codelineno-43-2" href="#__codelineno-43-2"></a><span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="n">x4</span><span class="p">,</span> <span class="n">x5</span><span class="p">,</span> <span class="n">x6</span><span class="p">,</span> <span class="n">x7</span><span class="p">,</span> <span class="n">x8</span><span class="p">,</span> <span class="n">x9</span><span class="p">,</span> <span class="n">x10</span><span class="p">]</span>
<a id="__codelineno-43-3" name="__codelineno-43-3" href="#__codelineno-43-3"></a><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">]</span>
<a id="__codelineno-43-4" name="__codelineno-43-4" href="#__codelineno-43-4"></a>
<a id="__codelineno-43-5" name="__codelineno-43-5" href="#__codelineno-43-5"></a><span class="c1"># Iteración 1:</span>
<a id="__codelineno-43-6" name="__codelineno-43-6" href="#__codelineno-43-6"></a><span class="c1"># Pesos iniciales: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]</span>
<a id="__codelineno-43-7" name="__codelineno-43-7" href="#__codelineno-43-7"></a><span class="c1"># Clasificador débil h₁: clasifica bien el 70% (ε₁ = 0.3)</span>
<a id="__codelineno-43-8" name="__codelineno-43-8" href="#__codelineno-43-8"></a><span class="c1"># α₁ = 0.5 * ln((1-0.3)/0.3) = 0.42</span>
<a id="__codelineno-43-9" name="__codelineno-43-9" href="#__codelineno-43-9"></a><span class="c1"># </span>
<a id="__codelineno-43-10" name="__codelineno-43-10" href="#__codelineno-43-10"></a><span class="c1"># Errores en: x4, x7, x9</span>
<a id="__codelineno-43-11" name="__codelineno-43-11" href="#__codelineno-43-11"></a><span class="c1"># Nuevos pesos (normalizados):</span>
<a id="__codelineno-43-12" name="__codelineno-43-12" href="#__codelineno-43-12"></a><span class="c1"># Correctos: peso disminuye a ~0.07</span>
<a id="__codelineno-43-13" name="__codelineno-43-13" href="#__codelineno-43-13"></a><span class="c1"># Incorrectos: peso aumenta a ~0.17</span>
<a id="__codelineno-43-14" name="__codelineno-43-14" href="#__codelineno-43-14"></a>
<a id="__codelineno-43-15" name="__codelineno-43-15" href="#__codelineno-43-15"></a><span class="c1"># Iteración 2:</span>
<a id="__codelineno-43-16" name="__codelineno-43-16" href="#__codelineno-43-16"></a><span class="c1"># Ahora h₂ se entrena dando más importancia a x4, x7, x9</span>
<a id="__codelineno-43-17" name="__codelineno-43-17" href="#__codelineno-43-17"></a><span class="c1"># h₂ clasifica bien el 80% en datos ponderados</span>
<a id="__codelineno-43-18" name="__codelineno-43-18" href="#__codelineno-43-18"></a><span class="c1"># α₂ = 0.5 * ln((1-0.2)/0.2) = 0.69</span>
<a id="__codelineno-43-19" name="__codelineno-43-19" href="#__codelineno-43-19"></a><span class="c1"># ...</span>
<a id="__codelineno-43-20" name="__codelineno-43-20" href="#__codelineno-43-20"></a>
<a id="__codelineno-43-21" name="__codelineno-43-21" href="#__codelineno-43-21"></a><span class="c1"># Predicción final:</span>
<a id="__codelineno-43-22" name="__codelineno-43-22" href="#__codelineno-43-22"></a><span class="c1"># H(x) = sign(0.42*h₁(x) + 0.69*h₂(x) + 0.54*h₃(x) + ...)</span>
</code></pre></div><p></p>
<h3 id="propiedades-importantes">Propiedades Importantes<a class="headerlink" href="#propiedades-importantes" title="Permanent link">¶</a></h3>
<h4 id="training-error-bound">Training Error Bound<a class="headerlink" href="#training-error-bound" title="Permanent link">¶</a></h4>
<p>AdaBoost tiene una garantía teórica sobre el error de entrenamiento:</p>
<div class="arithmatex">\[\text{Training Error} \leq \prod_{t=1}^{T} \sqrt{\epsilon_t(1-\epsilon_t)} \leq \exp\left(-2\sum_{t=1}^{T}\gamma_t^2\right)\]</div>
<p>Donde <span class="arithmatex">\(\gamma_t = 0.5 - \epsilon_t\)</span> es el "margen" del clasificador débil.</p>
<p><strong>Implicación:</strong> Si cada clasificador débil es solo ligeramente mejor que el azar (<span class="arithmatex">\(\epsilon_t &lt; 0.5\)</span>), el error de entrenamiento converge exponencialmente rápido a 0.</p>
<h4 id="no-requiere-conocer-t-de-antemano">No requiere conocer εₜ de antemano<a class="headerlink" href="#no-requiere-conocer-t-de-antemano" title="Permanent link">¶</a></h4>
<p>A diferencia de otros métodos, AdaBoost:
- No necesita que especifiques qué tan "débil" es tu clasificador
- Se adapta automáticamente según el error observado
- Funciona con cualquier clasificador débil válido</p>
<h3 id="implementacion-en-scikit-learn_1">Implementación en scikit-learn<a class="headerlink" href="#implementacion-en-scikit-learn_1" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-44-1" name="__codelineno-44-1" href="#__codelineno-44-1"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
<a id="__codelineno-44-2" name="__codelineno-44-2" href="#__codelineno-44-2"></a><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<a id="__codelineno-44-3" name="__codelineno-44-3" href="#__codelineno-44-3"></a>
<a id="__codelineno-44-4" name="__codelineno-44-4" href="#__codelineno-44-4"></a><span class="c1"># AdaBoost con decision stumps (default)</span>
<a id="__codelineno-44-5" name="__codelineno-44-5" href="#__codelineno-44-5"></a><span class="n">ada</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span>
<a id="__codelineno-44-6" name="__codelineno-44-6" href="#__codelineno-44-6"></a>    <span class="n">estimator</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># Decision stump</span>
<a id="__codelineno-44-7" name="__codelineno-44-7" href="#__codelineno-44-7"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>         <span class="c1"># Número de clasificadores débiles</span>
<a id="__codelineno-44-8" name="__codelineno-44-8" href="#__codelineno-44-8"></a>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>       <span class="c1"># Peso en la actualización (default=1.0)</span>
<a id="__codelineno-44-9" name="__codelineno-44-9" href="#__codelineno-44-9"></a>    <span class="n">algorithm</span><span class="o">=</span><span class="s1">'SAMME.R'</span><span class="p">,</span>     <span class="c1"># SAMME.R usa probabilidades (mejor)</span>
<a id="__codelineno-44-10" name="__codelineno-44-10" href="#__codelineno-44-10"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-44-11" name="__codelineno-44-11" href="#__codelineno-44-11"></a><span class="p">)</span>
<a id="__codelineno-44-12" name="__codelineno-44-12" href="#__codelineno-44-12"></a>
<a id="__codelineno-44-13" name="__codelineno-44-13" href="#__codelineno-44-13"></a><span class="n">ada</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-44-14" name="__codelineno-44-14" href="#__codelineno-44-14"></a>
<a id="__codelineno-44-15" name="__codelineno-44-15" href="#__codelineno-44-15"></a><span class="c1"># Inspeccionar clasificadores y pesos</span>
<a id="__codelineno-44-16" name="__codelineno-44-16" href="#__codelineno-44-16"></a><span class="nb">print</span><span class="p">(</span><span class="s2">"Pesos de clasificadores:"</span><span class="p">,</span> <span class="n">ada</span><span class="o">.</span><span class="n">estimator_weights_</span><span class="p">)</span>
<a id="__codelineno-44-17" name="__codelineno-44-17" href="#__codelineno-44-17"></a><span class="nb">print</span><span class="p">(</span><span class="s2">"Errores de clasificadores:"</span><span class="p">,</span> <span class="n">ada</span><span class="o">.</span><span class="n">estimator_errors_</span><span class="p">)</span>
</code></pre></div>
<h3 id="variantes-de-adaboost">Variantes de AdaBoost<a class="headerlink" href="#variantes-de-adaboost" title="Permanent link">¶</a></h3>
<h4 id="samme-stagewise-additive-modeling-using-a-multiclass-exponential-loss">SAMME (Stagewise Additive Modeling using a Multiclass Exponential loss)<a class="headerlink" href="#samme-stagewise-additive-modeling-using-a-multiclass-exponential-loss" title="Permanent link">¶</a></h4>
<p>Generalización de AdaBoost para más de 2 clases:</p>
<div class="arithmatex">\[\alpha_t = \ln\left(\frac{1-\epsilon_t}{\epsilon_t}\right) + \ln(K-1)\]</div>
<p>Donde K es el número de clases.</p>
<h4 id="sammer-real-valued-predictions">SAMME.R (Real-valued predictions)<a class="headerlink" href="#sammer-real-valued-predictions" title="Permanent link">¶</a></h4>
<p>Usa probabilidades en lugar de predicciones discretas:
- Más suave y generalmente mejor rendimiento
- Requiere que el clasificador base tenga <code>predict_proba</code>
- Es el default en scikit-learn</p>
<h3 id="ventajas-de-adaboost">Ventajas de AdaBoost<a class="headerlink" href="#ventajas-de-adaboost" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Simple conceptualmente:</strong> Fácil de entender e implementar</li>
<li><strong>Garantías teóricas fuertes:</strong> Bounds en error de entrenamiento</li>
<li><strong>Flexible:</strong> Funciona con cualquier clasificador débil</li>
<li><strong>Poco tuning:</strong> Pocos hiperparámetros</li>
<li><strong>No requiere conocer ε:</strong> Se adapta automáticamente</li>
<li><strong>Efectivo:</strong> Puede alcanzar alta accuracy con clasificadores muy simples</li>
</ul>
<h3 id="limitaciones-de-adaboost">Limitaciones de AdaBoost<a class="headerlink" href="#limitaciones-de-adaboost" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Sensible a ruido y outliers:</strong> Les da mucho peso</li>
<li><strong>Sensible a overfitting:</strong> Con muchas iteraciones</li>
<li><strong>No paralelizable:</strong> Los clasificadores deben entrenarse secuencialmente</li>
<li><strong>Puede ser lento:</strong> Con clasificadores débiles lentos</li>
<li><strong>Requiere clasificadores débiles apropiados:</strong> No funciona con clasificadores peores que el azar</li>
</ul>
<h3 id="cuando-usar-adaboost">Cuándo usar AdaBoost<a class="headerlink" href="#cuando-usar-adaboost" title="Permanent link">¶</a></h3>
<p><strong>Usar AdaBoost cuando:</strong>
- ✅ Tienes un <strong>clasificador débil</strong> bueno y rápido
- ✅ Dataset <strong>limpio</strong> (poco ruido)
- ✅ Quieres entender <strong>boosting conceptualmente</strong>
- ✅ Clasificación binaria o multiclase simple
- ✅ Dataset pequeño-mediano</p>
<p><strong>Considerar alternativas cuando:</strong>
- ❌ Dataset con mucho <strong>ruido/outliers</strong> → Gradient Boosting más robusto
- ❌ Buscas <strong>máximo rendimiento</strong> → XGBoost/LightGBM
- ❌ Dataset muy grande → LightGBM más eficiente
- ❌ Muchas features categóricas → CatBoost</p>
<h3 id="ejemplo-completo_1">Ejemplo Completo<a class="headerlink" href="#ejemplo-completo_1" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-45-1" name="__codelineno-45-1" href="#__codelineno-45-1"></a><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<a id="__codelineno-45-2" name="__codelineno-45-2" href="#__codelineno-45-2"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">learning_curve</span>
<a id="__codelineno-45-3" name="__codelineno-45-3" href="#__codelineno-45-3"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
<a id="__codelineno-45-4" name="__codelineno-45-4" href="#__codelineno-45-4"></a><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<a id="__codelineno-45-5" name="__codelineno-45-5" href="#__codelineno-45-5"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-45-6" name="__codelineno-45-6" href="#__codelineno-45-6"></a><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<a id="__codelineno-45-7" name="__codelineno-45-7" href="#__codelineno-45-7"></a>
<a id="__codelineno-45-8" name="__codelineno-45-8" href="#__codelineno-45-8"></a><span class="c1"># Generar datos</span>
<a id="__codelineno-45-9" name="__codelineno-45-9" href="#__codelineno-45-9"></a><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
<a id="__codelineno-45-10" name="__codelineno-45-10" href="#__codelineno-45-10"></a>                          <span class="n">n_redundant</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-45-11" name="__codelineno-45-11" href="#__codelineno-45-11"></a><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-45-12" name="__codelineno-45-12" href="#__codelineno-45-12"></a>
<a id="__codelineno-45-13" name="__codelineno-45-13" href="#__codelineno-45-13"></a><span class="c1"># Decision stump individual</span>
<a id="__codelineno-45-14" name="__codelineno-45-14" href="#__codelineno-45-14"></a><span class="n">stump</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-45-15" name="__codelineno-45-15" href="#__codelineno-45-15"></a><span class="n">stump</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-45-16" name="__codelineno-45-16" href="#__codelineno-45-16"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Decision stump: </span><span class="si">{</span><span class="n">stump</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-45-17" name="__codelineno-45-17" href="#__codelineno-45-17"></a>
<a id="__codelineno-45-18" name="__codelineno-45-18" href="#__codelineno-45-18"></a><span class="c1"># AdaBoost con stumps</span>
<a id="__codelineno-45-19" name="__codelineno-45-19" href="#__codelineno-45-19"></a><span class="n">ada</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span>
<a id="__codelineno-45-20" name="__codelineno-45-20" href="#__codelineno-45-20"></a>    <span class="n">estimator</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
<a id="__codelineno-45-21" name="__codelineno-45-21" href="#__codelineno-45-21"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-45-22" name="__codelineno-45-22" href="#__codelineno-45-22"></a>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<a id="__codelineno-45-23" name="__codelineno-45-23" href="#__codelineno-45-23"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-45-24" name="__codelineno-45-24" href="#__codelineno-45-24"></a><span class="p">)</span>
<a id="__codelineno-45-25" name="__codelineno-45-25" href="#__codelineno-45-25"></a><span class="n">ada</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-45-26" name="__codelineno-45-26" href="#__codelineno-45-26"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"AdaBoost (100 stumps): </span><span class="si">{</span><span class="n">ada</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-45-27" name="__codelineno-45-27" href="#__codelineno-45-27"></a>
<a id="__codelineno-45-28" name="__codelineno-45-28" href="#__codelineno-45-28"></a><span class="c1"># Analizar evolución del error</span>
<a id="__codelineno-45-29" name="__codelineno-45-29" href="#__codelineno-45-29"></a><span class="n">train_errors</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-45-30" name="__codelineno-45-30" href="#__codelineno-45-30"></a><span class="n">test_errors</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-45-31" name="__codelineno-45-31" href="#__codelineno-45-31"></a><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">):</span>
<a id="__codelineno-45-32" name="__codelineno-45-32" href="#__codelineno-45-32"></a>    <span class="n">ada_temp</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span>
<a id="__codelineno-45-33" name="__codelineno-45-33" href="#__codelineno-45-33"></a>        <span class="n">estimator</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
<a id="__codelineno-45-34" name="__codelineno-45-34" href="#__codelineno-45-34"></a>        <span class="n">n_estimators</span><span class="o">=</span><span class="n">n</span><span class="p">,</span>
<a id="__codelineno-45-35" name="__codelineno-45-35" href="#__codelineno-45-35"></a>        <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-45-36" name="__codelineno-45-36" href="#__codelineno-45-36"></a>    <span class="p">)</span>
<a id="__codelineno-45-37" name="__codelineno-45-37" href="#__codelineno-45-37"></a>    <span class="n">ada_temp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-45-38" name="__codelineno-45-38" href="#__codelineno-45-38"></a>    <span class="n">train_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">ada_temp</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
<a id="__codelineno-45-39" name="__codelineno-45-39" href="#__codelineno-45-39"></a>    <span class="n">test_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">ada_temp</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<a id="__codelineno-45-40" name="__codelineno-45-40" href="#__codelineno-45-40"></a>
<a id="__codelineno-45-41" name="__codelineno-45-41" href="#__codelineno-45-41"></a><span class="c1"># Visualizar</span>
<a id="__codelineno-45-42" name="__codelineno-45-42" href="#__codelineno-45-42"></a><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<a id="__codelineno-45-43" name="__codelineno-45-43" href="#__codelineno-45-43"></a><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">),</span> <span class="n">train_errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Train Error'</span><span class="p">)</span>
<a id="__codelineno-45-44" name="__codelineno-45-44" href="#__codelineno-45-44"></a><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">),</span> <span class="n">test_errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Test Error'</span><span class="p">)</span>
<a id="__codelineno-45-45" name="__codelineno-45-45" href="#__codelineno-45-45"></a><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Número de clasificadores'</span><span class="p">)</span>
<a id="__codelineno-45-46" name="__codelineno-45-46" href="#__codelineno-45-46"></a><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Error'</span><span class="p">)</span>
<a id="__codelineno-45-47" name="__codelineno-45-47" href="#__codelineno-45-47"></a><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<a id="__codelineno-45-48" name="__codelineno-45-48" href="#__codelineno-45-48"></a><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Evolución del error en AdaBoost'</span><span class="p">)</span>
<a id="__codelineno-45-49" name="__codelineno-45-49" href="#__codelineno-45-49"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<h3 id="comparacion-adaboost-vs-bagging">Comparación: AdaBoost vs Bagging<a class="headerlink" href="#comparacion-adaboost-vs-bagging" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-46-1" name="__codelineno-46-1" href="#__codelineno-46-1"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span><span class="p">,</span> <span class="n">AdaBoostClassifier</span>
<a id="__codelineno-46-2" name="__codelineno-46-2" href="#__codelineno-46-2"></a>
<a id="__codelineno-46-3" name="__codelineno-46-3" href="#__codelineno-46-3"></a><span class="c1"># Bagging de stumps</span>
<a id="__codelineno-46-4" name="__codelineno-46-4" href="#__codelineno-46-4"></a><span class="n">bagging_stumps</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span>
<a id="__codelineno-46-5" name="__codelineno-46-5" href="#__codelineno-46-5"></a>    <span class="n">estimator</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
<a id="__codelineno-46-6" name="__codelineno-46-6" href="#__codelineno-46-6"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-46-7" name="__codelineno-46-7" href="#__codelineno-46-7"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-46-8" name="__codelineno-46-8" href="#__codelineno-46-8"></a><span class="p">)</span>
<a id="__codelineno-46-9" name="__codelineno-46-9" href="#__codelineno-46-9"></a>
<a id="__codelineno-46-10" name="__codelineno-46-10" href="#__codelineno-46-10"></a><span class="c1"># AdaBoost de stumps</span>
<a id="__codelineno-46-11" name="__codelineno-46-11" href="#__codelineno-46-11"></a><span class="n">adaboost_stumps</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span>
<a id="__codelineno-46-12" name="__codelineno-46-12" href="#__codelineno-46-12"></a>    <span class="n">estimator</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
<a id="__codelineno-46-13" name="__codelineno-46-13" href="#__codelineno-46-13"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-46-14" name="__codelineno-46-14" href="#__codelineno-46-14"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-46-15" name="__codelineno-46-15" href="#__codelineno-46-15"></a><span class="p">)</span>
<a id="__codelineno-46-16" name="__codelineno-46-16" href="#__codelineno-46-16"></a>
<a id="__codelineno-46-17" name="__codelineno-46-17" href="#__codelineno-46-17"></a><span class="n">bagging_stumps</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-46-18" name="__codelineno-46-18" href="#__codelineno-46-18"></a><span class="n">adaboost_stumps</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-46-19" name="__codelineno-46-19" href="#__codelineno-46-19"></a>
<a id="__codelineno-46-20" name="__codelineno-46-20" href="#__codelineno-46-20"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Bagging: </span><span class="si">{</span><span class="n">bagging_stumps</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-46-21" name="__codelineno-46-21" href="#__codelineno-46-21"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"AdaBoost: </span><span class="si">{</span><span class="n">adaboost_stumps</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<p><strong>Resultado típico:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-47-1" name="__codelineno-47-1" href="#__codelineno-47-1"></a>Bagging: 0.78
<a id="__codelineno-47-2" name="__codelineno-47-2" href="#__codelineno-47-2"></a>AdaBoost: 0.92
</code></pre></div><p></p>
<p>AdaBoost supera a Bagging con clasificadores débiles porque:
- Bagging reduce varianza (pero stumps ya tienen baja varianza)
- AdaBoost reduce sesgo (stumps tienen alto sesgo)</p>
<hr>
<h2 id="42-gradient-boosting">4.2. Gradient Boosting<a class="headerlink" href="#42-gradient-boosting" title="Permanent link">¶</a></h2>
<p>Gradient Boosting generaliza la idea de boosting mediante una perspectiva de <strong>optimización numérica</strong>. En lugar de ajustar pesos de ejemplos (como AdaBoost), cada nuevo modelo se ajusta al <strong>gradiente negativo de la función de pérdida</strong>.</p>
<p>Desarrollado por Jerome Friedman (1999-2001), Gradient Boosting es la base teórica de métodos modernos como XGBoost, LightGBM y CatBoost.</p>
<h3 id="marco-conceptual">Marco Conceptual<a class="headerlink" href="#marco-conceptual" title="Permanent link">¶</a></h3>
<p><strong>Idea clave:</strong> Ver boosting como un problema de optimización en el espacio de funciones.</p>
<p>Queremos encontrar una función <span class="arithmatex">\(F(x)\)</span> que minimice la pérdida esperada:</p>
<div class="arithmatex">\[F^* = \arg\min_F \mathbb{E}_{x,y}[L(y, F(x))]\]</div>
<p>Donde <span class="arithmatex">\(L\)</span> es una función de pérdida diferenciable.</p>
<p><strong>Aproximación:</strong> Construir <span class="arithmatex">\(F\)</span> como una suma de funciones más simples:</p>
<div class="arithmatex">\[F(x) = \sum_{m=0}^{M} f_m(x)\]</div>
<p>Donde cada <span class="arithmatex">\(f_m\)</span> es un modelo débil (típicamente un árbol).</p>
<h3 id="analogia-con-gradient-descent">Analogía con Gradient Descent<a class="headerlink" href="#analogia-con-gradient-descent" title="Permanent link">¶</a></h3>
<p><strong>Gradient Descent (optimización en espacio de parámetros):</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-48-1" name="__codelineno-48-1" href="#__codelineno-48-1"></a>θ = θ₀  # Inicialización
<a id="__codelineno-48-2" name="__codelineno-48-2" href="#__codelineno-48-2"></a>for t in range(T):
<a id="__codelineno-48-3" name="__codelineno-48-3" href="#__codelineno-48-3"></a>    g = ∇L(θ)  # Gradiente respecto a parámetros
<a id="__codelineno-48-4" name="__codelineno-48-4" href="#__codelineno-48-4"></a>    θ = θ - η * g  # Actualización
</code></pre></div><p></p>
<p><strong>Gradient Boosting (optimización en espacio de funciones):</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-49-1" name="__codelineno-49-1" href="#__codelineno-49-1"></a>F = F₀  # Inicialización
<a id="__codelineno-49-2" name="__codelineno-49-2" href="#__codelineno-49-2"></a>for m in range(M):
<a id="__codelineno-49-3" name="__codelineno-49-3" href="#__codelineno-49-3"></a>    g = ∇L(F)  # Gradiente respecto a predicciones
<a id="__codelineno-49-4" name="__codelineno-49-4" href="#__codelineno-49-4"></a>    f_m = fit_model(g)  # Ajustar modelo a gradientes
<a id="__codelineno-49-5" name="__codelineno-49-5" href="#__codelineno-49-5"></a>    F = F + η * f_m  # Actualización
</code></pre></div><p></p>
<h3 id="algoritmo-general-de-gradient-boosting">Algoritmo General de Gradient Boosting<a class="headerlink" href="#algoritmo-general-de-gradient-boosting" title="Permanent link">¶</a></h3>
<p><strong>Inicialización:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-50-1" name="__codelineno-50-1" href="#__codelineno-50-1"></a>F₀(x) = arg min_γ Σᵢ L(yᵢ, γ)
<a id="__codelineno-50-2" name="__codelineno-50-2" href="#__codelineno-50-2"></a>
<a id="__codelineno-50-3" name="__codelineno-50-3" href="#__codelineno-50-3"></a>Típicamente:
<a id="__codelineno-50-4" name="__codelineno-50-4" href="#__codelineno-50-4"></a>- Regresión: F₀(x) = mean(y)
<a id="__codelineno-50-5" name="__codelineno-50-5" href="#__codelineno-50-5"></a>- Clasificación binaria: F₀(x) = log(p/(1-p)) donde p = mean(y)
</code></pre></div><p></p>
<p><strong>Para m = 1 hasta M:</strong></p>
<p><strong>Paso 1: Calcular pseudo-residuos (gradientes negativos)</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-51-1" name="__codelineno-51-1" href="#__codelineno-51-1"></a>rᵢₘ = -[∂L(yᵢ, F(xᵢ))/∂F(xᵢ)]_{F=F_{m-1}}
<a id="__codelineno-51-2" name="__codelineno-51-2" href="#__codelineno-51-2"></a>
<a id="__codelineno-51-3" name="__codelineno-51-3" href="#__codelineno-51-3"></a>Estos son los "residuos generalizados"
</code></pre></div><p></p>
<p><strong>Paso 2: Ajustar modelo débil a los residuos</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-52-1" name="__codelineno-52-1" href="#__codelineno-52-1"></a>hₘ = arg min_h Σᵢ (rᵢₘ - h(xᵢ))²
<a id="__codelineno-52-2" name="__codelineno-52-2" href="#__codelineno-52-2"></a>
<a id="__codelineno-52-3" name="__codelineno-52-3" href="#__codelineno-52-3"></a>Típicamente hₘ es un árbol de decisión poco profundo
</code></pre></div><p></p>
<p><strong>Paso 3: Encontrar multiplicador óptimo (line search)</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-53-1" name="__codelineno-53-1" href="#__codelineno-53-1"></a>γₘ = arg min_γ Σᵢ L(yᵢ, F_{m-1}(xᵢ) + γ·hₘ(xᵢ))
<a id="__codelineno-53-2" name="__codelineno-53-2" href="#__codelineno-53-2"></a>
<a id="__codelineno-53-3" name="__codelineno-53-3" href="#__codelineno-53-3"></a>En la práctica, a menudo se fija γₘ = 1
</code></pre></div><p></p>
<p><strong>Paso 4: Actualizar el ensemble</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-54-1" name="__codelineno-54-1" href="#__codelineno-54-1"></a>Fₘ(x) = F_{m-1}(x) + ν·γₘ·hₘ(x)
<a id="__codelineno-54-2" name="__codelineno-54-2" href="#__codelineno-54-2"></a>
<a id="__codelineno-54-3" name="__codelineno-54-3" href="#__codelineno-54-3"></a>Donde ν ∈ (0,1] es el learning rate
</code></pre></div><p></p>
<p><strong>Predicción final:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-55-1" name="__codelineno-55-1" href="#__codelineno-55-1"></a>ŷ = Fₘ(x)
</code></pre></div><p></p>
<h3 id="funciones-de-perdida-comunes">Funciones de Pérdida Comunes<a class="headerlink" href="#funciones-de-perdida-comunes" title="Permanent link">¶</a></h3>
<h4 id="regresion">Regresión<a class="headerlink" href="#regresion" title="Permanent link">¶</a></h4>
<p><strong>1. Squared Error (L2):</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-56-1" name="__codelineno-56-1" href="#__codelineno-56-1"></a>L(y, F) = (y - F)²/2
<a id="__codelineno-56-2" name="__codelineno-56-2" href="#__codelineno-56-2"></a>
<a id="__codelineno-56-3" name="__codelineno-56-3" href="#__codelineno-56-3"></a>Gradiente: ∂L/∂F = F - y
<a id="__codelineno-56-4" name="__codelineno-56-4" href="#__codelineno-56-4"></a>Residuo: r = y - F  (residuo ordinario)
</code></pre></div><p></p>
<p><strong>2. Absolute Error (L1):</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-57-1" name="__codelineno-57-1" href="#__codelineno-57-1"></a>L(y, F) = |y - F|
<a id="__codelineno-57-2" name="__codelineno-57-2" href="#__codelineno-57-2"></a>
<a id="__codelineno-57-3" name="__codelineno-57-3" href="#__codelineno-57-3"></a>Gradiente: ∂L/∂F = sign(F - y)
<a id="__codelineno-57-4" name="__codelineno-57-4" href="#__codelineno-57-4"></a>Residuo: r = sign(y - F)
<a id="__codelineno-57-5" name="__codelineno-57-5" href="#__codelineno-57-5"></a>Más robusto a outliers que L2
</code></pre></div><p></p>
<p><strong>3. Huber Loss:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-58-1" name="__codelineno-58-1" href="#__codelineno-58-1"></a>L(y, F) = {
<a id="__codelineno-58-2" name="__codelineno-58-2" href="#__codelineno-58-2"></a>    (y - F)²/2           si |y - F| ≤ δ
<a id="__codelineno-58-3" name="__codelineno-58-3" href="#__codelineno-58-3"></a>    δ|y - F| - δ²/2      si |y - F| &gt; δ
<a id="__codelineno-58-4" name="__codelineno-58-4" href="#__codelineno-58-4"></a>}
<a id="__codelineno-58-5" name="__codelineno-58-5" href="#__codelineno-58-5"></a>
<a id="__codelineno-58-6" name="__codelineno-58-6" href="#__codelineno-58-6"></a>Combina robustez de L1 cerca de 0 con suavidad de L2
</code></pre></div><p></p>
<p><strong>4. Quantile Loss:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-59-1" name="__codelineno-59-1" href="#__codelineno-59-1"></a>L(y, F) = Σᵢ ρ_α(yᵢ - F(xᵢ))
<a id="__codelineno-59-2" name="__codelineno-59-2" href="#__codelineno-59-2"></a>
<a id="__codelineno-59-3" name="__codelineno-59-3" href="#__codelineno-59-3"></a>Donde ρ_α(u) = u·(α - 𝟙(u &lt; 0))
<a id="__codelineno-59-4" name="__codelineno-59-4" href="#__codelineno-59-4"></a>
<a id="__codelineno-59-5" name="__codelineno-59-5" href="#__codelineno-59-5"></a>Permite predecir cuantiles (ej: mediana con α=0.5)
</code></pre></div><p></p>
<h4 id="clasificacion-binaria">Clasificación Binaria<a class="headerlink" href="#clasificacion-binaria" title="Permanent link">¶</a></h4>
<p><strong>Logistic Loss (Deviance):</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-60-1" name="__codelineno-60-1" href="#__codelineno-60-1"></a>y ∈ {-1, +1}
<a id="__codelineno-60-2" name="__codelineno-60-2" href="#__codelineno-60-2"></a>L(y, F) = log(1 + exp(-2yF))
<a id="__codelineno-60-3" name="__codelineno-60-3" href="#__codelineno-60-3"></a>
<a id="__codelineno-60-4" name="__codelineno-60-4" href="#__codelineno-60-4"></a>Gradiente: ∂L/∂F = -2y/(1 + exp(2yF))
<a id="__codelineno-60-5" name="__codelineno-60-5" href="#__codelineno-60-5"></a>Residuo: r = 2y/(1 + exp(2yF))
<a id="__codelineno-60-6" name="__codelineno-60-6" href="#__codelineno-60-6"></a>
<a id="__codelineno-60-7" name="__codelineno-60-7" href="#__codelineno-60-7"></a>Predicción: P(y=1|x) = 1/(1 + exp(-2F(x)))
</code></pre></div><p></p>
<p><strong>Exponential Loss:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-61-1" name="__codelineno-61-1" href="#__codelineno-61-1"></a>L(y, F) = exp(-yF)
<a id="__codelineno-61-2" name="__codelineno-61-2" href="#__codelineno-61-2"></a>
<a id="__codelineno-61-3" name="__codelineno-61-3" href="#__codelineno-61-3"></a>Gradiente: ∂L/∂F = -y·exp(-yF)
<a id="__codelineno-61-4" name="__codelineno-61-4" href="#__codelineno-61-4"></a>
<a id="__codelineno-61-5" name="__codelineno-61-5" href="#__codelineno-61-5"></a>Nota: Esta es la pérdida que AdaBoost minimiza implícitamente
</code></pre></div><p></p>
<h4 id="clasificacion-multiclase">Clasificación Multiclase<a class="headerlink" href="#clasificacion-multiclase" title="Permanent link">¶</a></h4>
<p><strong>Softmax (Multinomial Deviance):</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-62-1" name="__codelineno-62-1" href="#__codelineno-62-1"></a>K clases, entrenar K funciones F_k(x)
<a id="__codelineno-62-2" name="__codelineno-62-2" href="#__codelineno-62-2"></a>
<a id="__codelineno-62-3" name="__codelineno-62-3" href="#__codelineno-62-3"></a>L(y, F) = -Σₖ yₖ·log(pₖ)
<a id="__codelineno-62-4" name="__codelineno-62-4" href="#__codelineno-62-4"></a>
<a id="__codelineno-62-5" name="__codelineno-62-5" href="#__codelineno-62-5"></a>Donde pₖ = exp(Fₖ)/Σⱼ exp(Fⱼ)
</code></pre></div><p></p>
<h3 id="ejemplo-con-squared-loss">Ejemplo con Squared Loss<a class="headerlink" href="#ejemplo-con-squared-loss" title="Permanent link">¶</a></h3>
<p>Para entender mejor, veamos un ejemplo completo con squared loss:
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-63-1" name="__codelineno-63-1" href="#__codelineno-63-1"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-63-2" name="__codelineno-63-2" href="#__codelineno-63-2"></a>
<a id="__codelineno-63-3" name="__codelineno-63-3" href="#__codelineno-63-3"></a><span class="c1"># Datos sintéticos</span>
<a id="__codelineno-63-4" name="__codelineno-63-4" href="#__codelineno-63-4"></a><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-63-5" name="__codelineno-63-5" href="#__codelineno-63-5"></a><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-63-6" name="__codelineno-63-6" href="#__codelineno-63-6"></a><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<a id="__codelineno-63-7" name="__codelineno-63-7" href="#__codelineno-63-7"></a>
<a id="__codelineno-63-8" name="__codelineno-63-8" href="#__codelineno-63-8"></a><span class="c1"># Inicialización: F₀ = mean(y)</span>
<a id="__codelineno-63-9" name="__codelineno-63-9" href="#__codelineno-63-9"></a><span class="n">F</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<a id="__codelineno-63-10" name="__codelineno-63-10" href="#__codelineno-63-10"></a>
<a id="__codelineno-63-11" name="__codelineno-63-11" href="#__codelineno-63-11"></a><span class="c1"># Parámetros</span>
<a id="__codelineno-63-12" name="__codelineno-63-12" href="#__codelineno-63-12"></a><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<a id="__codelineno-63-13" name="__codelineno-63-13" href="#__codelineno-63-13"></a><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">100</span>
<a id="__codelineno-63-14" name="__codelineno-63-14" href="#__codelineno-63-14"></a>
<a id="__codelineno-63-15" name="__codelineno-63-15" href="#__codelineno-63-15"></a><span class="c1"># Boosting manual</span>
<a id="__codelineno-63-16" name="__codelineno-63-16" href="#__codelineno-63-16"></a><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">):</span>
<a id="__codelineno-63-17" name="__codelineno-63-17" href="#__codelineno-63-17"></a>    <span class="c1"># Paso 1: Calcular residuos</span>
<a id="__codelineno-63-18" name="__codelineno-63-18" href="#__codelineno-63-18"></a>    <span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">F</span>
<a id="__codelineno-63-19" name="__codelineno-63-19" href="#__codelineno-63-19"></a>
<a id="__codelineno-63-20" name="__codelineno-63-20" href="#__codelineno-63-20"></a>    <span class="c1"># Paso 2: Ajustar árbol a residuos</span>
<a id="__codelineno-63-21" name="__codelineno-63-21" href="#__codelineno-63-21"></a>    <span class="c1"># (aquí simplificado, en práctica se usa un árbol real)</span>
<a id="__codelineno-63-22" name="__codelineno-63-22" href="#__codelineno-63-22"></a>    <span class="n">tree</span> <span class="o">=</span> <span class="n">fit_tree</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">residuals</span><span class="p">)</span>  <span class="c1"># Árbol de profundidad limitada</span>
<a id="__codelineno-63-23" name="__codelineno-63-23" href="#__codelineno-63-23"></a>
<a id="__codelineno-63-24" name="__codelineno-63-24" href="#__codelineno-63-24"></a>    <span class="c1"># Paso 3: Predecir con el nuevo árbol</span>
<a id="__codelineno-63-25" name="__codelineno-63-25" href="#__codelineno-63-25"></a>    <span class="n">predictions</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<a id="__codelineno-63-26" name="__codelineno-63-26" href="#__codelineno-63-26"></a>
<a id="__codelineno-63-27" name="__codelineno-63-27" href="#__codelineno-63-27"></a>    <span class="c1"># Paso 4: Actualizar F</span>
<a id="__codelineno-63-28" name="__codelineno-63-28" href="#__codelineno-63-28"></a>    <span class="n">F</span> <span class="o">=</span> <span class="n">F</span> <span class="o">+</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">predictions</span>
<a id="__codelineno-63-29" name="__codelineno-63-29" href="#__codelineno-63-29"></a>
<a id="__codelineno-63-30" name="__codelineno-63-30" href="#__codelineno-63-30"></a>    <span class="k">if</span> <span class="n">m</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-63-31" name="__codelineno-63-31" href="#__codelineno-63-31"></a>        <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">F</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-63-32" name="__codelineno-63-32" href="#__codelineno-63-32"></a>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Iteración </span><span class="si">{</span><span class="n">m</span><span class="si">}</span><span class="s2">: MSE = </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div><p></p>
<h3 id="learning-rate-shrinkage">Learning Rate (Shrinkage)<a class="headerlink" href="#learning-rate-shrinkage" title="Permanent link">¶</a></h3>
<p>El learning rate <span class="arithmatex">\(\nu\)</span> controla la contribución de cada árbol:</p>
<div class="arithmatex">\[F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)\]</div>
<p><strong>Efecto:</strong>
- <strong>ν grande (ej: 1.0):</strong> Convergencia rápida, riesgo de overfitting
- <strong>ν pequeño (ej: 0.01-0.1):</strong> Convergencia lenta, mejor generalización</p>
<p><strong>Trade-off learning rate vs número de árboles:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-64-1" name="__codelineno-64-1" href="#__codelineno-64-1"></a>ν = 1.0 → 50 árboles suficientes, pero puede overfit
<a id="__codelineno-64-2" name="__codelineno-64-2" href="#__codelineno-64-2"></a>ν = 0.1 → 500 árboles necesarios, mejor generalización
<a id="__codelineno-64-3" name="__codelineno-64-3" href="#__codelineno-64-3"></a>ν = 0.01 → 5000 árboles necesarios, excelente generalización
</code></pre></div><p></p>
<p><strong>Recomendación práctica:</strong>
- Usar ν pequeño (0.01-0.1)
- Aumentar n_estimators proporcionalmente
- Usar early stopping para encontrar el número óptimo</p>
<h3 id="arboles-de-decision-en-gradient-boosting">Árboles de Decisión en Gradient Boosting<a class="headerlink" href="#arboles-de-decision-en-gradient-boosting" title="Permanent link">¶</a></h3>
<p><strong>Características típicas:</strong>
- <strong>Poco profundos:</strong> max_depth = 3-8 (vs Random Forest: sin límite)
- <strong>Capturam interacciones:</strong> Profundidad k → interacciones de orden k
- <strong>Rápidos de entrenar:</strong> Árboles pequeños
- <strong>Complementarios:</strong> Cada árbol captura un patrón residual diferente</p>
<p><strong>Ejemplo:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-65-1" name="__codelineno-65-1" href="#__codelineno-65-1"></a><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<a id="__codelineno-65-2" name="__codelineno-65-2" href="#__codelineno-65-2"></a>
<a id="__codelineno-65-3" name="__codelineno-65-3" href="#__codelineno-65-3"></a><span class="c1"># Árbol típico en Gradient Boosting</span>
<a id="__codelineno-65-4" name="__codelineno-65-4" href="#__codelineno-65-4"></a><span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span>
<a id="__codelineno-65-5" name="__codelineno-65-5" href="#__codelineno-65-5"></a>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>           <span class="c1"># Poco profundo</span>
<a id="__codelineno-65-6" name="__codelineno-65-6" href="#__codelineno-65-6"></a>    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>  <span class="c1"># Evitar splits en pocas muestras</span>
<a id="__codelineno-65-7" name="__codelineno-65-7" href="#__codelineno-65-7"></a>    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">10</span>    <span class="c1"># Hojas razonablemente grandes</span>
<a id="__codelineno-65-8" name="__codelineno-65-8" href="#__codelineno-65-8"></a><span class="p">)</span>
</code></pre></div><p></p>
<h3 id="implementacion-en-scikit-learn_2">Implementación en scikit-learn<a class="headerlink" href="#implementacion-en-scikit-learn_2" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-66-1" name="__codelineno-66-1" href="#__codelineno-66-1"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span><span class="p">,</span> <span class="n">GradientBoostingRegressor</span>
<a id="__codelineno-66-2" name="__codelineno-66-2" href="#__codelineno-66-2"></a>
<a id="__codelineno-66-3" name="__codelineno-66-3" href="#__codelineno-66-3"></a><span class="c1"># Gradient Boosting para clasificación</span>
<a id="__codelineno-66-4" name="__codelineno-66-4" href="#__codelineno-66-4"></a><span class="n">gb_clf</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-66-5" name="__codelineno-66-5" href="#__codelineno-66-5"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>         <span class="c1"># Número de árboles</span>
<a id="__codelineno-66-6" name="__codelineno-66-6" href="#__codelineno-66-6"></a>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>        <span class="c1"># Shrinkage</span>
<a id="__codelineno-66-7" name="__codelineno-66-7" href="#__codelineno-66-7"></a>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>              <span class="c1"># Profundidad de árboles</span>
<a id="__codelineno-66-8" name="__codelineno-66-8" href="#__codelineno-66-8"></a>    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>     <span class="c1"># Mínimo para split</span>
<a id="__codelineno-66-9" name="__codelineno-66-9" href="#__codelineno-66-9"></a>    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>      <span class="c1"># Mínimo en hojas</span>
<a id="__codelineno-66-10" name="__codelineno-66-10" href="#__codelineno-66-10"></a>    <span class="n">subsample</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>            <span class="c1"># Stochastic GB (fracción de muestras)</span>
<a id="__codelineno-66-11" name="__codelineno-66-11" href="#__codelineno-66-11"></a>    <span class="n">max_features</span><span class="o">=</span><span class="s1">'sqrt'</span><span class="p">,</span>      <span class="c1"># Features por split</span>
<a id="__codelineno-66-12" name="__codelineno-66-12" href="#__codelineno-66-12"></a>    <span class="n">loss</span><span class="o">=</span><span class="s1">'log_loss'</span><span class="p">,</span>          <span class="c1"># Función de pérdida (default)</span>
<a id="__codelineno-66-13" name="__codelineno-66-13" href="#__codelineno-66-13"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-66-14" name="__codelineno-66-14" href="#__codelineno-66-14"></a><span class="p">)</span>
<a id="__codelineno-66-15" name="__codelineno-66-15" href="#__codelineno-66-15"></a>
<a id="__codelineno-66-16" name="__codelineno-66-16" href="#__codelineno-66-16"></a><span class="n">gb_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-66-17" name="__codelineno-66-17" href="#__codelineno-66-17"></a>
<a id="__codelineno-66-18" name="__codelineno-66-18" href="#__codelineno-66-18"></a><span class="c1"># Gradient Boosting para regresión</span>
<a id="__codelineno-66-19" name="__codelineno-66-19" href="#__codelineno-66-19"></a><span class="n">gb_reg</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span>
<a id="__codelineno-66-20" name="__codelineno-66-20" href="#__codelineno-66-20"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-66-21" name="__codelineno-66-21" href="#__codelineno-66-21"></a>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
<a id="__codelineno-66-22" name="__codelineno-66-22" href="#__codelineno-66-22"></a>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<a id="__codelineno-66-23" name="__codelineno-66-23" href="#__codelineno-66-23"></a>    <span class="n">loss</span><span class="o">=</span><span class="s1">'squared_error'</span><span class="p">,</span>     <span class="c1"># Para regresión (default)</span>
<a id="__codelineno-66-24" name="__codelineno-66-24" href="#__codelineno-66-24"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-66-25" name="__codelineno-66-25" href="#__codelineno-66-25"></a><span class="p">)</span>
<a id="__codelineno-66-26" name="__codelineno-66-26" href="#__codelineno-66-26"></a>
<a id="__codelineno-66-27" name="__codelineno-66-27" href="#__codelineno-66-27"></a><span class="n">gb_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<h3 id="stochastic-gradient-boosting">Stochastic Gradient Boosting<a class="headerlink" href="#stochastic-gradient-boosting" title="Permanent link">¶</a></h3>
<p>Introducido por Friedman (1999), añade subsample de datos en cada iteración:
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-67-1" name="__codelineno-67-1" href="#__codelineno-67-1"></a>Para m = 1 hasta M:
<a id="__codelineno-67-2" name="__codelineno-67-2" href="#__codelineno-67-2"></a>    1. Muestrear fracción f de datos (sin reemplazo)
<a id="__codelineno-67-3" name="__codelineno-67-3" href="#__codelineno-67-3"></a>    2. Calcular residuos en subsample
<a id="__codelineno-67-4" name="__codelineno-67-4" href="#__codelineno-67-4"></a>    3. Entrenar árbol en subsample
<a id="__codelineno-67-5" name="__codelineno-67-5" href="#__codelineno-67-5"></a>    4. Actualizar F en todo el dataset
</code></pre></div><p></p>
<p><strong>Ventajas:</strong>
- ✅ Reduce overfitting (más regularización)
- ✅ Más rápido (menos datos por árbol)
- ✅ Introduce diversidad entre árboles</p>
<p><strong>Subsample típico:</strong> 0.5-0.8
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-68-1" name="__codelineno-68-1" href="#__codelineno-68-1"></a><span class="n">gb</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-68-2" name="__codelineno-68-2" href="#__codelineno-68-2"></a>    <span class="n">subsample</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>  <span class="c1"># Usar 80% de datos por árbol</span>
<a id="__codelineno-68-3" name="__codelineno-68-3" href="#__codelineno-68-3"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-68-4" name="__codelineno-68-4" href="#__codelineno-68-4"></a>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span>
<a id="__codelineno-68-5" name="__codelineno-68-5" href="#__codelineno-68-5"></a><span class="p">)</span>
</code></pre></div><p></p>
<h3 id="hiperparametros-importantes_1">Hiperparámetros Importantes<a class="headerlink" href="#hiperparametros-importantes_1" title="Permanent link">¶</a></h3>
<h4 id="control-de-complejidad">Control de complejidad:<a class="headerlink" href="#control-de-complejidad" title="Permanent link">¶</a></h4>
<p><strong>1. n_estimators:</strong> Número de árboles
- Más árboles → más capacidad
- Usar con learning_rate bajo y early stopping
- Típico: 100-1000</p>
<p><strong>2. learning_rate:</strong> Tasa de aprendizaje
- Controla contribución de cada árbol
- Típico: 0.01-0.1
- Trade-off con n_estimators</p>
<p><strong>3. max_depth:</strong> Profundidad máxima de árboles
- Controla interacciones capturadas
- Típico: 3-8
- Profundidad k → interacciones de orden k</p>
<p><strong>4. min_samples_split / min_samples_leaf:</strong>
- Previene splits en pocas muestras
- Regularización importante
- Típico: 20/10</p>
<p><strong>5. subsample:</strong> Fracción de muestras por árbol
- Stochastic GB
- Típico: 0.8
- Reduce overfitting</p>
<p><strong>6. max_features:</strong> Features consideradas por split
- Similar a Random Forest
- Típico: 'sqrt' o None
- Añade diversidad</p>
<h3 id="early-stopping">Early Stopping<a class="headerlink" href="#early-stopping" title="Permanent link">¶</a></h3>
<p>Detener el entrenamiento cuando el rendimiento en validación deja de mejorar:
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-69-1" name="__codelineno-69-1" href="#__codelineno-69-1"></a><span class="n">gb</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-69-2" name="__codelineno-69-2" href="#__codelineno-69-2"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>        <span class="c1"># Número grande</span>
<a id="__codelineno-69-3" name="__codelineno-69-3" href="#__codelineno-69-3"></a>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>       <span class="c1"># Learning rate bajo</span>
<a id="__codelineno-69-4" name="__codelineno-69-4" href="#__codelineno-69-4"></a>    <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>  <span class="c1"># Fracción para validación</span>
<a id="__codelineno-69-5" name="__codelineno-69-5" href="#__codelineno-69-5"></a>    <span class="n">n_iter_no_change</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>      <span class="c1"># Parar si no mejora en 50 iteraciones</span>
<a id="__codelineno-69-6" name="__codelineno-69-6" href="#__codelineno-69-6"></a>    <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>                 <span class="c1"># Tolerancia de mejora</span>
<a id="__codelineno-69-7" name="__codelineno-69-7" href="#__codelineno-69-7"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-69-8" name="__codelineno-69-8" href="#__codelineno-69-8"></a><span class="p">)</span>
<a id="__codelineno-69-9" name="__codelineno-69-9" href="#__codelineno-69-9"></a>
<a id="__codelineno-69-10" name="__codelineno-69-10" href="#__codelineno-69-10"></a><span class="n">gb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-69-11" name="__codelineno-69-11" href="#__codelineno-69-11"></a>
<a id="__codelineno-69-12" name="__codelineno-69-12" href="#__codelineno-69-12"></a><span class="c1"># Número óptimo de árboles encontrado</span>
<a id="__codelineno-69-13" name="__codelineno-69-13" href="#__codelineno-69-13"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Número óptimo de árboles: </span><span class="si">{</span><span class="n">gb</span><span class="o">.</span><span class="n">n_estimators_</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div><p></p>
<h3 id="ventajas-de-gradient-boosting">Ventajas de Gradient Boosting<a class="headerlink" href="#ventajas-de-gradient-boosting" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Muy efectivo:</strong> Estado del arte en muchos problemas</li>
<li><strong>Flexible:</strong> Cualquier función de pérdida diferenciable</li>
<li><strong>Reduce sesgo y varianza:</strong> Mejor que bagging o boosting básico</li>
<li><strong>Maneja features mixtas:</strong> Numéricas y categóricas</li>
<li><strong>Robusto:</strong> Con funciones de pérdida apropiadas</li>
<li><strong>Interpretable:</strong> Feature importance disponible</li>
</ul>
<h3 id="limitaciones-de-gradient-boosting">Limitaciones de Gradient Boosting<a class="headerlink" href="#limitaciones-de-gradient-boosting" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Propenso a overfitting:</strong> Requiere tuning cuidadoso</li>
<li><strong>No paralelizable:</strong> Entrenamiento secuencial</li>
<li><strong>Lento:</strong> Comparado con Random Forest</li>
<li><strong>Sensible a hiperparámetros:</strong> Más que Random Forest</li>
<li><strong>Sensible a outliers:</strong> Con squared loss</li>
</ul>
<h3 id="prevencion-de-overfitting">Prevención de Overfitting<a class="headerlink" href="#prevencion-de-overfitting" title="Permanent link">¶</a></h3>
<p>Estrategias múltiples:
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-70-1" name="__codelineno-70-1" href="#__codelineno-70-1"></a><span class="n">gb</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-70-2" name="__codelineno-70-2" href="#__codelineno-70-2"></a>    <span class="c1"># 1. Reduce learning rate, aumenta árboles</span>
<a id="__codelineno-70-3" name="__codelineno-70-3" href="#__codelineno-70-3"></a>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
<a id="__codelineno-70-4" name="__codelineno-70-4" href="#__codelineno-70-4"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<a id="__codelineno-70-5" name="__codelineno-70-5" href="#__codelineno-70-5"></a>
<a id="__codelineno-70-6" name="__codelineno-70-6" href="#__codelineno-70-6"></a>    <span class="c1"># 2. Limita complejidad de árboles</span>
<a id="__codelineno-70-7" name="__codelineno-70-7" href="#__codelineno-70-7"></a>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<a id="__codelineno-70-8" name="__codelineno-70-8" href="#__codelineno-70-8"></a>    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<a id="__codelineno-70-9" name="__codelineno-70-9" href="#__codelineno-70-9"></a>    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<a id="__codelineno-70-10" name="__codelineno-70-10" href="#__codelineno-70-10"></a>
<a id="__codelineno-70-11" name="__codelineno-70-11" href="#__codelineno-70-11"></a>    <span class="c1"># 3. Stochastic GB</span>
<a id="__codelineno-70-12" name="__codelineno-70-12" href="#__codelineno-70-12"></a>    <span class="n">subsample</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
<a id="__codelineno-70-13" name="__codelineno-70-13" href="#__codelineno-70-13"></a>    <span class="n">max_features</span><span class="o">=</span><span class="s1">'sqrt'</span><span class="p">,</span>
<a id="__codelineno-70-14" name="__codelineno-70-14" href="#__codelineno-70-14"></a>
<a id="__codelineno-70-15" name="__codelineno-70-15" href="#__codelineno-70-15"></a>    <span class="c1"># 4. Early stopping</span>
<a id="__codelineno-70-16" name="__codelineno-70-16" href="#__codelineno-70-16"></a>    <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
<a id="__codelineno-70-17" name="__codelineno-70-17" href="#__codelineno-70-17"></a>    <span class="n">n_iter_no_change</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
<a id="__codelineno-70-18" name="__codelineno-70-18" href="#__codelineno-70-18"></a>
<a id="__codelineno-70-19" name="__codelineno-70-19" href="#__codelineno-70-19"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-70-20" name="__codelineno-70-20" href="#__codelineno-70-20"></a><span class="p">)</span>
</code></pre></div><p></p>
<h3 id="comparacion-adaboost-vs-gradient-boosting">Comparación: AdaBoost vs Gradient Boosting<a class="headerlink" href="#comparacion-adaboost-vs-gradient-boosting" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Aspecto</th>
<th>AdaBoost</th>
<th>Gradient Boosting</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Actualización</strong></td>
<td>Pesos de ejemplos</td>
<td>Ajuste a residuos/gradientes</td>
</tr>
<tr>
<td><strong>Función de pérdida</strong></td>
<td>Exponential (fija)</td>
<td>Cualquiera (flexible)</td>
</tr>
<tr>
<td><strong>Clasificador débil</strong></td>
<td>Cualquiera</td>
<td>Típicamente árboles</td>
</tr>
<tr>
<td><strong>Robustez a outliers</strong></td>
<td>Baja</td>
<td>Media-Alta (depende de loss)</td>
</tr>
<tr>
<td><strong>Flexibilidad</strong></td>
<td>Menor</td>
<td>Mayor</td>
</tr>
<tr>
<td><strong>Teoría</strong></td>
<td>Más simple</td>
<td>Más general</td>
</tr>
<tr>
<td><strong>Rendimiento</strong></td>
<td>Bueno</td>
<td>Excelente</td>
</tr>
</tbody>
</table>
<h3 id="ejemplo-completo_2">Ejemplo Completo<a class="headerlink" href="#ejemplo-completo_2" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-71-1" name="__codelineno-71-1" href="#__codelineno-71-1"></a><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<a id="__codelineno-71-2" name="__codelineno-71-2" href="#__codelineno-71-2"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_val_score</span>
<a id="__codelineno-71-3" name="__codelineno-71-3" href="#__codelineno-71-3"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<a id="__codelineno-71-4" name="__codelineno-71-4" href="#__codelineno-71-4"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-71-5" name="__codelineno-71-5" href="#__codelineno-71-5"></a>
<a id="__codelineno-71-6" name="__codelineno-71-6" href="#__codelineno-71-6"></a><span class="c1"># Datos</span>
<a id="__codelineno-71-7" name="__codelineno-71-7" href="#__codelineno-71-7"></a><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-71-8" name="__codelineno-71-8" href="#__codelineno-71-8"></a><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-71-9" name="__codelineno-71-9" href="#__codelineno-71-9"></a>
<a id="__codelineno-71-10" name="__codelineno-71-10" href="#__codelineno-71-10"></a><span class="c1"># Modelo</span>
<a id="__codelineno-71-11" name="__codelineno-71-11" href="#__codelineno-71-11"></a><span class="n">gb</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-71-12" name="__codelineno-71-12" href="#__codelineno-71-12"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-71-13" name="__codelineno-71-13" href="#__codelineno-71-13"></a>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
<a id="__codelineno-71-14" name="__codelineno-71-14" href="#__codelineno-71-14"></a>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<a id="__codelineno-71-15" name="__codelineno-71-15" href="#__codelineno-71-15"></a>    <span class="n">subsample</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
<a id="__codelineno-71-16" name="__codelineno-71-16" href="#__codelineno-71-16"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
<a id="__codelineno-71-17" name="__codelineno-71-17" href="#__codelineno-71-17"></a>    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span>  <span class="c1"># Mostrar progreso</span>
<a id="__codelineno-71-18" name="__codelineno-71-18" href="#__codelineno-71-18"></a><span class="p">)</span>
<a id="__codelineno-71-19" name="__codelineno-71-19" href="#__codelineno-71-19"></a>
<a id="__codelineno-71-20" name="__codelineno-71-20" href="#__codelineno-71-20"></a><span class="c1"># Entrenar</span>
<a id="__codelineno-71-21" name="__codelineno-71-21" href="#__codelineno-71-21"></a><span class="n">gb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-71-22" name="__codelineno-71-22" href="#__codelineno-71-22"></a>
<a id="__codelineno-71-23" name="__codelineno-71-23" href="#__codelineno-71-23"></a><span class="c1"># Evaluar</span>
<a id="__codelineno-71-24" name="__codelineno-71-24" href="#__codelineno-71-24"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Train score: </span><span class="si">{</span><span class="n">gb</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="w"> </span><span class="n">y_train</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-71-25" name="__codelineno-71-25" href="#__codelineno-71-25"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Test score: </span><span class="si">{</span><span class="n">gb</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-71-26" name="__codelineno-71-26" href="#__codelineno-71-26"></a>
<a id="__codelineno-71-27" name="__codelineno-71-27" href="#__codelineno-71-27"></a><span class="c1"># Feature importance</span>
<a id="__codelineno-71-28" name="__codelineno-71-28" href="#__codelineno-71-28"></a><span class="n">importances</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">feature_importances_</span>
<a id="__codelineno-71-29" name="__codelineno-71-29" href="#__codelineno-71-29"></a><span class="n">top_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="mi">5</span><span class="p">]</span>
<a id="__codelineno-71-30" name="__codelineno-71-30" href="#__codelineno-71-30"></a><span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Top 5 features:"</span><span class="p">)</span>
<a id="__codelineno-71-31" name="__codelineno-71-31" href="#__codelineno-71-31"></a><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">top_features</span><span class="p">:</span>
<a id="__codelineno-71-32" name="__codelineno-71-32" href="#__codelineno-71-32"></a>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Feature </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">importances</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-71-33" name="__codelineno-71-33" href="#__codelineno-71-33"></a>
<a id="__codelineno-71-34" name="__codelineno-71-34" href="#__codelineno-71-34"></a><span class="c1"># Learning curves</span>
<a id="__codelineno-71-35" name="__codelineno-71-35" href="#__codelineno-71-35"></a><span class="n">train_scores</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-71-36" name="__codelineno-71-36" href="#__codelineno-71-36"></a><span class="n">test_scores</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-71-37" name="__codelineno-71-37" href="#__codelineno-71-37"></a><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">train_pred</span><span class="p">,</span> <span class="n">test_pred</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span>
<a id="__codelineno-71-38" name="__codelineno-71-38" href="#__codelineno-71-38"></a>    <span class="n">gb</span><span class="o">.</span><span class="n">staged_predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span>
<a id="__codelineno-71-39" name="__codelineno-71-39" href="#__codelineno-71-39"></a>    <span class="n">gb</span><span class="o">.</span><span class="n">staged_predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<a id="__codelineno-71-40" name="__codelineno-71-40" href="#__codelineno-71-40"></a><span class="p">)):</span>
<a id="__codelineno-71-41" name="__codelineno-71-41" href="#__codelineno-71-41"></a>    <span class="n">train_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_pred</span> <span class="o">==</span> <span class="n">y_train</span><span class="p">))</span>
<a id="__codelineno-71-42" name="__codelineno-71-42" href="#__codelineno-71-42"></a>    <span class="n">test_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_pred</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">))</span>
<a id="__codelineno-71-43" name="__codelineno-71-43" href="#__codelineno-71-43"></a>
<a id="__codelineno-71-44" name="__codelineno-71-44" href="#__codelineno-71-44"></a><span class="c1"># El error converge gradualmente</span>
<a id="__codelineno-71-45" name="__codelineno-71-45" href="#__codelineno-71-45"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Scores cada 25 árboles:"</span><span class="p">)</span>
<a id="__codelineno-71-46" name="__codelineno-71-46" href="#__codelineno-71-46"></a><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span> <span class="mi">74</span><span class="p">,</span> <span class="mi">99</span><span class="p">]:</span>
<a id="__codelineno-71-47" name="__codelineno-71-47" href="#__codelineno-71-47"></a>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Árboles </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: Train=</span><span class="si">{</span><span class="n">train_scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Test=</span><span class="si">{</span><span class="n">test_scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<hr>
<h2 id="43-histgradientboosting-scikit-learn">4.3. HistGradientBoosting (scikit-learn)<a class="headerlink" href="#43-histgradientboosting-scikit-learn" title="Permanent link">¶</a></h2>
<p>HistGradientBoosting es la implementación moderna de Gradient Boosting en scikit-learn, inspirada en LightGBM y disponible desde la versión 0.21.</p>
<p><strong>Innovación principal:</strong> Usar <strong>histogramas</strong> para discretizar features continuas, acelerando enormemente la búsqueda de splits.</p>
<h3 id="diferencias-con-gradientboosting-tradicional">Diferencias con GradientBoosting tradicional<a class="headerlink" href="#diferencias-con-gradientboosting-tradicional" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Aspecto</th>
<th>GradientBoosting</th>
<th>HistGradientBoosting</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Búsqueda de splits</strong></td>
<td>Exacta (todos los valores)</td>
<td>Histogramas (valores discretizados)</td>
</tr>
<tr>
<td><strong>Velocidad</strong></td>
<td>Lenta</td>
<td><strong>10-100x más rápida</strong></td>
</tr>
<tr>
<td><strong>Escalabilidad</strong></td>
<td>Dataset mediano</td>
<td><strong>Dataset grande</strong></td>
</tr>
<tr>
<td><strong>Missing values</strong></td>
<td>No soportado</td>
<td><strong>Soporte nativo</strong></td>
</tr>
<tr>
<td><strong>Features categóricas</strong></td>
<td>Manual encoding</td>
<td><strong>Soporte experimental</strong></td>
</tr>
<tr>
<td><strong>API</strong></td>
<td>n_estimators</td>
<td><strong>max_iter</strong></td>
</tr>
</tbody>
</table>
<h3 id="algoritmo-basado-en-histogramas">Algoritmo basado en Histogramas<a class="headerlink" href="#algoritmo-basado-en-histogramas" title="Permanent link">¶</a></h3>
<p><strong>Idea:</strong> En lugar de considerar todos los posibles valores para splits, discretizar features en bins (histogramas).
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-72-1" name="__codelineno-72-1" href="#__codelineno-72-1"></a>Feature continua: [0.1, 0.3, 0.5, 0.7, 0.9, 1.2, 1.5, ...]
<a id="__codelineno-72-2" name="__codelineno-72-2" href="#__codelineno-72-2"></a>
<a id="__codelineno-72-3" name="__codelineno-72-3" href="#__codelineno-72-3"></a>Tradicional: Considerar todos los valores como candidatos a split
<a id="__codelineno-72-4" name="__codelineno-72-4" href="#__codelineno-72-4"></a>→ O(n * p) comparaciones por nivel
<a id="__codelineno-72-5" name="__codelineno-72-5" href="#__codelineno-72-5"></a>
<a id="__codelineno-72-6" name="__codelineno-72-6" href="#__codelineno-72-6"></a>Histogramas: Discretizar en 255 bins
<a id="__codelineno-72-7" name="__codelineno-72-7" href="#__codelineno-72-7"></a>→ O(255 * p) comparaciones por nivel (independiente de n!)
</code></pre></div><p></p>
<p><strong>Construcción de histogramas:</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-73-1" name="__codelineno-73-1" href="#__codelineno-73-1"></a>1. Para cada feature:
<a id="__codelineno-73-2" name="__codelineno-73-2" href="#__codelineno-73-2"></a>   - Calcular quantiles para crear bins
<a id="__codelineno-73-3" name="__codelineno-73-3" href="#__codelineno-73-3"></a>   - Mapear valores continuos → índices de bins (0-254)
<a id="__codelineno-73-4" name="__codelineno-73-4" href="#__codelineno-73-4"></a>
<a id="__codelineno-73-5" name="__codelineno-73-5" href="#__codelineno-73-5"></a>2. Para encontrar mejor split:
<a id="__codelineno-73-6" name="__codelineno-73-6" href="#__codelineno-73-6"></a>   - Solo considerar los 255 bins como candidatos
<a id="__codelineno-73-7" name="__codelineno-73-7" href="#__codelineno-73-7"></a>   - Acumulación eficiente de gradientes por bin
</code></pre></div><p></p>
<p><strong>Ventaja masiva:</strong> Complejidad independiente del número de ejemplos.</p>
<h3 id="implementacion-en-scikit-learn_3">Implementación en scikit-learn<a class="headerlink" href="#implementacion-en-scikit-learn_3" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-74-1" name="__codelineno-74-1" href="#__codelineno-74-1"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">,</span> <span class="n">HistGradientBoostingRegressor</span>
<a id="__codelineno-74-2" name="__codelineno-74-2" href="#__codelineno-74-2"></a>
<a id="__codelineno-74-3" name="__codelineno-74-3" href="#__codelineno-74-3"></a><span class="c1"># Clasificación</span>
<a id="__codelineno-74-4" name="__codelineno-74-4" href="#__codelineno-74-4"></a><span class="n">hgb</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-74-5" name="__codelineno-74-5" href="#__codelineno-74-5"></a>    <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>              <span class="c1"># Número de árboles (equivalente a n_estimators)</span>
<a id="__codelineno-74-6" name="__codelineno-74-6" href="#__codelineno-74-6"></a>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>         <span class="c1"># Tasa de aprendizaje</span>
<a id="__codelineno-74-7" name="__codelineno-74-7" href="#__codelineno-74-7"></a>    <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>            <span class="c1"># Sin límite (usa max_leaf_nodes)</span>
<a id="__codelineno-74-8" name="__codelineno-74-8" href="#__codelineno-74-8"></a>    <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">31</span><span class="p">,</span>         <span class="c1"># Máximo número de hojas (default)</span>
<a id="__codelineno-74-9" name="__codelineno-74-9" href="#__codelineno-74-9"></a>    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>       <span class="c1"># Mínimo en hojas</span>
<a id="__codelineno-74-10" name="__codelineno-74-10" href="#__codelineno-74-10"></a>    <span class="n">l2_regularization</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>     <span class="c1"># Regularización L2</span>
<a id="__codelineno-74-11" name="__codelineno-74-11" href="#__codelineno-74-11"></a>    <span class="n">max_bins</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>              <span class="c1"># Número de bins (default)</span>
<a id="__codelineno-74-12" name="__codelineno-74-12" href="#__codelineno-74-12"></a>    <span class="n">categorical_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># Índices de features categóricas</span>
<a id="__codelineno-74-13" name="__codelineno-74-13" href="#__codelineno-74-13"></a>    <span class="n">early_stopping</span><span class="o">=</span><span class="s1">'auto'</span><span class="p">,</span>     <span class="c1"># Early stopping automático</span>
<a id="__codelineno-74-14" name="__codelineno-74-14" href="#__codelineno-74-14"></a>    <span class="n">scoring</span><span class="o">=</span><span class="s1">'loss'</span><span class="p">,</span>            <span class="c1"># Métrica para early stopping</span>
<a id="__codelineno-74-15" name="__codelineno-74-15" href="#__codelineno-74-15"></a>    <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>   <span class="c1"># Fracción para validación</span>
<a id="__codelineno-74-16" name="__codelineno-74-16" href="#__codelineno-74-16"></a>    <span class="n">n_iter_no_change</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>       <span class="c1"># Parar si no mejora</span>
<a id="__codelineno-74-17" name="__codelineno-74-17" href="#__codelineno-74-17"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-74-18" name="__codelineno-74-18" href="#__codelineno-74-18"></a><span class="p">)</span>
<a id="__codelineno-74-19" name="__codelineno-74-19" href="#__codelineno-74-19"></a>
<a id="__codelineno-74-20" name="__codelineno-74-20" href="#__codelineno-74-20"></a><span class="n">hgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<h3 id="diferencias-de-api-importantes">Diferencias de API importantes<a class="headerlink" href="#diferencias-de-api-importantes" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-75-1" name="__codelineno-75-1" href="#__codelineno-75-1"></a><span class="c1"># GradientBoosting tradicional</span>
<a id="__codelineno-75-2" name="__codelineno-75-2" href="#__codelineno-75-2"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<a id="__codelineno-75-3" name="__codelineno-75-3" href="#__codelineno-75-3"></a><span class="n">gb</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-75-4" name="__codelineno-75-4" href="#__codelineno-75-4"></a>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="c1"># ← Nombre del parámetro</span>
<a id="__codelineno-75-5" name="__codelineno-75-5" href="#__codelineno-75-5"></a>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span>
<a id="__codelineno-75-6" name="__codelineno-75-6" href="#__codelineno-75-6"></a><span class="p">)</span>
<a id="__codelineno-75-7" name="__codelineno-75-7" href="#__codelineno-75-7"></a>
<a id="__codelineno-75-8" name="__codelineno-75-8" href="#__codelineno-75-8"></a><span class="c1"># HistGradientBoosting</span>
<a id="__codelineno-75-9" name="__codelineno-75-9" href="#__codelineno-75-9"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingClassifier</span>
<a id="__codelineno-75-10" name="__codelineno-75-10" href="#__codelineno-75-10"></a><span class="n">hgb</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-75-11" name="__codelineno-75-11" href="#__codelineno-75-11"></a>    <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>      <span class="c1"># ← Diferente nombre!</span>
<a id="__codelineno-75-12" name="__codelineno-75-12" href="#__codelineno-75-12"></a>    <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">31</span>  <span class="c1"># ← Controla complejidad diferente</span>
<a id="__codelineno-75-13" name="__codelineno-75-13" href="#__codelineno-75-13"></a><span class="p">)</span>
</code></pre></div>
<h3 id="manejo-nativo-de-missing-values">Manejo Nativo de Missing Values<a class="headerlink" href="#manejo-nativo-de-missing-values" title="Permanent link">¶</a></h3>
<p>Una de las grandes ventajas:
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-76-1" name="__codelineno-76-1" href="#__codelineno-76-1"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-76-2" name="__codelineno-76-2" href="#__codelineno-76-2"></a>
<a id="__codelineno-76-3" name="__codelineno-76-3" href="#__codelineno-76-3"></a><span class="c1"># Datos con valores faltantes</span>
<a id="__codelineno-76-4" name="__codelineno-76-4" href="#__codelineno-76-4"></a><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
<a id="__codelineno-76-5" name="__codelineno-76-5" href="#__codelineno-76-5"></a><span class="n">X_train</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
<a id="__codelineno-76-6" name="__codelineno-76-6" href="#__codelineno-76-6"></a>
<a id="__codelineno-76-7" name="__codelineno-76-7" href="#__codelineno-76-7"></a><span class="c1"># GradientBoosting: ERROR</span>
<a id="__codelineno-76-8" name="__codelineno-76-8" href="#__codelineno-76-8"></a><span class="c1"># gb = GradientBoostingClassifier()</span>
<a id="__codelineno-76-9" name="__codelineno-76-9" href="#__codelineno-76-9"></a><span class="c1"># gb.fit(X_train, y_train)  # ValueError: Input contains NaN</span>
<a id="__codelineno-76-10" name="__codelineno-76-10" href="#__codelineno-76-10"></a>
<a id="__codelineno-76-11" name="__codelineno-76-11" href="#__codelineno-76-11"></a><span class="c1"># HistGradientBoosting: Funciona!</span>
<a id="__codelineno-76-12" name="__codelineno-76-12" href="#__codelineno-76-12"></a><span class="n">hgb</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">()</span>
<a id="__codelineno-76-13" name="__codelineno-76-13" href="#__codelineno-76-13"></a><span class="n">hgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>  <span class="c1"># OK, aprende dirección óptima para NaN</span>
</code></pre></div><p></p>
<p><strong>Cómo maneja NaN:</strong>
- Trata NaN como un valor especial en el histograma
- Aprende la mejor dirección (izquierda o derecha) para NaN en cada split
- No requiere imputación manual</p>
<h3 id="soporte-experimental-para-features-categoricas">Soporte Experimental para Features Categóricas<a class="headerlink" href="#soporte-experimental-para-features-categoricas" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-77-1" name="__codelineno-77-1" href="#__codelineno-77-1"></a><span class="c1"># Marcar features categóricas</span>
<a id="__codelineno-77-2" name="__codelineno-77-2" href="#__codelineno-77-2"></a><span class="n">categorical_features</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>  <span class="c1"># Índices de columnas categóricas</span>
<a id="__codelineno-77-3" name="__codelineno-77-3" href="#__codelineno-77-3"></a>
<a id="__codelineno-77-4" name="__codelineno-77-4" href="#__codelineno-77-4"></a><span class="n">hgb</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-77-5" name="__codelineno-77-5" href="#__codelineno-77-5"></a>    <span class="n">categorical_features</span><span class="o">=</span><span class="n">categorical_features</span>
<a id="__codelineno-77-6" name="__codelineno-77-6" href="#__codelineno-77-6"></a><span class="p">)</span>
<a id="__codelineno-77-7" name="__codelineno-77-7" href="#__codelineno-77-7"></a>
<a id="__codelineno-77-8" name="__codelineno-77-8" href="#__codelineno-77-8"></a><span class="c1"># O pasar un boolean mask</span>
<a id="__codelineno-77-9" name="__codelineno-77-9" href="#__codelineno-77-9"></a><span class="n">categorical_mask</span> <span class="o">=</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
<a id="__codelineno-77-10" name="__codelineno-77-10" href="#__codelineno-77-10"></a><span class="n">hgb</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-77-11" name="__codelineno-77-11" href="#__codelineno-77-11"></a>    <span class="n">categorical_features</span><span class="o">=</span><span class="n">categorical_mask</span>
<a id="__codelineno-77-12" name="__codelineno-77-12" href="#__codelineno-77-12"></a><span class="p">)</span>
<a id="__codelineno-77-13" name="__codelineno-77-13" href="#__codelineno-77-13"></a>
<a id="__codelineno-77-14" name="__codelineno-77-14" href="#__codelineno-77-14"></a><span class="n">hgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<p><strong>Ventaja:</strong> No requiere one-hot encoding manual, maneja categorías de forma eficiente.</p>
<h3 id="early-stopping-automatico">Early Stopping Automático<a class="headerlink" href="#early-stopping-automatico" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-78-1" name="__codelineno-78-1" href="#__codelineno-78-1"></a><span class="n">hgb</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span>
<a id="__codelineno-78-2" name="__codelineno-78-2" href="#__codelineno-78-2"></a>    <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>            <span class="c1"># Número máximo de iteraciones</span>
<a id="__codelineno-78-3" name="__codelineno-78-3" href="#__codelineno-78-3"></a>    <span class="n">early_stopping</span><span class="o">=</span><span class="s1">'auto'</span><span class="p">,</span>    <span class="c1"># Activar early stopping</span>
<a id="__codelineno-78-4" name="__codelineno-78-4" href="#__codelineno-78-4"></a>    <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>  <span class="c1"># 10% para validación</span>
<a id="__codelineno-78-5" name="__codelineno-78-5" href="#__codelineno-78-5"></a>    <span class="n">n_iter_no_change</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>      <span class="c1"># Parar si no mejora en 10 iteraciones</span>
<a id="__codelineno-78-6" name="__codelineno-78-6" href="#__codelineno-78-6"></a>    <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span>                  <span class="c1"># Tolerancia de mejora</span>
<a id="__codelineno-78-7" name="__codelineno-78-7" href="#__codelineno-78-7"></a><span class="p">)</span>
<a id="__codelineno-78-8" name="__codelineno-78-8" href="#__codelineno-78-8"></a>
<a id="__codelineno-78-9" name="__codelineno-78-9" href="#__codelineno-78-9"></a><span class="n">hgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-78-10" name="__codelineno-78-10" href="#__codelineno-78-10"></a>
<a id="__codelineno-78-11" name="__codelineno-78-11" href="#__codelineno-78-11"></a><span class="c1"># Número real de iteraciones usadas</span>
<a id="__codelineno-78-12" name="__codelineno-78-12" href="#__codelineno-78-12"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Iteraciones usadas: </span><span class="si">{</span><span class="n">hgb</span><span class="o">.</span><span class="n">n_iter_</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<h3 id="comparacion-de-velocidad">Comparación de Velocidad<a class="headerlink" href="#comparacion-de-velocidad" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-79-1" name="__codelineno-79-1" href="#__codelineno-79-1"></a><span class="kn">import</span> <span class="nn">time</span>
<a id="__codelineno-79-2" name="__codelineno-79-2" href="#__codelineno-79-2"></a><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<a id="__codelineno-79-3" name="__codelineno-79-3" href="#__codelineno-79-3"></a>
<a id="__codelineno-79-4" name="__codelineno-79-4" href="#__codelineno-79-4"></a><span class="c1"># Dataset grande</span>
<a id="__codelineno-79-5" name="__codelineno-79-5" href="#__codelineno-79-5"></a><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-79-6" name="__codelineno-79-6" href="#__codelineno-79-6"></a><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<a id="__codelineno-79-7" name="__codelineno-79-7" href="#__codelineno-79-7"></a>
<a id="__codelineno-79-8" name="__codelineno-79-8" href="#__codelineno-79-8"></a><span class="c1"># GradientBoosting tradicional</span>
<a id="__codelineno-79-9" name="__codelineno-79-9" href="#__codelineno-79-9"></a><span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<a id="__codelineno-79-10" name="__codelineno-79-10" href="#__codelineno-79-10"></a><span class="n">gb</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<a id="__codelineno-79-11" name="__codelineno-79-11" href="#__codelineno-79-11"></a><span class="n">gb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-79-12" name="__codelineno-79-12" href="#__codelineno-79-12"></a><span class="n">gb_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
<a id="__codelineno-79-13" name="__codelineno-79-13" href="#__codelineno-79-13"></a>
<a id="__codelineno-79-14" name="__codelineno-79-14" href="#__codelineno-79-14"></a><span class="c1"># HistGradientBoosting</span>
<a id="__codelineno-79-15" name="__codelineno-79-15" href="#__codelineno-79-15"></a><span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<a id="__codelineno-79-16" name="__codelineno-79-16" href="#__codelineno-79-16"></a><span class="n">hgb</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<a id="__codelineno-79-17" name="__codelineno-79-17" href="#__codelineno-79-17"></a><span class="n">hgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-79-18" name="__codelineno-79-18" href="#__codelineno-79-18"></a><span class="n">hgb_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
<a id="__codelineno-79-19" name="__codelineno-79-19" href="#__codelineno-79-19"></a>
<a id="__codelineno-79-20" name="__codelineno-79-20" href="#__codelineno-79-20"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"GradientBoosting: </span><span class="si">{</span><span class="n">gb_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s, Score: </span><span class="si">{</span><span class="n">gb</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-79-21" name="__codelineno-79-21" href="#__codelineno-79-21"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"HistGradientBoosting: </span><span class="si">{</span><span class="n">hgb_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s, Score: </span><span class="si">{</span><span class="n">hgb</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-79-22" name="__codelineno-79-22" href="#__codelineno-79-22"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Speedup: </span><span class="si">{</span><span class="n">gb_time</span><span class="o">/</span><span class="n">hgb_time</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x"</span><span class="p">)</span>
</code></pre></div>
<p><strong>Resultado típico:</strong></p>
<p>GradientBoosting: 45.23s, Score: 0.8756
HistGradientBoosting: 2.14s, Score: 0.8812
Speedup: 21.1x</p>
<h3 id="hiperparametros-clave">Hiperparámetros Clave<a class="headerlink" href="#hiperparametros-clave" title="Permanent link">¶</a></h3>
<p><strong>Control de iteraciones:</strong>
- <code>max_iter</code>: Número de boosting iterations (100-1000)
- <code>learning_rate</code>: Shrinkage (0.01-0.1)</p>
<p><strong>Control de complejidad:</strong>
- <code>max_leaf_nodes</code>: Máximo número de hojas (31 default)
- <code>max_depth</code>: Profundidad máxima (None = sin límite)
- <code>min_samples_leaf</code>: Mínimo de muestras en hojas (20 default)Regularización:</p>
<p>l2_regularization: Regularización L2 en hojas (0.0 default)
Histogramas:</p>
<p>max_bins: Número de bins para histogramas (255 default, máximo)
Early stopping:</p>
<p>early_stopping: 'auto', True, False
validation_fraction: Fracción para validación (0.1 default)
n_iter_no_change: Paciencia para early stopping (10 default)
Ventajas de HistGradientBoosting
✅ Mucho más rápido que GradientBoosting tradicional (10-100x)
✅ Escala a datasets grandes (millones de ejemplos)
✅ Manejo nativo de missing values
✅ Soporte para categóricas (experimental)
✅ Early stopping incorporado
✅ Incluido en sklearn (sin instalación adicional)
✅ API similar a GradientBoosting tradicional
✅ Rendimiento comparable a XGBoost/LightGBM
Limitaciones
⚠️ Features categóricas aún experimental
⚠️ API ligeramente diferente (max_iter vs n_estimators)
⚠️ Menos features que XGBoost/LightGBM/CatBoost
⚠️ Sin soporte GPU
⚠️ Comunidad más pequeña que XGBoost
Cuándo usar HistGradientBoostingUsar HistGradientBoosting cuando:</p>
<p>✅ Quieres rapidez de LightGBM sin instalación extra
✅ Dataset grande (&gt;10k ejemplos, &gt;50 features)
✅ Tienes missing values (soporte nativo)
✅ Quieres quedarte dentro de sklearn (sin dependencias)
✅ No necesitas features avanzadas de XGBoost
Considerar alternativas cuando:</p>
<p>❌ Dataset pequeño → GradientBoosting tradicional está bien
❌ Necesitas features categóricas robustas → CatBoost
❌ Máximo rendimiento en competición → XGBoost/LightGBM
❌ Necesitas GPU → XGBoost/LightGBM/CatBoost</p>





                
              </article>
            </div>
          
          
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"></path></svg>
            Volver al principio
          </a>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.sections", "navigation.expand", "navigation.top", "navigation.indexes", "toc.follow", "toc.integrate", "search.suggest", "search.highlight", "content.code.copy", "content.code.annotate"], "search": "../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copiado al portapapeles", "clipboard.copy": "Copiar al portapapeles", "search.result.more.one": "1 m\u00e1s en esta p\u00e1gina", "search.result.more.other": "# m\u00e1s en esta p\u00e1gina", "search.result.none": "No se encontraron documentos", "search.result.one": "1 documento encontrado", "search.result.other": "# documentos encontrados", "search.result.placeholder": "Teclee para comenzar b\u00fasqueda", "search.result.term.missing": "Falta", "select.version": "Seleccionar versi\u00f3n"}}</script>
    
    
      <script src="../assets/javascripts/bundle.b78d2936.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>